# 音楽プログラミング言語の歴史

ここまで、第2章では音楽プログラミング言語のデザインを研究として正当化する方法、第3章では、今日における音楽家の制作実践としての音楽プログラミング言語設計という歴史的背景について検討してきた。

本章では、この2つの歴史を踏まえた上で音楽プログラミング言語とはどのような特性（≒ナラデハ特徴(by松永)）を持っているのかについてを再整理する。

まずは、既存の音楽プログラミング言語に関するサーベイの内容について軽く触れておく。音楽プログラミング言語に関する文献は基本的に、（本論文もそうだが）個別のプログラミング言語実装についてが主内容の論文の前段として書かれているものがほとんどだが、サーベイに特化した文献としては[@Nishino2016]と[@Dannenberg2018]が存在する[^authorsinfo]。

[^authorsinfo]:なお、NishinoはLC[@Nishino2014]の、DannenbergはNyquist[@Dannenberg1997]他多数の言語の設計者でもある。このことからもやはり、音楽プログラミング言語の歴史を記述するには基本的にその設計や実装に関する知識や経験が必要になっていることが窺える。


NishinoとNakatsuのサーベイでは1940年代の電子計算機誕生直後から試みられてきた、コンピューターを音楽に用いるための歴史を時系列に追いかけている。Dannenbergのサーベイでは、同様に歴史的変遷を辿った上で、各言語を特徴づける要素をSyntax、Semantics、Library、Development Environment、Community &Resourcesという6つの要素として提示し、また、汎用プログラミングでは考慮されることの少ない音楽特有の課題を列挙し、代表的な言語間での記述の違いを分析している。

また[@Tanaka2017]は70〜80年代におけるチップチューンと呼ばれる、初期のパーソナルコンピューターやゲーム機においてとられた、音声生成用のICチップを用いた音楽についての歴史を主に解説しているが、40〜60年代までのコンピューター音楽黎明期についての記述も厚い。

本章でも先行文献と同様に歴史的に代表的な言語を時系列に紹介するが、既出の文献では言及されていなかった点として、以下の2つの項目に着目し、大きく時代を1970年、1990年ごろを大きな区切りとし、3つに区分けして整理する。

1970年代の区切りは2章で見てきたコンピューターをメタメディア装置として扱う思想の始まりと、パーソナルコンピューターの登場という2種類の出来事である。

そもそも音楽プログラミング言語の祖先となるソフトウェアMUSICが開発されたのは正解で最初の(汎用)プログラミング言語FORTRANが作られた翌年であり、当然1章で見たアラン・ケイらによる対話的プログラミング環境や豊富な入出力インターフェースを備えるよりもずっと前のことである。
なので、必然的に音楽ソフトウェアのプログラミング自体も機械語を直接入力するかアセンブリ言語(機械語の命令をテキストと1対1対応させたプリミティブなプログラミング言語のようなもの)しかなかったし、そのソフトウェアに対する入力データ（≒楽譜）も同様の形式を取らざるを得ないものだった。

つまり、1950〜1970年代の音楽プログラミング環境は大まかにいってコンピューターで音楽を作るためのソフトウェア全般の祖先にあたるものであって、必ずしもプログラミングという行為やテキスト入力という形式の固有性を積極的に取り入れたものではない、ということだ。

逆に、70年代以降の音楽プログラミング言語/環境はマウスや(文字入力や、ピアノ鍵盤どちらにせよ)キーボード入力といった直感的（WYSIWYG的）なインターフェースが選択肢として存在する中で敢えてプログラミングという手段を使うものとして設計されてきた、という違いがあると言えるだろう。プログラミング環境であっても、GUIの誕生はMax(Puckette)を代表としてテキストインターフェースだけでなく、入出力を持つボックスをマウスで繋いでいくような形式など、テキストに留まらない形式でのプログラミング行為を可能にした。これは同時に、出力された信号などもオシロスコープのようなグラフィックとしてフィードバックが返ってきたり、パッチ（Maxにおけるプログラムのこと）中にスライダーのような、プログラムされたソフトウェアを操作するためのインターフェースが同居していたりといった、それまで存在していた”プログラムを構築するステップ”と”構築されたプログラムを使用するステップ”に明確な境目が無くなっていく歴史でもある。


また、1990年代の区切りは、パーソナルコンピューターが専用のサウンドチップなしに、CPUだけで音声信号処理をリアルタイムで行えるようになったこと、そして汎用プログラミング言語の理論が音楽向けの言語にも流入し始めたことの2種類である。

チップチューンを歴史に入れること

コンピューターアーキテクチャのスタンダード（もっと言ってしまえば、x86アーキテクチャ）が定まるまでのプログラミングは、特定のハードウェアのための特定のプログラムを作るという側面が大きく、書かれたソフトウェアが様々なプラットフォームで使い回しが効くということでもなかったことも頭に入れておくべきだろう。汎用プログラミング言語はそれまでの実在するハードウェアに対する命令列を可読性のあるテキストデータから出力するためのソフトウェアという側面だけでなく、ラムダ計算（引用）のような、計算過程自体を数学的になるべく普遍的になるように記述する代数学の理論との接続を見せるようになり、LISPやML、Haskellに代表されるような関数型プログラミング言語のパラダイムが発生してきた。
そしてこうした分野で培われたプログラミング言語の理論は現在ではFaustやKronosを代表とする、関数型でかつ音楽や音声処理のための言語の理論的基盤としても用いられるようになっている。


つまり2020年代現在において、本論文が定義する音楽プログラミング言語とは、**コンピューターを用いて音楽を生成するためのソフトウェア群に始まりつつも、並行して発展してきた汎用プログラミング言語やその理論を取り込みつつ発展してきたソフトウェアやツール**のことを指す。なので、Maxのように前者の流れを強く汲むものは、Dannenbergが言うように、言語体系とランタイムやライブラリ、開発/実行環境があらかじめ切り離せない形式(＝実装そのものが仕様)となっていることが多い。
逆に、汎用プログラミング言語の理論をベースに構築された言語、例えばExtemporeやFaust、Kronosでは、言語仕様は言語仕様として独立しておりランタイムが存在しないーあるいは複数のランタイムの実装があり得る、そのほか、決まったIDEが存在しなかったり、複数の開発/実行環境が存在するといった構成になっているものがある。


図を入れる

なお、本稿では既存のサーベイでは特に70年代以前の研究所レベルのコンピューターを用いた取り組みに関してはNishinoらのサーベイに十分詳しい記述がなされているので、本稿に大きく関係しないと事例については省くことにし、より記述の少ない2000年、2010年代に作られた言語について積極的に取り上げる。

また、用語として”音楽プログラミング環境(Music Programming Environment)”と”音楽プログラミング言語(Music Programming Language)”という2つの言葉は特に明示的な使い分けをされずに使用されることも多い（英語では、Computer Music LanguageやComputer Music Environment、Computer Music Systemsなどなど）が、本稿では、"音楽プログラミング環境”といった時には開発環境やライブラリなど、その言語を用いる時に利用するツール全体に重点を置きたいときに、”音楽プログラミング言語”といった場合にはその言語仕様や文法など、ソフトウェアというよりも言語そのものに着目したいときに、用いることを附しておく。

# 研究所レベルでの取り組み

世界で初めて音楽にコンピューターを利用した例、というのを考えるのは、世界初のコンピューターは何かという論争や、世界で最初のアルゴリズミック・コンポジションとは何かといった命題につながってしまいキリがなくなってしまうので、ひとまず本稿で扱うのはENIAC以降の電子計算機、つまり真空管を用いてHIGH/LOWの2値をスイッチングすることで任意の計算を行える機械が誕生して以降の計算機群についての事例に限定することにする。

はじめてコンピューターを用いて音楽を鳴らした最初期の例としては、イギリスのコンピューターBINAC、アメリカのUNIVAC I、オーストラリアのCSIRACなどが挙げられる。これらはもっぱらデバッグ目的で取り付けられていたスピーカーに2値の信号をマスタークロックの周波数から逆算して規則的な周期で送ってあげれば、任意の音程の信号が出せるだろうという考えで作られたものだ。

そのため、任意の音程やリズムを奏でることはできるが、レコードのように任意の音圧波形を想いのままに作れるというわけではなかった。

コンピューターを用いて任意の波形を生成するという課題に最初に真正面から取り組んだのがBell研究所のMathewsらによるMUSICシリーズだった。

MUSICがそれ以前のシステムと異なっていたのは、パルス符合変調（PCM）と呼ばれる、音声波形を一定時間に分割（標本化）、各時間の音圧を離散的な数値として表す（量子化）、今日のコンピューター上における音声表現の基礎的な方法に基づいた計算を行ったことだ。パルス符合変調の元となる標本化定理はナイキストによって1928年に示され[@Nyquist1928]、パルス符合変調はReevesにより1938年に開発されている。

この考え方を簡単に表したのがHartley1928における図nである。時間を横軸、音圧を縦軸にとった音声波形のグラフをグリッド状に区切り、連続した数値を離散化された数値のリストに変換する。区切るグリッドが少ないほど、実際の波形との誤差が量子化歪みとして現れる一方、グリッドを細かくするほどに必要なデータの量は増えていく。また、標本化定理より、例えば1000Hzの周波数成分までを持つ波形を標本化するとき、その2倍である2000Hz以上、つまり横軸のグリッドを2000分の1秒より細かく設定する必要があることが知られている。例えばサンプリング周波数が1800Hzだった場合、表現できるのは900Hzまでとなり、1000Hzの正弦波をこのサンプリング周波数で標本化すると$1800-1000=800Hz$の信号が折り返し歪み(エイリアス信号)として現れてしまう。

人間の知覚できる周波数の上限が20000Hz程度となっているので、その2倍である40000Hz以上の標本化周波数で、かつ量子化歪みが十分に少なくなるように量子化ビット数を決めておけば、人間が近くできうる範囲ではおおよそどのような波形でも数値として表現できるということになる[^compactdisk]。

実際には連続した波形を離散化するだけであればこのような考え方の考慮で十分なのだが、標本化/量子化した波形同士を演算する場合にはもう少し細かい事情を考慮する必要があるため問題点も指摘されてきており[@Puckette2015]、音楽プログラミング言語設計の根幹にも関わってくるのだが、それはのちに議論する。

[^compactdisk]: 例えばコンパクトディスクの規格などはこういった考慮から標本化周波数44000Hz、量子化ビット数16bitと定められている。

<!--図 清書したほうがいいかも -->

![Hartleyの論文におけるPCMの概念を表した図。](img/hartley_pcm.png){width=70% #fig:hartley_pcm}


MUSICはIからVまでの5バージョンが存在している。

1957年に作られたMUSIC IはIBM 704というコンピューター上で動作する、対称系の三角波の波形に対してエンベロープを掛ける程度の波形の合成ができるシステムだった。この頃のシステムはリアルタイムで波形を電気信号として生成しスピーカーを鳴らせるようなものではない。まずMUSICのプログラムをコンピュータが利用できるIBM本社まで持ち込み、計算結果を磁気テープにバイナリデータとして書き出し、磁気テープをベル研究所に持ち帰り、そこにあった真空管製の12bitデジタル-アナログコンバーターに通してはじめて音が出せるようなシステムになっていた[@Roads1980;@Roads2001]。

MUSIC Iの時点でのサンプリング周波数について記述されている文献はいないが、後年のIBM 7090上で動作していたMUSIC IV(1963年)では、同様に12bitでの量子化で、実際の計算速度が1秒間に5000サンプル程度が性能の限界だったのに対し、実際に計算結果を再生するときにはそれを1秒に30000サンプルに早回しすることができたと記述が残っている[@Mathews1963]。

MathewsはMUSICを制作するにあたって、シャノンらが示した標本化と量子化によって、考えうるあらゆる種類の音が計算によって生み出せる、それもこれまで既存の楽器では奏でられなかったような新しい種類の音が作れることに強く関心を持っていたことを語っている。

> Essentially the sampling theorem shows that there are really no limits to the sounds you can make from samples. Any sound the human can hear, you can make with the right number, accuracy, and combination of samples, so the computer is a universal instrument. Other instruments, the violin in particular, are beautiful, lovable, but they always sound like a violin—or at least it’s very difficult to make them sound not like a violin.[@Park2009]

また彼は同時に、自身がバイオリン演奏をしていたが決して演奏が巧くはなかったため、身体的卓越を必要としない仕組みを作りたかったこと、さらに作曲家が曲を作ってもオーケストラに演奏してもらえる機会がなければ発表できないことに対するひとつの解として、作曲したものをコンピューターに演奏させるという2つのモチベーションも挙げている。

> My interests came from two things. 
> One was that, although I’ve always loved to play the violin and I’ve had an amateur string quartet going most of my life, I was never very good at it, and so I wanted to be able to make better music that didn’t require such manual dexterity as almost all [musical] instruments require. 
> I also felt that there were many composers who would compose a piece for an orchestra and who would never hear the piece, or its performance would be delayed for years, and so [the computer] would provide a way for composers to write something and hear it almost immediately.

これらコンピューターを音楽に用いる3つの理由を簡単に言い換えると次のようになるだろう。

まず1つはこれまで存在しなかった新しい音楽の探求だ。これはサンプリング定理に基づくものでない、音価レベルでのアルゴリズミック・コンポジションにも共通して言えるモチベーションと言える。2つ目は身体拡張、あるいは自動演奏の追求である。これは今日のNew Interfaces for Musical Expressionにおける研究のような、コンピューターを用いて音を生成するための身体との境界面：インターフェースを作ることによって音楽と身体の新しい関係性を発見するものだ。これは最終的に1つ目の新しい音楽表現につながることもあるが、逆に、表現自体はこれまでも存在していたが演奏に高度な技能が必要とされるものを誰でも演奏できるように…といったモチベーションにも読み替えられる。そして最後の作曲家のためのオーケストラに代わるものとしてのコンピューターとしての視点は、クイック・プロトタイピングのしやすさと表現するのがわかりやすいだろう。

ただし、もう少し本論文の興味に引き付けて読んでみると、**コンピュータ以前はオーケストラという装置が作曲のインフラストラクチャとして機能していた**という視点を導入することもできるだろう。つまり、有名な作曲家であればオーケストラに頻繁に演奏してもらえる機会に恵まれ、そうでなければ作曲家は楽譜上に音を配置し、ピアノなど少ない数の楽器である程度のシミュレーションをしながら作曲することしかできないという状態だったと言える。特に実験的な作品、すなわち既に共通して使われている楽譜上の記述では足りない表現を指示しようとしたり、大人数が演奏してみてはじめて曲の修正ができるような複雑な作品を作ろうとしたときにはオーケストラに実際に演奏してもらえるか否かは思い通りに作品が作れるかどうかに大きく関わっていたということができるだろう。



MUSICシリーズにおいて触れておくべきことは、MUSIC IIIにおいてはじめて**Unit Generator**と呼ばれる、今日まで用いられる概念が登場したことと、MUSIC IVにおいてはじめてそれ自体の実装が汎用プログラミング言語(FORTRAN)で実装されたことである。

まず、Unit Generatorとは〜

# ボーンのIRCAM 4Xプログラミングの分析

リアルタイムで処理できない程度に重かったこと
楽器のモデルを計算できる理論が存在しなかったこと→Rissetに始まる物理モデリング合成研究（Analysis-Synthesisアプローチ）

80年代にしては計算のシンプルさに対して複雑な音色が出せるFM合成とかはあったが、コンピューター音楽言語は大学や研究所に限られていたし、リアルタイム性にもまだ欠けていた extreme mediation, both temporal and conceptual

モデルがしっかりしていないと音を改善することができないけど、トライ&エラーに時間がかかるのでモデルの妥当性を確かめるのにも時間がかかってしまうパラドックス

学生がCmusicで適当に音量をデカくしたせいでfoldover歪みが発生していたが、それが案外良かった→しかしシステム側で発生した歪みだったので再現できない 本当はなんでもできるはずのコンピューターが何故！

技術のトリクルダウン　研究所でパイオニア的テクノロジーが発達し、それがコマーシャルに低価格化していくという考え方をIRCAMは持っていた

ヤマハの人がCXのデモにIRCAMにきた話

4xは当時最強のスペックだった→これ何がそうさせたんだろう？オシレーターとかはソフトウェアで仮想化できたんだろうか？西野の文献読む必要あり→できた。Variable Digital Signal Processorの話

OSから作ってた　ハードは凄かったがソフトとペリフェラルが弱い

Chant 歌声合成、Formes PatchworkとOpenMusicの手前

Chant／Formesのグループからは、音楽概念の高度な発達というコンピューターのポテンシャルを無視していると思われてた

Chant/FormesはLISP製、VAX/UNIXシステムで動いていたノンリアルタイムシステム
    users could create their own "personalized environment"
    object oriented

The use and the development of software involve the writing of coded instructions within a software language or the creation of a completely new language, within the context of a hierarchy of such languages. At each of the hierarchy a traslation occurs between any two adjacent language or levels of code. Instructions from the language at a higher level mus be translated into aform whereby they can be "read" and executed by the lower-level code or language without any (or with minimal) loss of "meaning"

The hierarcy of codes that normally operates in computer software include, at the lowest level, machine code, the instructions that drive athe hardware, writtedn in binary form; at the next level up, assembler code, made of mnemonic abbreviations of machine code; above this, the general operating system that provides a basic framework and set of servicies; and above this, any of the major lkanguages such as FORTRAN, Pascal, C, or LISP.

〜〜

Computer music software such as that used and produced by IRCAM adds yet a further level of mediation, hierarchy, and translation, sincce the music languages are themselvels based upon, or written in, established general languages.

Thus, Music V is written in FORTRAN, Cmusic in C, IRCAM's Chant in FORTRAN, and Formes in LISP.


Chantを使うにはFORTRANの知識も必要だったし、Formesを使うにはLISPの知識も必要だったので、それを勉強するためにまずLISPについて勉強しなくてはならない →**つまりこの時点ではまだ、後にMcCartneyがSuperColliderの設計指針として挙げる、プログラミングというコンピューターハードウェアを使うための専門的知識が必要な事項をencapsulateし、音楽のNotationに集中させるという意味での音楽”言語”の概念は達成されていなかったということが言えるだろう。**



# 90年代


音楽プログラミング言語の歴史において90年代は、パーソナルコンピューターでも高性能化と低価格化を背景として、4Xのような研究所でしか使えない高級なハードウェアでなくとも、DSP専用の計算ハードウェアを用いないCPU処理でのリアルタイム音声合成が可能になった時代である。

ここでの高性能化と低価格化に関しては、具体的にどの製品の登場によってブレイクスルーが発生したという具体的な事例があるわけではないが、例えば久保田は「PowerPC G3やPentium IIといったCPUによって、CDクオリティのサウンド処理に必要な、ある種のスレッショルドを超えたのだろう」と説明している[Kubota2017,88p]。


# 2000年代

## より詳細な時間制御（ChucK、LC、Gwion）

## 低レイヤの拡張（Faust、Extempore、Kronos、Vult、Soul）

## 高レイヤの拡張（TidalCycles、Sonic Pi、IXI、Gibber、Foxdot、Takt、Alda）



# 小括

音楽プログラミング言語は、元々は50~60年代のMUSIC Nシリーズのような音楽をコンピューターで使うためのシステムとして生まれたものであるが、それはリアルタイムで音を生成できるわけではなく、磁気テープに書き出した結果を改めて再生するようなシステムだった。そしてこれは必ずしも今日の音楽プログラミング言語に限らず、DAWのような音楽制作ソフトウェア全般の祖先となるようなものだったと言えるだろう。

やがて70~80年代には、音楽プログラミングの歴史は2つに分岐する。一つはの時期普及し始めたパーソナルコンピューターにおける、音声合成ICチップを用いたアマチュアを中心とするチップチューン、もう一つはIRCAMに代表される研究所レベルにおけるプロフェッショナルな現代音楽の文脈における研究、こチップチューンに使われた音声合成チップは非常に限られた数のオシレーターに対してCPUがその周波数や発音タイミングの命令を行う構成をとることで、計算コストのかかる音色の制御をハードウェアに任せて（既に成立している音楽体系である）五線譜的な表現に注力できるようにした。
一方でIRCAMでの取り組みでは、4A,4B,4Cと言ったハードウェアではチップチューン同様にハードウェア的にオシレーターの上限が決まっているような構成から、4Xでハードウェア的にはオシレーターを持たず抽象的な計算ユニットだけでリアルタイム音声合成ができるようになったという進展が見られた。
チップチューンとIRCAMでの取り組みを対比すると、この時代の取り組みはいずれもリアルタイム性を重視する代わりに、表現の自由度かハードウェア的コストのどちらかを犠牲にしていたと見ることができる。チップチューンではオシレーターの数を超える発音はできないし、4Xは自由度が高い代わりにOSから独自で構成された非常に複雑な構成をしていた。このようにどちらかを犠牲にしてでもリアルタイム性を重視したのは、音楽をコンピューターを用いて生成するにあたって、人間は何かしらの記号、シンボルを解すことでしかコンピューターに対して命令を与えることができないという理由があった。つまり、音楽を生成するモデルを人間が考えコンピューターに実行してもらうプロセスが必要になるが、このモデルが妥当かどうかを検証するにはまたコンピューターに実際に実行してもらうプロセスが必要となる。（特にIRCAMのような新しい表現を追求する場においては）例え金銭的にコストがかかったとしてもリアルタイムにトライアンドエラーができる場所があることが重要だったと言えるだろう。

そして、90年代には、パーソナルコンピューターでも高性能化と低価格化を背景として、DSP専用の計算ハードウェアを用いないCPU処理でのリアルタイム音声合成が可能になった。またこの時期にパーソナルコンピューターに用いられるオペレーティングシステムもWindows、Macintoshなどに落ち着き、一つのCPUの中で複数のアプリケーションを実行するマルチタスキングのシステムもインフラストラクチャとして安定してきて、ユーザーアプリケーションとして音声合成を扱う場合にはプリエンプティブスケジューリングの中で可能な範囲でのリアルタイム性の追求という形に落ち着いたと考えられる。
またこの時期にSuperColliderが汎用プログラミング言語において進んできた構造化プログラミングの手法を音楽プログラミングのために取り入れた。ここで初めて、音楽プログラミング言語はプログラミングという専門的なタスクから音楽、音声合成の抽象化という行為を引き剥がす機能を持つようになったと言えるだろう。


2000年代以後の音楽プログラミング言語は大きく分けてChucKやLCに代表されるプリエンプティブスケジューリング環境下での正確なイベント制御、Faustに代表されるUGenレベルの低次の自己拡張性の追求、SuperColliderクライアントに代表される高次の自己拡張性の追求おいう3つの方向性があったとまとめられる。