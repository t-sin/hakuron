# 音楽プログラミング言語の歴史

ここまで、第2章では、コンピューターがメディア装置として扱われるようになるまでの歴史と、その中で培われてきた、コンピューターは不可視になってゆくべきだという思想についてを紹介した。そして第3章では音楽の流通インフラストラクチャの変遷の歴史を辿ってきたのだった。

本章では、この2つの歴史を踏まえた上で音楽プログラミング言語とはどのような特性（≒ナラデハ特徴）を持っているのかについてを再整理する。

まずは、既存の音楽プログラミング言語に関するサーベイの内容について軽く触れておく。音楽プログラミング言語に関する文献は基本的に、（本論文もそうだが）個別のプログラミング言語実装についてが主内容の論文の前段として書かれているものがほとんどだが、サーベイに特化した文献としては[@Nishino2016]と[@Dannenberg2018]が存在する[^authorsinfo]。

[^authorsinfo]:なお、NishinoはLC[@Nishino2014]の、DannenbergはNyquist[@Dannenberg1997]他多数の言語の設計者でもある。このことからもやはり、音楽プログラミング言語の歴史を記述するには基本的にその設計や実装に関する知識や経験が必要になっていることが窺える。


NishinoとNakatsuのサーベイでは1940年代の電子計算機誕生直後から試みられてきた、コンピューターを音楽に用いるための歴史を時系列に追いかけている。Dannenbergのサーベイでは、同様に歴史的変遷を辿った上で、各言語を特徴づける要素をSyntax、Semantics、Library、Development Environment、Community &Resourcesという6つの要素として提示し、また、汎用プログラミングでは考慮されることの少ない音楽特有の課題を列挙し、代表的な言語間での記述の違いを分析している。

また[@Tanaka2017]は70~80年代におけるチップチューンと呼ばれる、初期のパーソナルコンピューターやゲーム機においてとられた、音声生成用のICチップを用いた音楽についての歴史を主に解説しているが、40~60年代までのコンピューター音楽黎明期についての記述も厚い。

本章でも先行文献と同様に歴史的に代表的な言語を時系列に紹介するが、既出の文献では言及されていなかった点として、以下の2つの項目に着目し、大きく時代を1970年、1990年ごろを大きな区切りとし、3つに区分けして整理する。

1970年代の区切りは2章で見てきたコンピューターをメタメディア装置として扱う思想の始まりと、パーソナルコンピューターの登場という2種類の出来事であり、1990年代の区切りは、パーソナルコンピューターが専用のサウンドチップなしに、CPUだけで音声信号処理をリアルタイムで行えるようになったこと、そして汎用プログラミング言語の理論が音楽向けの言語にも流入し始めたことの2種類である。

チップチューンを歴史に入れること

図を入れる

なお、本稿では既存のサーベイでは特に70年代以前の研究所レベルのコンピューターを用いた取り組みに関してはNishinoらのサーベイに十分詳しい記述がなされているので、本稿に大きく関係しないと事例については省くことにし、より記述の少ない2000年、2010年代に作られた言語について積極的に取り上げる。

# 研究所レベルでの取り組み

世界で初めて音楽にコンピューターを利用した例、というのを考えるのは、世界初のコンピューターは何かという論争や、世界で最初のアルゴリズミック・コンポジションとは何かといった命題につながってしまいキリがなくなってしまうので、ひとまず本稿で扱うのはENIAC以降の電子計算機、つまり真空管を用いてHIGH/LOWの2値をスイッチングすることで任意の計算を行える機械が誕生して以降の計算機群についての事例に限定することにする。

はじめてコンピューターを用いて音楽を鳴らした最初期の例としては、イギリスのコンピューターBINAC、アメリカのUNIVAC I、オーストラリアのCSIRACなどが挙げられる。これらはもっぱらデバッグ目的で取り付けられていたスピーカーに2値の信号をマスタークロックの周波数から逆算して規則的な周期で送ってあげれば、任意の音程の信号が出せるだろうという考えで作られたものだ。

そのため、任意の音程やリズムを奏でることはできるが、レコードのように任意の音圧波形を想いのままに作れるというわけではなかった。

コンピューターを用いて任意の波形を生成するという課題に最初に真正面から取り組んだのがBell研究所のMathewsらによるMUSICシリーズだった。

MUSICがそれ以前のシステムと異なっていたのは、パルス符合変調（PCM）と呼ばれる、音声波形を一定時間に分割（標本化）、各時間の音圧を離散的な数値として表す（量子化）、今日のコンピューター上における音声表現の基礎的な方法に基づいた計算を行ったことだ。

パルス符合変調の元となる標本化定理はナイキストによって1928年に示され、パルス符合変調はReevesにより1938年に開発されている。

パルス符合変調自体の開発の過程はテスト

1957年に作られたMUSIC IはIBM 704というコンピューター上で動作する、対称系の三角波の波形に対してエンベロープを掛ける程度の波形の合成ができるシステムだった。この頃のシステムはリアルタイムで波形を電気信号として生成しスピーカーを鳴らせるようなものではなく、まずMUSICのプログラムをIBM本社まで持ち込み、計算結果を磁気テープにバイナリデータとして書き出し、磁気テープをベル研究所に持ち帰り、そこにあった真空管製の12bit デジタル-アナログコンバーターに通してはじめて音が出せるようなシステムになっていた[@Roads1980]。
