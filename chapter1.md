# 序文の前に

2021年9月。北九州市の植物園でこのイントロダクションを書いている。

植物園、というか自然公園みたいなこの場所にはフリーランスの仕事で来ている。公園を大きく使ったナイトウォークのようなイルミネーションのようなイベントで、そのための音楽やSEを流すためのシステムを作る仕事である。仕事の半分くらいはスピーカーをどこに配置するかとか、どの機材を使うかとかそういう話なのだが、もう半分はその沢山あるスピーカーにどう音を割り振ったり、パソコンを起動したら自動的に音が鳴り始めるようにするためのプログラムを作る作業になる。

制御のためのソフトウェアはCycling'74社のMaxという音声処理が得意なマルチメディアプログラミング環境で構築している。長い歴史を持ち、現在音楽のためのプログラミング環境としては（知る限り）最も多く使われており、個人的には2015年から使い始めたこのMaxというソフトウェアを使うと、たとえば展示空間にスピーカーを10個とか（それも、いわゆるサラウンドとかとは全然違うレイアウトで）配置してそれぞれに違う音を同期して流したりすることができる。あるいは、映像を生成するコンピュータからネットワーク経由で特定の信号を受け取ったら特定の効果音を流すようにしたり。

なんだか言葉にすると大したことがない仕事のように聞こえる。実際、プログラムを書く作業自体は主観的には大したことがないのだ（ソフトウェア特有のニッチな問題やらバグは細々あるにしても）。こんな作業でお金をもらってしまっていいんだろうかと思う時もある。

しかし実際のところ似たような仕事ができる人はそこまで多いわけでもないらしい。たとえば、仕事を受けようとしたが予定が埋まっているときに別の似たような職能を持った人を紹介しようと思っても、パッと思いつく人はそう多くない。

これは、自分がこうした仕事に慣れすぎてしまったのだろうか。半分はそうなのだろうが、この仕事をしているとやはり考えざるを得ない疑問は、コンピューターはなんでもできる装置のはずなのに、たかだか沢山のスピーカーに同時にたくさん音楽を流すぐらいのことに、どうしていちいちこんな専用の（しかも有料の）ソフトウェアを使ってカスタムツールを使ってプログラムを作るような手間がかかる作業になってしまうのだろう、ということなのだ。

これは別に、使うスピーカーが多いから必要なコンピューター処理能力が大きい、という話でもない。たとえば、映像インスタレーション作品でどうしてもフォーマット的に3chのスピーカーを制御したい、となったときにも、大体10chの時と同じくらいの面倒臭さが発生する。2chが3chになったりするだけで、普通の音楽制作ソフトウェアでは扱いづらくなる。なんでmacOSでファイルを選択してスペースバーを押したら音声ファイルが再生されるように、物理的なセンサーに触ったら音声ファイルを再生する程度のことに労力が必要なのか。なんでエクセルファイルをちょっと編集するぐらいの手間で10chのスピーカーに音声ファイルを割り振れるようにならないのか。

この疑問こそが、おそらくはインフラストラクチャやフォーマットの持つ力というものの説明なのだろう。つまり、人々は2chのステレオ音声のフォーマットがすでにあるから2chステレオで音楽を作る。作るためのツールも2chが一番主流なのでそれが最も作りやすいように良心的な配慮をしてしまう。それに、いかに音楽や芸術表現が新しさを欲するものだとしても、とりあえずはその2chのフォーマットの中で表現が可能な新しさであれば問題にはならない。そして、別に3ch以上の音声フォーマットも表現として "不可能なわけではない"。ただ、ちょっと手間がかかるのだ。

技術決定論者/あるいはマルクス主義者風の言い方をすればインフラストラクチャ:下部構造が上部構造ー、つまり音楽表現とかを規定するということになるのだろうか。おそらくはそうではない。インフラストラクチャは表現を完全に縛りはしないが**誘導**する。そしてその誘導方向は既存の上部構造によって作り出されるものであり、互いに循環し合いながら徐々に路面は踏み固められていく。

# 序文

本論文は、音楽のためのプログラミング言語mimiumの設計と実装を通じて、音楽のためのプログラミング言語(Programming Language for Music:**PLfM**)を作るという行為の概念化を試み、その上であるべきコンピューターと音楽の関係性を考え直すものである。

本論文で議論する「音楽のためのプログラミング言語」は、テキストやグラフィック操作を用いてコンピューターで音楽を生成するための人工言語である。

プログラミングを用いた音楽表現は、リアルタイムにセンサーの入力を読み取り、信号処理をしてアコースティック楽器では不可能な音を生成する、「ハイパー楽器」的にコンピューターを扱うものであったり、近年ではコードを書くプロセスそのものを演奏として見せるライブコーディングのような表現なども登場しており、その領域はますます広がりを見せている

プログラミングに限らず音楽とコンピューターの関係性へと視野を広げてみれば、2021年現在、音楽を聴いたり、演奏したり、作ったりする上で、コンピュータが一切関与しない、という状況を考えるのは難しくなっている。作曲にはProtoolsやCubaseに代表されるDAW（Digital Audio Workstation）ソフトウェアを使用し、配信にはApple MusicやSpotifyのようなストリーミングサービスを通じて、デジタルデータという形で音楽は配布される。最終的に、コンピューターやスマートフォン上のソフトウェアでそのデータをデコードし、DAC(Digital-Analog Converter)によって電気信号へと変換され、その信号はスピーカーへと送られようやく空気の振動になり、私たちの耳へ届く。スピーカーの中にさえデジタル信号処理(DSP:Digital Signal Processing)用のチップが入っていて計算によって音質の調整をしていることも珍しくはない。2020年以後のコロナウィルスの影響も含めれば、クラシック音楽のコンサートさえもその場で空気の振動を体感することよりも録画録音されたものをコンピューターを通じて摂取することの方が多くなってしまったかもしれない。とかく音楽文化を見れば、マーク・ワイザーの提唱したユビキタス（Ubiquitous:遍在する）・コンピューティング[@Wiser1999]の概念は字義通りには達成されたようにも見える。

それにもにもかかわらず、音楽文化全体を見てみれば、音楽制作自体にコンピューターの可変性を十分に活かせるはずのプログラミングという手段を用いること自体はメインストリームとは程遠いと言える。たとえば、今日のDAWソフトウェアの中で、プログラムを用いてソフトウェアそれ自体を拡張する方法は、Ableton社のLiveにおける、音楽プログラミング環境Maxを内部拡張として用いることができるMax for Live、独自のスクリプト言語を使用できるReaperなどを除けば、VSTプラグインなど一部の仕組みに限られる。こうした拡張も、多くはC++のような、音楽を専門とする人には難易度の高いプログラミング言語で記述する必要があり、作家が自ら道具の機能を拡張するには未だハードルが高い。

音楽にプログラミングを用いることですらこの現状であるので、音楽のためのプログラミング言語や環境自体を作ることの事例はますます限られている。今日ソースコード共有サービスGitHubにおいて、"Programming Language"と検索すれば87502のリポジトリが出てくるのに対して、"Programming Language Music"では137しか出てこないことからその探究の規模の小ささががわかるだろう。実際、第3章で見ていくことになる2000年代以後に開発された音楽プログラミング言語は多く数えて30ちょっとに限られ、年を追うごとに開発者の数が増えているとも言い難い。

そして、受容の形式という視点で音楽とテクノロジーとの関わりを見てみると、どれだけ高度なテクノロジーを用いて生成された音声も、最終的には何らかの5〜10分の音声ファイルとして編集され、ほとんどの場合スピーカーやヘッドホンという2chの音波を発する装置によって発され耳に届くという、コンピューターが発明される前の19世紀の録音技術黎明期の受容の形式から大きくは変化していないと言える。

本論文での、音楽プログラミング言語を設計する第一の問題意識となるのは、このような、コンピューターが本来万能とも言ってもいい可変性を持っているはずで、しかも現状音楽に関与するあらゆるマシンの中にコンピューターが関与しているにも関わらず、音楽や音を用いた表現の形式には変化が乏しいのは何故だろうかという疑問、そして、コンピューターの可変性を十全に発揮するための道具とも言えるプログラミング言語を、音楽という目的に特化させて作ることは音楽表現の新たな可能性の追求としては一見最も素直なアプローチにも見えるのに、なぜ未だにその開発事例が少ないのか、という疑問である。

# 音楽のためのプログラミング言語:PLfMの指すもの

すでに音楽のためのプログラミング言語：PLfMという用語を用いたが、この語は関連研究に置いても一般的に使われる用語ではない。この語が意味するところは文字通りの意味合い以上のものはないにせよ、なぜわざわざ新しい語を用いるかについての説明をしておく必要はあるだろう。

まず、音楽のためのプログラミング言語は既存の文献では、Computer Music Language[@McCartney2002;@Mcpherson2020]、Language for Computer Music[@Dannenberg2018]、Computer Music Programming Systems[@Lazzarini2013]などといった呼ばれ方がされており、それぞれの語の使用に明確なコンセンサスがあるわけではない。その中でも敢えて筆者がComputerという語を使わない理由として、ひとつはComputer Musicという語が、コンピューターを用いることで新しい音楽表現を追求する歴史的な取り組みの中にある、特定の音楽様式と結びついてしまうことを避けるためだ。既に述べたように、今日ではあらゆる音楽制作と再生のためにコンピューターが用いられている以上、あらゆる音楽が**弱い意味でのComputer Music**と呼ぶことができる。しかしそれらの多くはコンピューターでなければ不可能な、コンピューターというメディア固有の表現を行っているわけではない。同様に、たとえばFaust[@Orlarey2004]のような、信号処理のアルゴリズムを抽象化することに特化したプログラミング言語は新しい音楽表現を目的とした言語とは言えないが、その技術的要素の多くはComputer Musicのための言語と共通するところがある。またPLfMという枠組みを用いることで、これまでの文献では比較対象に入れられること自体が少なかった、MML:Music Macro Languageのような、五線譜上の記法を直接的にテキストに置き換えたような、単にコンピューター上のテキストというフォーマットで音楽を表すことを目的とした言語たちも、チップチューンのような広い意味でのコンピューターを用いた音楽文化を作るための要素として議論の土台にあげることができる。

加えて、Programming EnvironmentやProgramming Systemといった語を用いない理由も説明しておこう。これは、音楽のためのプログラミング言語といった時に、たとえばMaxのような、ある特定のアプリケーションを想像するニュアンスを抑えるための選択だ。たとえば、汎用プログラミング言語の理論においては、プログラミング言語、と言った時にはその言語を実行するためのソフトウェアやプログラムのことを必ずしも指していない。たとえば同じC++という言語であったとしても、それを実行するソフトウェア（コンパイラ）はGCC、Clang、Microsoft Visual C++といったように複数存在し得るからだ。これらのコンパイラは、どれもC++の厳格な言語仕様で定まっている通りの動作をするが、言語仕様で未定義とされてる動作はそれぞれ異なるし、コンパイラが出力する実行バイナリ（≒アプリケーション）の中身は同じソースコードだったとしても異なる。音楽プログラミング言語においては、基本的にある言語＝特定のアプリケーションであることがほとんどだが、根本的にはアプリケーションの設計実装という作業とプログラミング言語の設計実装という作業は異なり、本研究が対象にしたいのは言語の設計なのだ。こうしたニュアンスを込めて筆者はEnvironmentやSystemという語を用いないことにした。極論を言えば、Faustのような厳密に意味論が定義されている言語においては、コンピューターを用いなくてもそのソースコードを手作業で解釈し実行することが可能だということを考えれば、プログラミング言語はコンピューターを使うための道具であることは間違いないにせよ、人間が直接的にバイナリを操りプログラムを構築するには限界があるという理由で開発されているという意味で、逆説的に徹頭徹尾人間のための道具でしかない。だからComputer Music Languageとも、Computer Programming Languageとも、呼ばずに、ただ音楽のためのプログラミング言語：Programming Language for Music、PLfMなのだ。

# 方法論：デザイン実践を通した研究

本研究は、mimiumというPLfMの設計と実装を通して、音楽の制作や聴取環境においてプログラミングという手段がなぜメインストリームな手段とならないのか、音楽におけるコンピューターの利用のされ方が旧来の文化様式に固定されたままなのかの示唆を得ようというものだ。それゆえ、一見コンピューター科学における音楽という応用分野として捉えられそうなPLfMだが、本研究はその研究パラダイムの見直しも射程に入れている。

一般に音楽や、表現のためのソフトウェアやツールのデザイン、制作は、学術的研究としてはコンピューター科学の枠には入るとはいえ、その学問としての枠組みは理論物理学のような、世界を構成する要素を実験によって確かめたり、その法則を数式で記述するようなものとは大きく異なる。ある表現を支援するための道具作りには必ずその表現の様式が先立っており、数多ある表現の種類の中からその特定の表現の存在を肯定するための主張は常に政治的なものにしかなりえないからである。この時、とくにコンピューターを用いた表現につきものである、 "新しい"表現を可能にする道具を開発するという行為を肯定するためには、まだ誰も見たことも聞いたこともなく、それを記述するための言葉も存在しないような表現の存在を認めなければならないという根本的な矛盾が生じる。

それゆえ、mimiumというプログラミング言語を作ることによりあるXという表現（既存のものでは、たとえばアルゴリズミック・コンポジション、スペクトラル・プロセッシング、マイクロサウンドなど）が可能になると言った主張をせず、コンピューターを用いるユーザーが、自分自身で新しい様式や表現方法を開拓することを阻むような重力のようなものに引き寄せられているのではないか、そしてその重力の発生源を突き止めるには、表現のための道具をリバースエンジニアリングすることで（たとえ道具の構造そのものには問題がないとしても）迫れるのではないかという仮説のもとに行動することにする。そしてその時音楽のためのプログラミング言語をリバースエンジニアリングの対象とする理由は、道具を作るための道具というメタ的存在であること、必要な背景知識が一般的な音楽を作るための楽器やソフトウェアよりも、プログラミング言語理論という音楽とは一見遠そうな分野の知識を含む分、一層複雑に見えるからだ。

それゆえ、mimiumの設計と開発は学術的研究としては次の3つのような特徴を持っている。

**1:まず、研究のアウトプットのひとつであるプログラミング言語、mimiumの設計や実装はプロトタイプではなく、実際に使われることを想定している。**

なぜなら、音楽プログラミング言語がなぜ多くは作られていないのかという疑問に答えるには、その言語の機関部分だけを作るーつまり、Proof-of-Work的なプロトタイプを作るだけでは、たとえば実際にソースコード共有サイトへのアップロード、リリースの作業、さらにリリース後のメンテナンスや、ドキュメンテーションといった、ツールを実際に運用するプロセスの中にも、言語を実際に作るにあたって高いハードルが存在しているかもしれなからだ。実際に、mimiumの実行プログラムのソースコードはすでにGithubに公開されており、何人かが開発の一部に参加してくれてもいる。
<!-- また音楽向けの言語はオリジナルの開発者が中心的役割になって継続的開発、運用を続けているケースが多数を占める。代表的な例としてはPuredataを開発したMiller Pucketteは今日まで30年近く開発とメンテナンスを続けている。数少ない例外としてはオリジナルの作者が早期に開発から抜け、コミュニティベースで運用を行っているSuperColliderや、Cycling'74という企業によってメンテナンスされるMaxといった例を挙げることができる。 -->

**2:しかし、研究のアウトプットとしてのプログラムで実用的に役に立つほどに完成はしていなくてもよいものとする。**

LISPというプログラミング言語のプログラマーで「ハッカーと画家」というエッセイ集を書いたポール・グレアムはプログラミング言語が常に完成しないという特徴を次のように表現している。

> また、プログラミング言語はまず、 完成されたプログラムの形ではなく、 プログラムがまさに開発されている最中の形をとるということを忘れないで欲しい。 芸術の分野で仕事をしている人なら誰でも、その二つの過程には 異なる媒体が必要になるだろうと言うだろう。 例えば、大理石は最終的な考えを保持するには素晴らしく長持ちする媒体だが、 新しい考えを発展させている時にはおそろしく融通の利かない媒体でもある。

> （中略）われわれはつい、 完成したプログラムがどれだけ良く見えるかで言語の良さを測ってしまいがちだ。 二つの言語で書かれた同じプログラムを眺めて、 片方がもう片方よりうんと短かかったりすると、その議論にはひどく説得力がある。 だが芸術の方向からこの問題に取り組んでみれば、 そういう測り方をしてしまうことは少ないだろう。 プログラミング言語を、大理石のような、完成形は美しいけれど それを使った製作が苦痛であるような媒体にはしたくはないだろう。[@Graham2003](http://practical-scheme.net/trans/desres-j.html)

この文章にはプログラミング言語自体の評価の難しさに、「そのプログラミング言語で書かれたソースコードの読みやすさや簡潔さ」や「そのプログラミング言語でプログラミングをする時のユーザー体験」や、さらには「そのプログラミング言語自体を開発する過程の困難さ」といったことまでを含める必要があると提起しているように読める。

実際のところ、LISPに限らずプログラミング言語やそれを実行するプログラムには明確な完成という状態を定義しにくい。常に新しい言語仕様が追加されたり削除されたりを繰り返しながら時間をかけてアップデートされていく。同じC++という言語でも、C++98という初期バージョンの言語とC++20という最新の仕様では、言語仕様もそれで書かれるソースコードの様式もほとんど別の言語と言ってもいいほどに大きく異なる。

さらに、プログラミング言語の開発やそれが実際に使われるようになるまでには一般的に2年を超える長い時間がかかるため、実用になって初めてその内容を研究として提示するプロセスを取ってしまっては研究サイクルとして時間が掛かりすぎるという問題がある。たとえばRubyを開発したまつもとゆきひろによればRubyの開発自体は1993年に成され、最低限の機能が揃ったのは半年後[@Matsumoto2014,51p]ながら、最初に公開されたバージョンである0.95は1995年と2年の時間を要している。音楽系の言語の例で言えば、mimiumの参考になっている言語であるFaustはプロジェクト自体が始まったのが2002年(https://faust.grame.fr/about/)だが、最初にリリースされた正式なバージョン0.9.0はやはり2年後の2004年になってリリースされている（https://github.com/grame-cncm/faust/tree/v0-9-0）。

それでも研究対象としては具体的な幾つかの言語仕様に焦点を当てて議論を行うことで学術的な知見を生み出すことは十分に可能だと言えるはずだ。実際にmimiumは最初のバージョンをリリースするのに9ヶ月、そもそも音楽のための言語でありながら信号処理をして実際に音がなるまでに7ヶ月の時間を要しており、この論文が書かれている現在(バージョン0.4.0)でも、外部ファイルを読み込むinclude機能は単なるテキスト置換で行っており、実行性能の効率も悪いし予期しない動作を引き起こしかねないという、とりあえずの間に合わせとして実装されているように、暫定的な機能やバグが多数存在する。

**3:学術的研究として、作った言語の評価としてユーザーからのフィードバックを（定量的な実験結果であれ、インタビュー等の質的な内容であれ）必ずしも必要としない。**

これはコンピューター科学の研究として考えると一見異質に思えるかもしれないが、その理由としては、筆者が焦点を当てたいのは音楽プログラミング言語がなぜ使われないかよりも、なぜ作る人の人数が増えないのかという問題であること、さらには、プログラミングという行為は、誰かの作ったライブラリのソースコードが公開されてさえいれば、その中身をどこまでも辿っていける、知識の階層を思うがままに辿っていける道路のネットワークであるにもかかわらず、コンピューターの中で音を（アプリケーションなどを通して）扱うことと、音楽や音のためのプログラムを作ること、さらには音楽や音を**記述するためのプログラム**：音楽プログラミング言語を作るということにはそれぞれ大きな隔たりがある、という問題であるからだ。この疑問に答えるために調べなければいけない対象は、すでに多く研究対象となっているユーザー：音楽のためのプログラム、アプリケーションや音楽プログラミング言語を仕様する人たちだけではなく、むしろ研究主体である自分自身とその経験なのだ。つまり音楽プログラミング言語やそれを作るという行為がブラックボックス化されている現状を、自らの手で作ることによって開き、そこで使われる知識や語彙の体系化を改めて試みることで疑問に対するヒントを掴むような道筋を辿ることになる。

もちろんこれは、1.で述べたように実用的なツールとしてもmimiumを開発している以上、そのツールをよりよいものにしていくために今後ユーザーにインタビューなどの形でフィードバックを求めること自体は有益になるだろう、という意見を否定するものにはならない。


コンピューター科学の中でもHuman-Computer-Interaction(HCI)の分野や、Creativity Support Tools(CST)、End-User-Programming(EUP)といった分野で議論が行われている。このようなデザインの実践を通した研究:Research through Design(RtD)としての本研究の位置付けに関しては次章で詳しく議論するが、本研究は、何か既存の言語に足りない機能があったり、既存の言語ではできない表現があるので新しい言語を作るのではなく、言語の実装と文献調査を通して、音楽プログラミング言語という研究領域はどういった領域か、またその領域にどんな課題があるかといったものを明確化することを主眼においている。いわば、音楽プログラミング言語という、コンピュータ科学や音楽全体からすれば応用（あるいは、周縁的な）分野における基礎研究なのだ。

そのため、プログラミング言語mimium自体も特定の問題意識や仮説に基づいた設計や実装が行われてはいるものの、実装するうちにその問題意識も徐々に変化していることには留意する必要がある。第3章で提示する音楽プログラミング言語の歴史観や、第5章で議論する音楽プログラミング言語の存在論はmimium実装のためのモチベーションであると同時に、mimiumを実装することによって得られた洞察でもあるということだ。

このような考察→設計→実装→考察…といった循環的な作業を行う以上、それを問題提起→解決という線状のプロセスに形式的にでも開くことは、試行と実験を繰り返す中で発生する考えの変遷のディテールを削いでしまうことは否定できない。それを補うためにもmimiumの設計と実装が時系列的にどのように行われてきたかについてをまず簡単にまとめておくことにする。

## mimium制作までのバックグラウンド

本研究の主題となる音楽プログラミング言語mimiumの開発は2018年頃に遡る。

個人的な動機の話にはなるが、筆者のバックグラウンドはコンピューターサイエンスではなく、音楽プログラミング言語を作り始めようと思ってからプログラミング言語理論を学び始めたという順番を取っており、そのバックグラウンドが音楽プログラミング言語研究の方法論自体の見直しという本論文の大きなテーマの一つにも大きく影響してくるのでその経緯を簡単に記しておくことにする。

2018年の9月から11月にかけて、筆者はSchool for Poetic Computation(SFPC)という、ニューヨークにあるアーティスト・ラン・スクールへ留学した。SFPCはテクノロジーと表現を批評的に、かつ実践的に学ぶ私学校のような場所で、年に2回、20人ほどの学生（10代から50代まで幅広い）が集まり、OpenFrameworks[^openframeworks]を用いたグラフィックプログラミングから技術批評の文献購読までを3ヶ月間、対話を交わしながら進めていくものだった。

[^openframeworks]:openframeworks

筆者はそれまで、物理モデリング楽器を物理的な要素で再構築するインスタレーション作品の制作や、オーディオフィードバックを主要素とする電子音響楽器の開発とそれを用いた即興演奏などを中心に活動してきた。

これらの作品制作への大元のモチベーションは簡単に言うならば、どれだけ工夫したプログラムを作ったところで全ては2chのステレオPCMサウンドというフォーマットに収束してしまうことへの窮屈さから来る、より異なる音楽表現の可能性の追求であった。つまりインスタレーション作品も、オーディオフィードバックを用いる電子楽器も、コンピューターにスピーカーを繋げるだけでは実現不可能な表現領域を探索することに意義を見いだしていた。

しかし同時に、音楽のフォーマット（あるいは、より広範な意味合いでの"形式"）から離れたところで、インスタレーションや、展示、あるいは即興演奏のイディオムという異なる形式へと従属する対象が変わっただけのようにも感じていた。

そうした疑問を抱えながら過ごしたSFPCでの授業は全くこれまでと違う考え方を自分に与えてくれた場所であった。

SFPCの入居しているウエストベス・アーティスツ・コミュニティという建物は、元々1890年代から1960年代までベル研究所の建物だった場所で、有名なところではショックレーらによる切開で初めてのトランジスタが発明された場所でもある。そして同時にこの場所はニューヨークのテクノロジー・アートの歴史の中心地と言ってもよい歴史的な経緯を持つ場所でもある。

ベル研究所のエンジニア、ビリー・クルーヴァーは美術家のロバート・ラウシェンバーグとともに、60年代にExperiments in Arts And Technology(E.A.T)というアーティストとエンジニアの共同作業のための集団を組織し、《九つの夕べ》や、1970年の大阪万博ペプシ館の演出を担当するなど、のちのテクノロジーを用いた芸術制作へと大きな影響を与えている。こうしたニューヨークにおける技術者と芸術家の共同作業は、ラウシェンバーグを始め、ウエストベスにのちに入居するダンサーのマース・カニングハムや、彼らとの共同作業も行った作曲家のジョン・ケージやデイヴィッド・チューダーといったアーティストの参加したブラックマウンテン・カレッジ、またケージの参加したフルクサスのような、反制度、反形式的な思想や運動を背景としていることが特徴であり、これはたとえば人類学者のジョージナ・ボーンが"Rationalizing Culture"で、フランスの電子音楽研究所において70~80年代にピエール・ブーレーズのような作曲家が電子音楽のための技術を、現代音楽という積み重ねられてきた歴史の上で正当化する過程を描いた様子とは大きく異なる[@Born1995]。

SFPCのプログラムはこうした歴史的背景をもとに、ブラックマウンテン・カレッジのような既存の教育の枠を外し、生徒が互いに教え合う機会を多く設けた"Horizontal Pedagogy"(http://taeyoonchoi.com/2012/05/notes-on-critical-pedagogy/)と呼ぶ学びの姿勢を重視したものであり、かつ、大阪万博を一つのピークとして捉えられる、60年代のテクノロジーアートにおける、表現に先行して技術を無批判に用いることや、大規模化するごとに資本主義に迎合していってしまう傾向[^mediaarthistory]への内省を踏まえたものとなっている。技術を用いる際に暗黙的に発生する政治性に自覚的になり、また技術を与えられたブラックボックスとせず、すでにライブラリやツールが存在しているものだったとしても一度自分の手で作り直すことによって体でその内容を理解することで、異なる技術のエコシステムの可能性を想像することができるようになるというわけである。

[^mediaarthistory]: たとえば、[@Ma2014,58p]の坂根厳夫の引用では、70年代はじめのオイルショックや環境問題を単に発した科学技術自体への批判との共鳴が指摘されているし、大阪万博ペプシ館でのスポンサーによる会期中のプログラム中断と、E.A.Tのその後の活動の衰退をあげることができるだろう。

それゆえ、SFPCで教える講師達の活動形態も、必ずしも作品を作って美術館に展示することが中心なわけではない。OpenFrameworksでほぼ毎日短いCGアニメーションを作り投稿し続けるZachaly Liebermanや、CPUの動作原理を餃子作りに見立てた"CPU Dumpling"ワークショップを行うTaeyoon Choi、ポストコロニアルスタディーズをベースに、白人中心主義的なコンピューター文化批判のインスタレーションやZineを制作するAmerican Artist、100年以上ただ時を刻むだけのカウンターのようなオーバー・エンジニアリングなツールを自宅の工房で作って自分たちで売り続けるCW&T……のように、多様な活動を見せる彼/彼女らの活動を通して（それでどうやって生計を立てられるかはともかくとしても）テクノロジーと社会の関係性への向き合い方は必ずしもアートという閉じられた空間の中だけで行うものでなくてもよいのだと実感させられた。

こうした経験が音楽プログラミング言語という実に微妙な領域の制作へ筆者が本格的に入っていくきっかけとなった。

詰まるところ、はじめにあげた"どれだけ工夫したプログラムを作ったところで全ては2chのステレオPCMサウンドというフォーマットに収束してしまうことへの窮屈さ"に対して真に向き合うには、何が社会的にそのフォーマットを構築しているのかについて突き詰めて考える必要があったのだ。違う表現の形式へスライドしても結局その形式の束縛を受けるだけになってしまう。ならばやるべきは制作を無意識的に支配している形式や制度そのものの脱構築に他ならない。つまり筆者にとって音楽プログラミング言語の制作とは音楽を生み出す下部構造＝インフラストラクチャになりうる道具自体を自らの手で作ることを通じて、異なる音楽の形式や制度そのものをメタ的に制作する芸術（と言い切ってしまうことにより失われるニュアンスがあり他の言葉を探している途中だが、暫定的に芸術と言ってしまう）実践でもある。

この大きな問題意識のもと、筆者の音楽プログラミング言語制作の目標はその中で二転三転を経てmimiumという現在のプロジェクトに落ち着く。
始めにある程度まとまった人数に対して言語設計の話を持ち出したのは、SFPC滞在期間中の自己紹介兼、3ヶ月の間に何をやろうとしているかのプレゼンテーションをする場所だった[^meetthestudent]。そこでは、1つのソースコードに対して複数の処理系を通すことで異なる音楽が発生するという、どちらかといえばCode Poetry[^CodePoetry]などの詩としてのソースコードとその処理系の関係性、コードを書く人と処理系を作る人のオーサーシップの関係性に焦点を当てたものだった。結局SFPCの滞在期間中に制作したのはよりハードウェアのレイヤーでコンピューターの構造と音楽という時間芸術の形式の関係性を問い直すものになり[^edtac]、言語設計を実際に始めるのはもう少し後になる。

[^meetthestudent]: SFPC 2018 fall classにおけるMeet the Studentsという公開イベント。YoutubeのURLとSpeakerDeckのスライドを貼る

[^CodePoetry]: プログラミング言語のソースコード自体を詩のように扱う芸術の形式。実際には実行できないがソースコードの見た目を模したもの、実行することで初めて詩として読めるもの、ソースコードとしても可読性があり、実行することでさらにグラフィックを生成するものなど様々な種類がある。[@Montfort2013]などを参照。

[^edtac]: Electronic Delay Time Automatic Calculator(EDTAC)という作品。修士論文[@Matsuura2019mathesis]や2019年JSSA研究会での[@Matsuura2019jssa]を参照。

2019年ごろの設計初期段階では、Mayer、ChughらのSketch-n-Sketch[@Mayer2018]や、JacobsらのPara[@Jacobs2017]、橋本麦によるGlisp(NiU)[^Glisp]など、グラフィック生成のためのアプリケーションにおいて、ソースコードの編集をDirect Manipulation(Adobe IllustlatorやMicrosoft PowerPointにおけるベクター図形編集のような、図形の要素をGUIで直接サイズ調整できるような種類のアプリケーション)と組み合わせたり、相互に行き来できるソフトウェアを、音楽プログラミングの領域においても実現できないかというアイデアが中心に置かれていた。

[^Glisp]: https://glisp.app。なお、橋本もSFPCへの滞在経験があるほか、Jacobsは筆者の滞在期間中にフィールドワークの一環でSFPCを訪れている。

実際、この提案は2019年情報処理推進機構未踏IT人材発掘・育成事業という、革新的なソフトウェア制作に対する支援事業に採択され、その当初のアイデアはプログラミング言語設計とそのソースコードをグラフィカルに編集できるソフトウェアという2つのプロジェクトを並列して行うものになっていた。[^mitou]

[^mitou]: 未踏IT人材発掘・育成事業：2019年度採択プロジェクト概要（松浦PJ）(https://www.ipa.go.jp/jinzai/mitou/2019/gaiyou_tk-1.html)を参照。また同時期に情報処理学会音声情報処理研究会にて同様の案についてのポスター発表を行っている[@Matsuura2019MUS]。

結果として、2019年6月から2020年2月までの同事業でのディスカッションを中心に実装を進める中で、実装内容はその根幹となる、音を生成するプログラミング言語一本に絞っていくことになった。その理由は主に、筆者がプログラミング言語実装そのものの領域に対しては初心者であったが故に実装にかかる時間的な都合の問題が主な理由であった。言い訳がましく聞こえるかもしれないがこれは同事業で言語設計、実装を進めた中での大きな収穫の一つだと考えている。つまり、**音楽プログラミング言語を設計したり実装することは、想像しているよりもずっと難しかった**という、本論文の1つのテーマでもある、なぜ音楽プログラミング言語を作ることのハードルが高いのかという疑問に答えるための材料を身を持って入手したわけである。

# 本論文の貢献

本論文を通じて学術的に貢献する内容は、大きく分けて3つである。

1つは、"音楽プログラミング言語とは何か？"という、音楽プログラミング言語の存在論の提示だ。音楽プログラミング言語は歴史的にコンピューターを用いて音楽を生成するための技術としてスタートしつつも汎用プログラミング言語の理論を取り込んで発展してきたことにより、一重に音楽のためのプログラミング言語/環境といってもその応用範囲や想定される使用方法、さらに内部の実装方法までが多岐に及び、単純な比較をすることが難しい。また、評価のための語彙も "表現力が高い" "効率的" "汎用的"などの言葉が共通認識の無いまま慣例的に使われており、実際に何を意味するかもはっきりしないことがある。本稿ではまず音楽プログラミング言語の歴史的変遷を改めて整理した上で、"音楽プログラミング言語にはどんな種類があるのか"、"音楽プログラミング言語の特性はどう記述できるか"といった概念を整理して提示する。

もう1つは、"音楽プログラミング言語を音楽を設計するという行為とは何か？"　録音技術（音響再生産技術）に端を欲した音楽のフォーマットは現在空間音響技術のためのフォーマットの普及などよって、原音再現という従来の明確な一つの目標を失いつつあるだけでなく、新しさを謳いながら一方でインフラストラクチャの性質による音楽の形式を画一化する傾向を持っている。その環境でコンピュータ上の表現の自由度を最大限担保するためにはプログラミング言語そのものを音楽のためのフォーマットすることが必要になってくる。この背景から、音楽プログラミング言語を設計することは2020年代以降における、テクノロジーを主体的に使う音楽実践の1つのあり方に位置付けることを試みる。さらにこれは、デザイン学におけるデザイン実践を通した研究の領域の広がる中で、社会構造自体を長い時間をかけて変化させていくことを試みる動きとも呼応する。これまでデザイン学の分野の研究はHumam-Computer Interactionの研究法の1つとして狭い意味で音楽プログラミング言語にも適用されてきたが、本研究ではプログラミング言語設計という行為を、誰もがアクセス可能な音楽のためのテクノロジーの変革のための政治的行動という広い領域へ接続する。

3つ目は、筆者が設計/実装した自己拡張性の高い音楽プログラミング言語"mimium"についての設計思想とその具体的な実装を提示した上で、以上2つの観点からの位置付けを試みることだ。mimiumは先述した音楽のためのプログラミング言語の歴史を見直し、汎用プログラミング言語の設計の上に最低限の音楽のために特化した言語機能を備える構造を取ることで音楽のための言語におけるブラックボックスを減らしながらもその実装の単純さと自己拡張性の高さを同時に実現できるように設計されている。


# 構成

![本書で提示する音楽プログラミングの歴史の概要。](img/history_overview.png){#fig:history width=100%}


第2章以降は以下のような構成で論じる。本論文全体の構造を[@fig:history]に示した(変更予定)。

第3章では、デザイン実践を通じた研究（Research through Design:Rtd）という、科学とは異なる方法論で人工物を生み出すことにより学術知に貢献する方法のあり方を、プログラミング言語という研究領域にどう適用できるか検討する。HCIにおけるRtD研究の価値とプログラミング言語研究における主張と根拠の整合性という指標を参照し、人工言語というアウトプットを修飾するものとして主張と根拠がそれぞれ独立して存在し、根拠の部分は工学研究同様に客観的に評価が可能であり、全体としては個別の研究の差異を持って一般化されない理論を見出すような研究領域と位置づける。またMagnussonの認識論的道具[@Magnusson2009]概念を参照し、RtDがその研究領域の対象を広げる状況の一例として、プログラミング言語設計は音楽テクノロジーのインフラストラクチャを長い時間をかけてデザインする行為の一つだと位置付ける。

第3章では、現在のテクノロジー環境における音楽家の実践のあり方として、2000年代までは機能していたサーキットベンディングやグリッチのようなアマチュアリズムを伴う意図的な技術の誤用が今日のソフトウェア中心の音楽技術文化の中ではもはや機能しないことを示す。技術が不可知なものとしてアクセス不能になっていく状況に対抗する手段として、技術を深く理解した上で1.パフォーマンスとして提示する、2.技術者の視点に鑑賞者を引き寄せる、3.異なるインフラストラクチャを作り始めるという異なるアプローチを比較し、PLfMの設計を広義の音楽実践として捉え直す。



第4章ではPLfMの歴史を既存の文献を基に、前章で概観した音楽インフラストラクチャと計算機科学という大きな二つの歴史に挟まれた存在として改めて位置付ける。本稿では歴史を50年代、70年代、90年代以降に大きく3分割し、それぞれがラボの中のコンピューター音楽研究、チップチューンの時代、汎用プログラミング言語の理論との合流というトピックに対応する。この歴史の中で今日のPLfMとは数多存在するインターフェースの中から敢えて言語という形式とその特性を活かした音楽ソフトウェアと位置付けられることを示す。

第5章ではPLfMの現代における特性や概念を明確にする。そのために、PLfMの実装の違いをドメイン固有言語のデザインパターン、汎用プログラミング言語における評価語彙の研究を参照しつつ、PLfMを使用するプロセスをHuman-in-the-Loopモデルとして提示し、ランタイムのブラックボックスを大きくすれば動的変更に強くなる、小さくすれば表現自体の汎用性が高まる、動的変更と汎用性を両立しようとすると設計が複雑化し実装のコストが嵩むといったトレードオフが存在することを提示する。

第6章では筆者が設計した音楽のためのプログラミング言語mimium[@Matsuura2021]の詳細を記述する。まず第2〜4章で示してきた背景を総括し、1.ブラックボックスを可能な限り減らす,2.新しい表現を生むことを目的としない, 3.インフラストラクチャとしてのプログラミング言語の必要性という3つの設計方針を示す。mimiumは型推論を伴う静的型付け言語であり、汎用の関数型プログラミング言語の設計や実装に、音楽のための言語仕様を最小限追加する形で実装されている。1つは関数の実行のスケジューリングを行う$@$演算子、もう1つは内部状態を伴う信号処理を通常の関数と同じよう記述可能な、状態付き関数の記法である。mimiumはこの2種類の記法を持つことで、これまではブラックボックスとして与えられていた基本的処理の単位をライブラリとして(実行性能を損なわないまま)実装可能であることを、既存の言語との比較を交えて示す。

第7章では、mimiumの現状における実装の問題点を基点にして、PLfMの設計上存在するトレードオフの選択、PLfMの歴史におけるmimiumの位置付け、RtDとしてのmimium設計、音楽実践の1つとしてのプログラミング言語実装という視点で制作を分析する。この中でPLfM制作においてブラックボックスの少ない言語を作る方針を突き詰めると、ホスト言語の実装とその上でのライブラリ構築という2種類の作業へ分岐することが示唆された。そのため、結果的にホスト言語の実装には音楽に関わる作業が少なくなるという分断が発生してしまうという矛盾も明らかになった。

最終章では、前半で示したPLfMの存在論、歴史と後半で示したmimiumの実装とその実装過程の分析をより一般化し、音楽とテクノロジーの狭間で行われる非常に個人的な動機からの問題設定を基に始めたもの道具づくりを、多くの人間が共有して使うものへと開いていく研究領域を「音楽土木工学」として、近年の音楽情報処理研究との対比や計算機科学の動向交えて検討する。今後の研究課題として、音楽という応用領域の中では背景化されてきたオペレーティングシステムやコンピューターアーキテクチャといった計算機の根幹に関わる理論自体の見直しを行うことで、mimiumの実装に伴って明らかになった問題の解決の糸口になることを示す。