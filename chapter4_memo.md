
音楽のためのプログラミング言語は既存の文献では、Computer Music Language[@McCartney2002;@Mcpherson2020]、Language for Computer Music[@Dannenberg2018]、Computer Music Programming Systems[@Lazzarini2013]などの呼ばれ方がされているが、それぞれの語の使用に明確なコンセンサスがあるわけではない。その中でも敢えて筆者がComputerという語を使わない理由のひとつは、Computer Musicという語が、コンピューターを用いることで新しい音楽表現を追求する歴史的な取り組みの中にある、特定の音楽様式と結びついてしまうことを避けるためだ。既に述べたように、今日ではあらゆる音楽制作と再生のためにコンピューターが用いられている以上、あらゆる音楽が**弱い意味でのComputer Music**と呼ぶことができる。しかしそれらの多くはコンピューターでなければ不可能な、コンピューターというメディア固有の表現を行っているわけではない。同様に、たとえばFaust[@Orlarey2004]のような、信号処理のアルゴリズムを抽象化することに特化したプログラミング言語は新しい音楽表現を必ずしも目的としていないが、その技術的要素の多くはComputer Musicのための言語と共通するところがある。またPLfMという枠組みを用いることで、これまでの文献では比較対象に入れられること自体が少なかった、MML:Music Macro Languageのような、五線譜上の記法を直接的にテキストに置き換えたような、単にコンピューター上のテキストというフォーマットで音楽を表すことを目的とした言語たちも、チップチューンのような広い意味でのコンピューターを用いた音楽文化を作るための要素として議論の土台にあげることができる。

加えて、Programming EnvironmentやProgramming Systemといった語を用いない理由も説明しておこう。これは、音楽のためのプログラミング言語といった時に、たとえばMaxのような、ある特定のアプリケーションを想像するニュアンスを抑えるための選択だ。たとえば、汎用プログラミング言語の理論においては、プログラミング言語、と言った時にはその言語を実行するためのソフトウェアやプログラムのことを必ずしも指していない。たとえば同じC++という言語であったとしても、それを実行するソフトウェア（コンパイラ）はGCC、Clang、Microsoft Visual C++といったように複数存在し得るからだ。これらのコンパイラは、どれもC++の厳格な言語仕様で定まっている通りの動作をするが、言語仕様で未定義とされてる動作はそれぞれ異なるし、コンパイラが出力する実行バイナリ（≒アプリケーション）の中身は同じソースコードだったとしても異なる。音楽プログラミング言語においては、基本的にある言語＝特定のアプリケーションであることがほとんどだが、根本的にはアプリケーションの設計実装という作業とプログラミング言語の設計実装という作業は異なり、本研究が対象にしたいのは言語の設計なのだ。こうしたニュアンスを込めて筆者はEnvironmentやSystemという語を用いないことにした。極論を言えば、Faustのような厳密に意味論が定義されている言語においては、コンピューターを用いなくてもそのソースコードを手作業で解釈し実行することが可能だということを考えれば、プログラミング言語はコンピューターを使うための道具であることは間違いないにせよ、人間が直接的にバイナリを操りプログラムを構築するには限界があるという理由で開発されているという意味で、逆説的に徹頭徹尾人間のための道具でしかない。だからComputer Music Languageとも、Computer Programming Languageとも、呼ばずに、ただ音楽のためのプログラミング言語：Programming Language for Music、PLfMなのだ。

1970年代の区切りは2章で見てきたコンピューターをメタメディア装置として扱う思想の始まりと、パーソナルコンピューターの登場という2種類の出来事である。

そもそも音楽プログラミング言語の祖先となるソフトウェアMUSICが開発されたのは正解で最初の(汎用)プログラミング言語FORTRANが作られた翌年であり、当然1章で見たアラン・ケイらによる対話的プログラミング環境や豊富な入出力インターフェースを備えるよりもずっと前のことである。
なので、必然的に音楽ソフトウェアのプログラミング自体も機械語を直接入力するかアセンブリ言語(機械語の命令をテキストと1対1対応させたプリミティブなプログラミング言語のようなもの)しかなかったし、そのソフトウェアに対する入力データ（≒楽譜）も同様の形式を取らざるを得ないものだった。

つまり、1950〜1970年代の音楽プログラミング環境は大まかにいってコンピューターで音楽を作るためのソフトウェア全般の祖先にあたるものであって、必ずしもプログラミングという行為やテキスト入力という形式の固有性を積極的に取り入れたものではない、ということだ。

逆に、70年代以降の音楽プログラミング言語/環境はマウスや(文字入力や、ピアノ鍵盤どちらにせよ)キーボード入力といった直感的（WYSIWYG的）なインターフェースが選択肢として存在する中で敢えてプログラミングという手段を使うものとして設計されてきた、という違いがあると言えるだろう。プログラミング環境であっても、GUIの誕生はMax(Puckette)を代表としてテキストインターフェースだけでなく、入出力を持つボックスをマウスで繋いでいくような形式など、テキストに留まらない形式でのプログラミング行為を可能にした。これは同時に、出力された信号などもオシロスコープのようなグラフィックとしてフィードバックが返ってきたり、パッチ（Maxにおけるプログラムのこと）中にスライダーのような、プログラムされたソフトウェアを操作するためのインターフェースが同居していたりといった、それまで存在していた "プログラムを構築するステップ"と "構築されたプログラムを使用するステップ"に明確な境目が無くなっていく歴史でもある。


また、1990年代の区切りは、パーソナルコンピューターが専用のサウンドチップなしに、CPUだけで音声信号処理をリアルタイムで行えるようになったこと、そして汎用プログラミング言語の理論が音楽向けの言語にも流入し始めたことの2種類である。


コンピューターアーキテクチャのスタンダード（もっと言ってしまえば、x86アーキテクチャ）が定まるまでのプログラミングは、特定のハードウェアのための特定のプログラムを作るという側面が大きく、書かれたソフトウェアが様々なプラットフォームで使い回しが効くということでもなかったことも頭に入れておくべきだろう。汎用プログラミング言語はそれまでの実在するハードウェアに対する命令列を可読性のあるテキストデータから出力するためのソフトウェアという側面だけでなく、ラムダ計算（引用）のような、計算過程自体を数学的になるべく普遍的になるように記述する代数学の理論との接続を見せるようになり、LISPやML、Haskellに代表されるような関数型プログラミング言語のパラダイムが発生してきた。
そしてこうした分野で培われたプログラミング言語の理論は現在ではFaustやKronosを代表とする、関数型でかつ音楽や音声処理のための言語の理論的基盤としても用いられるようになっている。


つまり2020年代現在において、本論文が定義する音楽プログラミング言語とは、**コンピューターを用いて音楽を生成するためのソフトウェア群に始まりつつも、並行して発展してきた汎用プログラミング言語やその理論を取り込みつつ発展してきたソフトウェアやツール**のことを指す。なので、Maxのように前者の流れを強く汲むものは、Dannenbergが言うように、言語体系とランタイムやライブラリ、開発/実行環境があらかじめ切り離せない形式(＝実装そのものが仕様)となっていることが多い。
逆に、汎用プログラミング言語の理論をベースに構築された言語、例えばExtemporeやFaust、Kronosでは、言語仕様は言語仕様として独立しておりランタイムが存在しないーあるいは複数のランタイムの実装があり得る、そのほか、決まったIDEが存在しなかったり、複数の開発/実行環境が存在するといった構成になっているものがある。


音のなるおもちゃの電子回路を改造しておかしな音が出る楽器へと変えてしまうサーキット・ベンディング文化の原点の1人として捉えられる、音楽家のミシェル・ワイシュヴィッツは、1970年代にCrackleboxという、たったひとつのオペアンプ（演算増幅器）の端子を直接身体で触りフィードバックと発振を起こすことで予測不能な音を出す楽器を作った。彼はその楽器の思想の原点に子供の頃のラジオの蓋を開け、回路を直接触ることで音を変化させて遊んでいるエピソードを挙げている。


---

# Unit Generatorとモジュラーシンセサイザーの関係性

MUSICシリーズにおいて触れておくべきことは、MUSIC IIIにおいてはじめて**Unit Generator**と呼ばれる、今日まで用いられる概念が登場したことである。Unit Generator(UGen)とは、簡単にいえば正弦波や三角波、矩形波などの発振器や、各種フィルターといった信号処理における基礎単位を抽象化したものである。

これは


、MUSIC IVにおいてはじめてそれ自体の実装が汎用プログラミング言語(FORTRAN)で実装されたことである。

まず、Unit Generatorとは〜

---

# インターミッション–1970年代 {#sec:intermission}

MUSIGOLの背景にもあったように、1960年代後半から1970年代にかけて、MUSIC Nシリーズは様々な派生系がアメリカ各地の大学で実装されるようになった。しかし、ここから現代のPLfMまでの繋がりを考えるためには、これらの派生系の系譜を追うだけでは不十分である。なぜなら、MUSIC Nの派生が多数作られた要因の一つが、トランジスタを用いたこれまでより小型なコンピューターが設置されることにあったように、電子計算機の産業化が研究にも如実に影響を及ぼすようになってきたからである。例えばDigital Equipment Corporation（DEC）社が発売したPDP-10は、スタンフォード大学人工知能研究所（SAIL）でMUS10（MUSIC 10とも呼ばれる、1966年）の開発に、PDP-11は現在まで使われるCsoundの原型であるMUSIC-11（1973年）の開発に用いられた。

そこで1970年代に関しては遠回りになるが、今一度この時代の背景を、2章や3章で見てきた内容に重ねて考えてみる必要がある。まず第2章で示したとおり、1970年代はサイバネティクスの影響を受けたデザインサイエンス的運動からデザイン思考のような反省的実践の兆しが現れる移行期である。

特に1970年は大阪万博での鉄鋼館スペース・シアターでスピーカーを1000個以上使った音楽作品が作られたり、ペプシ館でのExperiments in Art and Technology(E.A.T)の展示やパフォーマンスが行われるなど、芸術とテクノロジーの単純なコラボレーションとしては一時期のピークとも言える年である。しかし、コンピューターと音楽の関係性により焦点を当てると、クラシック音楽に端を欲した音楽実践で、テクノロジーを取り入れることを試みてきた中にも実は2つの対立する方向性が存在している。フランスの国立電子音楽研究所IRCAMでのフィールドワークを1984年に行なった、文化人類学者のジョージナ・ボーンはその違いをこのように説明する[@Born1995,p3]。

ひとつは、無調音楽を発展させ、十二音技法と呼ばれる技法を生んだアルノルト・シェーンベルクに連なる、厳密で規則化された音価の制御を指向する、普遍主義、科学主義的モダニストである。十二音技法はさらに、音程以外の様々なパラメータもルール付けするトータル・セリエリズムに発展していく。この系列には例えばのちにフランスの電子音楽研究所でIRCAMの音楽ディレクターに就任する作曲家・指揮者のピエール・ブーレーズ、アメリカのプリンストン大学で、RCA Mark IIなど初期の大型シンセサイザーを用いて作曲を行なったミルトン・バビットなどが挙げられる。反対には、ともにシェーンベルクらに影響を受けながらも、音楽の概念自体の脱構築（図形楽譜、演奏より聴取にフォーカスする、不確定性、日常への介入、etc…）を試みた、ジョン・ケージに連なるポストモダニストたちがいた。

この態度の違いは、セリエリストたちが作る難解な音楽が大衆の理解から遠ざかることに対する問題意識への違いと、芸術家の組織的援助への態度として現れた。特に、ミルトン・バビットが1958年に書いたエッセイ、「専門家としての音楽家」（原題：「人が聞くかどうかなど気にする必要があるか？」[^takaoka]）[@Babbitt2011]では、 "数学者の研究が大学によって支えられているように、難解で複雑な音楽の創作も大学によって保障されるべきである"と、音楽の専門知と、大学のような研究機関の関係性に対する態度をはっきりと示し物議を醸した。

[^takaoka]: 引用部分も含め訳出は[@Takoka2011]を参考にした。

一方、ケージの立場に近い人間も、十二音技法や単なる楽音に留まらないオルタナティブな表現を追求しつつもどこか技術決定論的な、テクノロジーが新しい音楽を生み出してくれるという期待を持っており、それは結果的に、大学や国家ではなく大企業と資本主義という異なる形での権威との付き合い方への見直しを迫られることになっていく。そもそもケージが無音の《4分33秒》にたどり着いたのも、ハーバード大学の無響室を体験したことで完全な無音など存在しないと気づかされたことがひとつのきっかけであったし、その《4分33秒》の初演のピアニスト（実際には何もしないのが演奏なので鍵盤を弾くことはないだが）で、ケージと長く付き合いのあるデヴィッド・チュードアは自らの手で様々な電子回路を組み音楽を作る作曲家でもあった。そして何よりこうした、1960年代におけるアーティストとテクノロジーのコラボレーションを推し進めるのに一役買っていたの、E.A.Tを立ち上げたビリー・クルーヴァーは、他ならぬMUSIC Nが開発されていたベル研究所のエンジニアだった。







トランジスタの登場などにより電子部品が低価格しアーティストでも利用可能になった


E.A.T

The use and the development of software involve the writing of coded instructions within a software language or the creation of a completely new language, within the context of a hierarchy of such languages. At each of the hierarchy a traslation occurs between any two adjacent language or levels of code. Instructions from the language at a higher level mus be translated into aform whereby they can be "read" and executed by the lower-level code or language without any (or with minimal) loss of "meaning"

The hierarcy of codes that normally operates in computer software include, at the lowest level, machine code, the instructions that drive the hardware, written in binary form; at the next level up, assembler code, made of mnemonic abbreviations of machine code; above this, the general operating system that provides a basic framework and set of servicies; and above this, any of the major lkanguages such as FORTRAN, Pascal, C, or LISP.

リアルタイムで処理できない程度に重かったこと
楽器のモデルを計算できる理論が存在しなかったこと→Rissetに始まる物理モデリング合成研究（Analysis-Synthesisアプローチ）

80年代にしては計算のシンプルさに対して複雑な音色が出せるFM合成とかはあったが、コンピューター音楽言語は大学や研究所に限られていたし、リアルタイム性にもまだ欠けていた extreme mediation, both temporal and conceptual

モデルがしっかりしていないと音を改善することができないけど、トライ&エラーに時間がかかるのでモデルの妥当性を確かめるのにも時間がかかってしまうパラドックス


技術のトリクルダウン：研究所でパイオニア的テクノロジーが発達し、それがコマーシャルに低価格化していくという考え方をIRCAMは持っていた
一方で、マシューズのようなインターフェース系のを拒否し、かなり基礎的な部分に投資するという矛盾

その上で、当時出てきたばかりのMacintoshの登場や、コンピューターアーキテクチャの発展にかなり振り回されている


ヤマハの人がCXのデモにIRCAMにきた話

4xは当時最強のスペックだった→これ何がそうさせたんだろう？オシレーターとかはソフトウェアで仮想化できたんだろうか？西野の文献読む必要あり→できた。Variable Digital Signal Processorの話

OSから作ってた　ハードは凄かったがソフトとペリフェラルが弱い

Chant 歌声合成、Formes PatchworkとOpenMusicの手前

Chant／Formesのグループからは、音楽概念の高度な発達というコンピューターのポテンシャルを無視していると思われてた

Chant/FormesはLISP製、VAX/UNIXシステムで動いていたノンリアルタイムシステム
    users could create their own "personalized environment"
    object oriented



〜〜

Computer music software such as that used and produced by IRCAM adds yet a further level of mediation, hierarchy, and translation, sincce the music languages are themselvels based upon, or written in, established general languages.

Thus, Music V is written in FORTRAN, Cmusic in C, IRCAM's Chant in FORTRAN, and Formes in LISP.


Chantを使うにはFORTRANの知識も必要だったし、Formesを使うにはLISPの知識も必要だったので、それを勉強するためにまずLISPについて勉強しなくてはならない →**つまりこの時点ではまだ、後にMcCartneyがSuperColliderの設計指針として挙げる、プログラミングというコンピューターハードウェアを使うための専門的知識が必要な事項をencapsulateし、音楽のNotationに集中させるという意味での音楽”言語”の概念は達成されていなかったということが言えるだろう。**
