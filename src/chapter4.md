# 音楽のためのプログラミング言語とはなんなのか？

> コンピューターは、作曲家を補助するために、あるいは補助なしに作曲するために、"器楽的な"音楽を演奏するようにプログラムできる。[@Mathews1963,筆者訳]


本章では音楽のためのプログラミング言語（Programming Language for Music：PLfM）の存在論の通時的な整理を行う。

## PLfMの定義

ここで初めて、ここまでは字義通りの定義に留めて暗黙的に用いてきたPLfMという用語をより明確に定義することにする。音楽のためのプログラミング言語は既存の文献では、Computer Music Language[@McCartney2002;@Mcpherson2020]、Language for Computer Music[@Dannenberg2018]、Computer Music Programming Systems[@Lazzarini2013]などの呼ばれ方がされているが、それぞれの語の使用に明確なコンセンサスがあるわけではない。

とはいえ、なぜ既存の語を捨ててまでPLfMという新しい語を用いる必要があるのだろうか。これには少なくとも3つの理由がある。

1つ目は、Computer Musicという語を用いることによって、そのための道具が、コンピューターを用いることで新しい音楽表現を追求するというイデオロギーを持つ、特定の音楽様式と結びついてしまうことを避けるためだ。今日ではあらゆる音楽制作と再生のためにコンピューターが用いられている以上、あらゆる音楽が**弱い意味でのComputer Music**と呼ぶことができる。しかしそれらの多くはコンピューターでなければ不可能な、コンピューターというメディア固有の表現を行っているわけではない。同様に、たとえばFaust[@Orlarey2004]のような、信号処理のアルゴリズムを抽象化することに特化したプログラミング言語は新しい音楽表現を必ずしも目的としていないが、その技術的要素の多くはComputer Musicのための言語と共通するところがある。またそれらのコンピューター音楽の文脈とは、3章で解説した、Macintosh以後のユーザーの誕生以前から行われてきた取り組みであることを頭に入れる必要がある。つまり、プログラミングという作業がユーザーから隠蔽される以前の1950〜1970年代におけるコンピューターの利用することはほぼ必然的にプログラミングを行うことでもあった。それゆえ、1990年代以降に現れてきた、パーソナルコンピューティング環境で使うことを想定された言語とは、コンピューターを使うためのインターフェースの中でよりユーザーフレンドリーで直感的な選択肢があるにも関わらず、それでもなお人工言語という形を選んでいるという面が強調されて然るべきである。

2つ目はPLfMという枠組みを用いることで、これまでの文献では比較対象に入れられること自体が少なかった、MML:Music Macro Languageのような言語を、五線譜上の記法を直接的にテキストに置き換えたような、単にコンピューター上のテキストというフォーマットで音楽を表すことを目的とした言語たちも、チップチューンのような広い意味でのコンピューターを用いた音楽文化を作るための要素として議論の土台にあげることができる。

3つ目は、Computer Musicという語を避けたのと同じように、Programming EnvironmentやProgramming Systemといった語を避ける理由である。これは、音楽のためのプログラミング言語といった時に、たとえばMaxのような、ある特定のアプリケーションを想像するニュアンスを抑えるための選択だ。たとえば、汎用プログラミング言語の理論においては、プログラミング言語、と言った時にはその言語を実行するためのソフトウェアやプログラムのことを必ずしも指していない。たとえば同じC++という言語であったとしても、それを実行するソフトウェア（コンパイラあるいはインタプリタ）はGCC、Clang、Microsoft Visual C++といったように複数存在し得るからだ。これらのコンパイラは、どれもC++の厳格な言語仕様で定まっている通りの動作をするが、言語仕様で未定義とされてる動作はそれぞれ異なるし、コンパイラが出力する実行バイナリ（≒アプリケーション）の中身は同じソースコードだったとしても異なる。音楽プログラミング言語においては、基本的にある言語＝特定のアプリケーションであることがほとんどだが、根本的にはアプリケーションの設計実装という作業とプログラミング言語の設計実装という作業は異なり、本研究が対象にしたいのは言語（Language）の設計なのだ。こうしたニュアンスを込めて筆者はEnvironmentやSystemという語を用いないことにした。

極論を言えば、Faustのような厳密に意味論が定義されている言語においては、コンピューターを用いなくてもそのソースコードを手作業で解釈し実行することが可能だということを考えれば、プログラミング言語はコンピューターを使うための道具であることは間違いないにせよ、徹頭徹尾人間のための道具でしかない。だからComputer Music Languageとも、Computer Music Programming Environmentとも、呼ばずに、ただ音楽のためのプログラミング言語：Programming Language for Music、PLfMなのだ。

## PLfMに関連する既存のサーベイ

まずは、既存の音楽のためのプログラミング言語に関連したサーベイについてを列挙しておく。

既存の音楽のためのプログラミング言語や、コンピューター音楽のためのシステムのサーベイとしては[@Roads2001]や[@Nishino2016]、[@Dannenberg2018]が存在する[^authorsinfo]。ローズによるコンピューター音楽の手法を網羅的に記述した著書では、（原題：Computer Music Tutorial）初期のコンピューター音楽の試みが過去のインタビュー[@Roads1980]なども踏まえて記述されている。西野と中津のサーベイではこれよりも詳しく、1940年代から2010年代まで時系列順でコンピューターを用いた音、音楽生成の試みの歴史が網羅的に記述されている。ダネンバーグの文献は西野同様歴史的に代表的な言語の紹介と、より言語の特性や分類について構成的な整理が行われている（これは主に第5章で足掛かりとする部分である）。

[^authorsinfo]:なお、西野はLC[@Nishino2014]の、ダネンバーグはNyquist[@Dannenberg1997]他多数の言語の設計者でもある。このことからもやはり、音楽プログラミング言語の歴史を記述するには基本的にその設計や実装に関する知識や経験が必要になっていることが窺える。


またメディア考古学的検証を行う本研究において重要な1940〜1960年代までのコンピューター音楽黎明期歴史の整理が詳しくなされている文献として、オーストラリアにおける初期のコンピューターを用いた音楽生成の試みを研究したポール・ドーンブッシュの調査がある[@Doornbusch2005]。この調査を契機として2005年以降、イギリスの放送局BBCやアーティストのirrlicht projectにより最古のコンピューター音楽生成の検証が進んだ[@Fildes2008][@Irrlicht2015]。これらの調査はのちに日本において、田中がチップチューン（1980年代の初期パーソナルコンピューターやゲーム機において用いられていた音声生成用のICチップを利用した音楽ジャンル）の歴史的起源として位置付けることで、改めてまとめられている[@Tanaka2017][^chiptune]。またドーンブッシュ自身もこれらの検証を踏まえ改めて黎明期の歴史を整理し、アメリカ中心的コンピューター音楽の歴史観を批判的検証を行った[@Doornbusch2017]。

[^chiptune]: 本研究内であまり触れられなかったが、チップチューン自体は日本での独自の発達を遂げたこともあり、田中も記述するように、いわゆるコンピューター音楽の歴史としては傍流の歴史として扱われていもるが、その文化の中から生まれたMML（Music Macro Language）と呼ばれる、テキストで音楽の内容を記述するための言語の系列は、そのハードウェアやプラットフォームごとに仕様が異なる派生の複雑さや、ゲーム音楽研究との接点、その後のTakt[@Nishimura2014]のような、より汎用プログラミング言語の理論へ近づいた言語など各方面に影響を与えていることを考慮すれば、音楽のためのプログラミング言語の歴史研究や、より今日正統な発展を遂げたと思われている音楽文化との関連性の批判的検証が必要だと考えられる。チップチューンをメディア論の視点から検証している近年の研究には[@Hidaka2021]がある。

本研究ではこれらの研究を参照しつつも、3章で見てきたメディア装置としてのコンピューターと、パーソナルコンピューティングの歴史との対応関係から、時代ごとの取り組みの焦点の変化をより明確に描き出すことを試みる。

## PLfM史概略

そのために本章では、PLfMの歴史的推移の大枠を捉えるために3つの視点での区分を導入する（1.2.は西野らのサーベイで使われていた観点である）。

1. **リアルタイム性**：一度結果を書き込んでから読み出すのではなく、実時間で音声信号の処理ができるかどうか。（そもそも音声信号処理をしない、というのも含め)
2. **可変DSP**：音声合成のためのモジュールが有限か、（理論上）ソフトウェアで無制限に利用できるか。
3. **Lab or PC**：研究所における取り組みかパーソナルコンピューティングか

これらの取り組みが歴史的にそれぞれ異なる方向性の試みが重なるように行われてきた。2、3章同様に、歴史の見取り図を[@fig:plfmhistory]に示す。

![PLfMの歴史を、リアルタイム性、可変DSP、Lab or PCの視点で分類した概略。](./img/plfmhistory.pdf){#fig:plfmhistory width=100%}

1950〜1970年代の計算機自体の黎明期は全ての信号処理をCPU上で行うと、例えば10秒の音声信号の計算結果を得るのに10秒以上かかってしまう状況だった。そのため、研究内容は例え非リアルタイムだとしてもソフトウェア上で音声を計算して合成する可変DSPを重視した試みと、それよりも、たとえ限られた範囲での合成しかできなくともリアルタイムインタラクションを重視する試みという、トレードオフの選択があった。この両立は研究所レベルでは1984年のIRCAM 4Xというワークステーションによって成されたというのが一般的な認識だ。

そして、行われていた場所が大学などのラボラトリーか個人かという違いも重要な点である。すでに述べたようにMacintosh以後のパーソナルコンピューティングこそがプログラミングという作業をコンピューターの利用から分離したからである。1980年代以降のパーソナルコンピューター黎明期は、個人向けの低価格コンピューターにおける計算機リソースという制限から、1970年代までのラボラトリーにおける状況が繰り返されていた。つまり、音声合成ICによる限られた範囲での音楽制作の試みである、チップチューンがこの部分に当てはまる。

1990年代に入って以降は、パーソナルコンピューターでも専用の音声合成ICなしにリアルタイムの信号処理が可能な程度に性能向上と低価格化が進み、SuperColliderや（OpCode社によって商用化された）Max、Puredataなどよりハードウェアに依存しない形式のPLfMが登場した。

2000年代以降はさらに3つの流れが発生する。1つはより**正確なイベントスケジューリング**である。これは、専用のICを用いず音声合成をすることによって、処理タイミングがOSのスケジューラーに依存するようになり複雑さと不確定性が増したことと、この頃登場したmicrosoundのような、音声データを細かく切り刻み再配置するような手法に正確なタイミング制御が求められたことという、技術的背景と表現からの要求の両側面がある。またこの時期、汎用プログラミング言語のためのツールや理論が発展することによって、音楽のための言語にもその技術が流入してくることになった。その結果、Faustに代表される、これまでブラックボックスとして隠蔽されていた信号処理のプリミティブな部分をハードウェアに依存せず記述可能な、**低レイヤー方向の拡張**を目的とした言語の登場につながった。また同時に、この時期登場してきたRubyやHaskell、Pythonなど様々な汎用性の高い言語をフロントエンドとして用い、SuperColliderを音声合成エンジンとしてだけ利用することで、言語の意味論をこれまでのPLfMと大きく変える、**オルタナティブな抽象化**を試みる言語も多く現れることになる。

本章において注意して読んで欲しいのは、時代ごとの変遷の見通しをよくするために3つの視点の区分を導入してはいるものの、それらはむしろ、キッパリと区分できない境界線のにじみを強調するために用いているということだ。例えば、計算リソースが限られた時代におけるリアルタイム性と可変DSPのトレードオフは、そのどちらに重きを置いていたかという、各システムのスタンスを理解しやすくするが、現実には微妙な例がいくつもある。これには例えばチップチューンで用いられるテクニックのことを考えると理解しやすくなる。

初期のチップチューンはPSG（Programmable Sound Generator）という、CPUからの命令で、3~4種類の音程や発音タイミング、エンベロープ（音量変化）の形を制御できるシンセサイザーを用いることで作られる音楽だった。しかし、エンベロープによる音量調節をCPU側から、つまりソフトウェアの命令で非常に細かく制御してしまうことで、例えば矩形波しか出せないようなチップでも疑似的に三角波やノコギリ波に近い波形を生み出せる[@Tanaka2017,p73]。この疑似三角波は純粋な三角波とも異なる音色を持つため今やチップチューンらしさを象徴する音色でもある[^magical8bit]。この方法論はさらに突き詰めると、音量制御を細かく制御することで任意の音声波形を再生するというテクニックにもつながる[@Holst1998]。このように、リアルタイム性と可変DSPのトレードオフは、コンピューターが理論上はどんな処理も可能であることを謳う以上、同じシステム同じハードウェアだとしてもその境界線が揺らぐことは防ぎようがない。

[^magical8bit]: 例えばチップチューンとジャズ、ポップスを掛け合わせた音楽を作っている8bitミュージック・ユニット、YMCKのメンバーであるYokemuraが開発するMagical 8bit Plugというソフトウェアシンセサイザーは疑似三角波が作れることを売りにしている。 "三角波を8bit機の処理能力の限界内で擬似的に再現した波形です。ベースなどに適しています。波形のギザギザによるビービーというノイズが特徴で、8bitっぽさの演出には欠かせない音色です。"[@Yokemura2020]

同様に、研究所における取り組みと個人の取り組みという区分も、直感的に理解しやすい分け方として用いたものではあるが注意して読む必要がある。そのためには、[@fig:plfmhistory]における1990年代以降の取り組みをやや個人の側に寄りながらも、研究所にもまたがる形で配置している意図を説明しなくてはならない。

現在活発に使われているPLfMは、コミュニティベースで開発されていることはあっても、その開発は当初の設計者が継続して中心的役割を果たしていることがほとんどで、かつその多くは（筆者も含めて）大学などの研究機関、あるいは企業に所属している人間である。言い換えると、組織の力を借りつつ個人のスキルを活かすことでどうにか成り立っているという状況である。（唯一の希有な例外としてはSuperColliderがあり、これについては本章内で改めて解説するのだが、）こうした視点に立つと、黎明期の研究所におけるコンピューター音楽の研究の中にも、非常に個人的な興味の中から生まれてきた言語やソフトウェアもあるのではないかと考えられるだろう。しかしこの視点を用いて筆者は、伝統的なコンピューター音楽の権威主義的性質を否定しようとしているわけではない。第2章でロスナーの個人主義批判を見てきたように、研究所における音楽のためのシステム開発の中にも、論文には名前が残らない形での様々な貢献があったはずだということには常に注意を払い続ける必要がある。

さらに、研究所中心なコンピュータ音楽史に対するオルタナティブの提示、という構造は、PC以後のコンピューターを用いる音楽実践を、安易に権威化されたコンピューター音楽からの市民による民主的取り組みの萌芽という図式を作ることで、結果的にチップやコンピューターを規格化/製造する企業などによるインフラストラクチャが生み出す、研究所とは異なる形の権力構造から目を逸らすことにもなりかねない。集団の中の個人性、これが本章を考える上で持つべき視点である。音楽を作り出す個人は常にある集団の中の個人でしかありえないし、同様にテクノロジーを生み出すある社会集団もまた、特異な個人の寄せ集めである。

# 最古の "コンピューター音楽"の解体（1950年代初頭-リアルタイム）

本研究が焦点を当てるのは音楽のためのプログラミング言語であるとはいえ、直接的に言語という手段を用いない時代まで遡って、コンピューターと音楽の関わりを見つめることは背理法的に、どこからが音楽のためのプログラミングをよりそれらしくしたのかを明らかにしてくれるだろう。

同時に、コンピューターを用いた音楽の生成の試みを掘り下げるということは、ここから先は "音楽的"な試みであるとされる一般的理解の境界線を明らかにすることである。これは音楽土木工学における問いの1つ、音楽に関わる技術を作ったりメンテナンスする人は一体どこまでが "音楽的"な試みをしているのだろうか、という問題につながる。音楽学者のクリストファー・スモールは音楽を作られた作品ではなくて行為として捉えるためにミュージッキングという概念を導入した[@Small2010]。スモールの考えでは音楽にまつわる様々な行動＝ミュージッキングには例えばコンサートの中でチケットもぎりを担当する人も等しく音楽実践の一部に位置づけられる。スターンはこの考え方を音楽に関わるテクノロジーにも同様のことが言えると指摘する[@Sterne2014]。例えば楽器やソフトウェアを作ったりする人は音楽産業の中にはカウントされないがどうしてなのか、そもそも本当に "音楽産業"など存在しているのか、と。ではこの問いは突き詰めれば当然このようになるはずである。コンピューターそのものを開発する行為も音楽的行為の中に含まれる可能性はないのだろうか、と。すなわち音楽的行為の外側からの解体である。

この試みのために、まずは歴史を電子計算機の登場した時代である1950年付近まで遡ることにしよう。本稿で取り扱う "コンピューター"とは、ひとまず、リレーのような機械的スイッチではなく真空管やトランジスタのような電子的スイッチを用いて計算を行うもの以降という区分を取り扱うことにする。具体的な例としては、アイオワ州立大学で1941年に開発されたABCとペンシルベニア大学（通称ムーア・スクール）で1946年に開発されたENIACなどが挙げられる。ENIACが当初弾道計算という軍事技術のために作られたのはよく知られている事実であるが、同様に黎明期のコンピューターの用途は弾道計算を含めた数理モデルのシミュレーションや暗号解析であった。それにも関わらず、コンピューターで音楽を鳴らす事例はENIACのわずか3年後の1949年にすでに行われていることが知られている。それが、BINACである。

BINACはENIACを開発したジョン・プレスパー・エッカートとジョン・モークリーが立ち上げたエッカート・モークリー・コンピューター・コーポレーション（EMCC）によって作られた電子計算機である。BINAC開発に携わったエンジニアのルイス・ウィルソンは、1990年にチャールズ・バベッジ・インスティテュートで行われた通称UNIVACカンファレンスと呼ばれる会議でBINACの制作段階を振り返り[^univac]、近くに置いていたAMラジオが、真空管をスイッチングする際に発生する微弱な電磁波を拾うことで規則的な音を出すことに気がついた。これを逆手にとって、ウィルソンはコンピューターの出力にスピーカーとパワーアンプを繋ぎ検証のために利用し始めたのだ。黎明期のコンピューターにはこうして、デバッグ用途などにスピーカーが取り付けられることが珍しくなかった。さらにこのスピーカーの機能を用いて音楽を鳴らしたのがフランシス・エリザベス・スナイダー・ホルバートン（通称、ベティ・スナイダーもしくはベティ・ホルバートン）であるとされている[^betty]。ホルバートンはENIACの実際の利用におけるプログラミング作業を行なった6人の女性の1人として知られる人物で、エッカートやモークリーとともにBINACの開発作業にも関わっていた。彼女がアメリカで「Happy Birthday to You」の次に定番の歌である「彼はいい奴だ（For He's A Jolly Good Fellow）」を演奏したのは公的なパフォーマンスというよりも、仲間内のエンジニアを驚かせるためにしたこととされている[@UNIVACConf1990,p72][^binac]。

[^univac]: UNIVACはEMCCが作った世界初の商業コンピューターで、当初先に作られていたUNIVACの制作段階で資金が枯渇し、UNIVACの発注元とは別のノースロップ社へ納品するために先にBINACが制作された。このためBINACの方を世界初のコンピューターと見なす場合もあるが、BINACは納品後は動作不良などが多く実用的にあまり使われなかったことからそこには数えいれられない場合が多い。

[^binac]: [@Irrlicht2015]。田中はIrrlicht Projectがはじめてホルバートンが世界初のコンピューター音楽の実践を行なったことを指摘している[@Tanaka2017,p13]が、後述の2012年の宮崎による文献で既に、（ホルバートンが音楽を鳴らしたことには触れていないものの、）ウィルソンのラジオから聞こえる動作音の部分を引用していることは付しておく。なお、田中の文献の該当部分には引用者名の誤植があることにも注意されたい。

[^betty]: ただし、ドーンブッシュはこのカンファレンス内での発言を除いて音楽を演奏したとされる記述が見当たらず、計算機が音楽を演奏するという、当時にしてはわかりやすい目を引く活用例がどこにも記述されていないことから、ホルバートンの演奏については、行われた可能性があった、と言うに留めるべきだと注意を促している[@Doornbusch2017,p299]。もっとも、BINACを最古に位置付けることに消極的な立ち位置に立つことで、ドーンブッシュの研究対象であるCSIRAC（と同年のFerranti Mark I）が最古のコンピューターによる音楽生成であるという主張につなげている側面も否めない。

宮崎はこの、アルゴリズムが作り出すビットパターンが生み出す音を聴く行為がプログラミングやその中のデバッグ作業に組み込まれている状況を "Algo**rhythmic** Listening"と形容し、ややもすると視覚表象文化の視点からの分析に偏りがちなメディアとしてのコンピューターを扱う実践[^chun]の中に、聴覚的な技法が取り入れられてきたことを分析している[@Miyazaki2012]。この視点は本研究のような、音楽の実践者が技術の極めて基礎的な部分を扱う学問を考える時に興味深い視点である。極めて誇張した言い方をすれば、**コンピューターを音楽制作に応用するまでもなく、コンピューターを扱うことはそもそも音楽的な実践だったのではないか**、という問いを立てる必要があるのだ。

[^chun]: 例えば、メディア研究者ウェンディ・フイ・キョン・チュンによる、ソフトウェアの持つイデオロギー性を視覚文化との兼ね合いから検証した "ソフトウェアについて、もしくは視覚的知識への固執"という論考を見よ[@Chun2005]。

実際、UNIVACカンファレンスの中には、UNIVAC開発に関わったランス・アームストロングによるこんな発言がある。初期のコンピューターのプログラミングの熟練に関して、統計数学を扱う研究者と、計算手（つまり電子計算機以前の人力による計算作業を行う人たち）のプロジェクトマネージャーを比べると、はじめは統計数学の研究者の方が覚えるのが早いのだが、4、6ヶ月も経つとどちらも似たようなものになるのだという。ところが、それとは関係なく、誰が気づいたかもわからないし、なぜかもわからないのだが、音楽の（おそらくは演奏の）技能が高い人はプログラミングの上達も速いという相関があるのだという。発言はさらにアームストロング同様UNIVAC開発に関わったフローレンス・クーンズとモデレーターでフンボルト州立大学のコンピューター史研究者ヘンリー・トロップを交えて、以下のように続く。

> アームストロング：まあそれはそれで、プログラマーたちをテストするために "弓とフィドルを持ってくるのを忘れずに。"なんてのはひどい話になってしまうわけだけど。（会場笑い声）

> トロップ：それは、後々には確かにそうで、トランジスタが世界を一変させてしまった後、誰もそんなことになるなんて考えもしなかったほどにプログラマーが必要になってしまって、彼らは本当に音楽専攻の学生たちを大量に採用したんだよ。

> クーンズ：確かに、音楽と数学には大きな相関がある。

> トロップ：でもそれは1960年代になるまでは起きなかったのだけど。[@UNIVACConf1990,p60 筆者訳]

トロップの指す、1960年代の音楽専攻の学生の大量雇用という事実の詳細に関しては不明であるし、音楽の技能とプログラミングの技能の相関性もひどく主観的なものではある。しかしそれでも筆者は、初期のコンピュータープログラミングの作業にAlgorhythmic Listening的な実践が入ってくることは必然だったと主張したい。着目すべきは、BINACと同時期のコンピューター、例えばUNIVAC I（1951）やオーストラリアのCSIRAC（当初の名前はCSIR Mark 1）（1950）、イギリスのACE(1952〜1953)などに使われていた記憶装置、音響遅延線メモリーである。

音響遅延線メモリーとは、何か物理的な状態を変えることでデータを保存するのではなく、水銀などで満たされたタンクの端に取り付けたスピーカーから、バイナリデータを超音波のパルスの有り/無しに対応させ発音し、その音を反対側の端に取り付けたマイクロフォンで拾い、またデータをスピーカーへフィードバックすることで、スピーカーからマイクロフォンへ届くまでの音波の遅延時間分のパルスを保存できるという、いわば延々と通信し続けることによってデータを保存するメモリーである。

筆者はかつてこの音響遅延線メモリーという、今や使われる理由のない技術を作り直す作品《Acoustic Delay (⇔) Memory》（2015、[@fig:adm]）と《送れ｜遅れ/post|past》（2016、[@fig:adm2]）を制作した[@Matsuura2016][^admother]。筆者はこの作品の制作過程で行なったサーベイで、ミシガン大学のコンピューター史研究者、ピーター・エクスタインが描いた、ENIACやBINACの設計者であり、音響遅延線メモリーの仕組みの考案者、ジョン・プレスパー・エッカートの人生史[@Eckstein1996]に着目した。そこで描かれていたのは、エッカートが音に関するエンジニアリングを続ける中でコンピューター制作にたどり着いたという軌跡である。宮崎が着目していなかった点として、ラジオに流れる電磁波を拾うまでもなく、音響遅延線メモリーという装置はそもそも音楽文化とテクノロジーの交差点上に生まれた装置であることが彼の生い立ちを知ることでわかる。

<!-- ![《Acoustic Delay (⇔) Memory》(2015)。ディスクリートICで作られた音響遅延線メモリー回路によって、展示空間に8bitのバイナリデータを保持し、Webサイト上からそのデータを読み書きできる。大きな音を出したりマイクを遮ったりすることで物理的にデータに干渉することもできる。](img/adm.JPG){#fig:adm width=80%}

![《送れ｜遅れ/post|past》(2016)。物理的には独立した2台の通信機能だけを持つ送受信デバイスが、空間に配置されることによってはじめて音響遅延線メモリーとして記憶装置の機能を果たす。](img/postpast.JPG){#fig:postpast width=80%} -->

\begin{figure}[htbp]
  \begin{minipage}{0.5\hsize}
    \begin{center}
        \includegraphics[width=1\hsize,keepaspectratio]{img/adm.JPG}
    \end{center}
    \label{fig:adm}
    \caption{《Acoustic Delay (⇔) Memory》(2015)}
  \end{minipage}
  \begin{minipage}{0.5\hsize}
    \begin{center}
        \includegraphics[width=1\hsize,keepaspectratio]{img/postpast.JPG}
    \end{center}
        \label{fig:adm2}
    \caption{《送れ｜遅れ/post｜past》(2016)}
  \end{minipage}

\caption*{{《Acoustic Delay (⇔) Memory》では、ディスクリートICで作られた音響遅延線メモリー回路によって、展示空間に8bitのバイナリデータを保持し、Webサイト上からそのデータを読み書きできる。大きな音を出したりマイクを遮ったりすることで物理的にデータに干渉することもできる。《送れ｜遅れ/post｜past》は物理的には独立した2台の通信機能だけを持つ送受信デバイスが、空間に配置されることによってはじめて音響遅延線メモリーとして記憶装置の機能を果たす。}}
\end{figure}


[^admother]: 筆者に先行して、アーティストのキム・ユンチュルが2005年に同様に音響遅延線メモリーを再構築する作品 "Hello World!"を作成している[@Yunchul2005]。他、本論文の執筆中に開催された、人工生命研究者の団体ALTERNATIVE MACHINEが行なっている展示 "ALTERNATIVE MACHINE"にて、音響遅延線メモリーを構築しその中にブロックチェーン上に記録されるNFT（Non-Fungible-Token）データを保存する作品「遅延記憶装置」を作成、展示しており、筆者は自身の制作の経験から技術アドバイザーとして関わっている。 https://7768697465686f757365.com/portfolio/wh015-alternativemachine/

エッカートは幼少期から音に関わるテクノロジーに強く興味を持っており、5歳の時に親からラジオを買ってもらい、7歳の頃には塹壕ラジオ（コイルとクリスタルなどで作れる簡単な仕組みのラジオ）を作り、13歳の頃にはレコード用のアンプリファイアーを制作していた。高校生や、ムーア・スクールに入学した頃にはすでに礼拝堂に鐘の音をスピーカーから流すためのサウンドシステムを構築したりして小金稼ぎをしていたともされている。しかも、特に着目すべきことは、彼が単に問題解決や功利主義のためにだけでなく、ある種のエンタテインメントを目的としたものづくりを積極的に行っていた、今日でいうところのメイカー的側面があったことである（この中でも、極め付けは、嘘発見器の応用の要領で作られた、カップルにそれぞれハンドルを握らせ、その状態でキスをするとその情熱の度合いを電球や大音量の発振音で賑やかす "osculometer(キス測定装置)"なるものである）。

エッカート自身は直接的に音楽や音声合成のようなテクノロジーに取り組むことはなかったものの、彼がムーア・スクールに勤める中での仕事の一つとして、1940年のニューヨーク万博にベル研究所が出展した、人工発話装置Voder[^voder]のような、計算機以前の電気的音声合成の技術には強く影響を受けたことが語られている。また、高校生の時にパイプオルガンに触れたこと、そしてのちにハモンド社が開発した100本以上の真空管が用いた電子オルガンを知ったことは、後の膨大な数の真空管で構成される電子計算機の存在に現実味を持たせるのにも一役買っていた。真珠湾攻撃の年である1941年、ムーアスクールが海軍やマサチューセッツ工科大学（MIT）のレーダー研究を行う研究所と協力を始めるようになり、エッカートは音声や通信に関わる技術のバックグラウンドを買われて仕事を行うようになる。そこで担当したのが、のちにトランジスタを開発する1人であるウィリアム・ショックレーが先行して研究していた、レーダー信号からノイズを除去するためのパルス遅延装置だった。ショックレーは超音波パルスの媒体に不凍液であるエチレングリコールと水の混合液を用いており、しかも片方の端からパルスを出し、反対の端で一度反射させてから同じ側でもう1度受信する仕組みを採用しており、結果として帰ってきたパルスの波形がどうしても歪んでしまう欠点があった。エッカートはここに、以前映画用フィルムの音声トラック書き込み装置を高精度化させる試みの中で培った、超音波の媒体としての水銀の有用性という知識を持ち込んだのである。さらに、片方で反射するのではなく、反対側で一度マイクロフォンでパルスを受信してフィードバックさせればより長い遅延時間を作り出すこともできる。

[^voder]: エクスタインの文献ではVocoderとなっているが、エッカートが体験したとされるキーボードで発音できるデバイスはVoderである。Vocoderは今日一般的に人の声を模す音声合成を指して使う語だが、ベル研究所のVocoderとはここではVoderという合成機能と対にになっていた分析を行う機械の名前である。 https://120years.net/wordpress/the-voder-vocoderhomer-dudleyusa1940/

こうした流れで音響遅延線メモリーの仕組みは形作られた。エッカートはこの時すでにヴァネバー・ブッシュの機械式計算機である微分解析器（Differential Analyzer）を見て、真空管によるスイッチングを大量に用いれば同様の機能を、機械では不可能な圧倒的速度で実現できるのではないかという構想をあたためていた。しかし、そこで問題になっていたのは記憶装置をどうするかということである。電子計算機の速度を生かすのであれば、メモリーも真空管を組み合わせたフリップフロップ回路[^flipflop]で実現すればいいが、そこまですると必要な真空管の数が一気に膨大になり、真空管が熱で劣化することを考えるとメンテナンスの問題も出てくる。一方でメモリだけを信頼性の高い機械式のスイッチなどで代用してもせっかくの実行速度のボトルネックとなってしまう。そこにはまった最後のピースこそが、音波のパルス列を用いることで、適度に高速で、かつ適度に大容量な音響遅延線メモリーという仕組みだったのである。

[^flipflop]: スイッチング回路の出力をフィードバックすることで状態を保持することができる回路。1ビットのデータ保存に複数個の真空管が必要になる。

つまり、音響遅延線メモリーという、その一見してどういった経緯で思いつくだろうかのかという奇妙な仕組みの考案の背景にはそれなりの必然性があったのである。エッカートが音にまつわる技術を再編成することで初めて電子計算機の誕生にたどり着けたのだとすれば、その仕組みを用いたコンピューターのデバッグの中に聴覚の技法が現れるのは不思議なことではない。改めて言い切ってしまおう。コンピューターを作ることそれ自体が音楽に関わる行為のひとつだった。

エッカートの作った音響遅延線メモリーはラボラトリーのように制度化された場所における個人のDIY的試みが与えた影響は、この時期のいわゆる "コンピューターを使った音楽生成"から、いわゆる "コンピュータ音楽"における最も古いシステムである、ベル研究所で1957年に作られたMUSICの誕生についての橋渡しについて考える材料にもなる。

1950年前後のコンピューターから鳴らされる音楽は、基本的に既存の音楽のメロディの制御だけ、つまり周波数と発音する/しないという2種類のパラメータの制御に過ぎなかった。パルスを出すか出さないかの制御しかできない故に、その音色は常にいわゆるビープ音のような音にしかならない。加えて、これらの実践はあくまでデバッグ目的のスピーカーや、偶然の発見からの、いわば音楽のためには作られていない技術の流用である。田中はこの時代以降のコンピューターを用いた音楽文化が、この後ベル研究所で開発されるMUSICシリーズのような**専門化**された正当なコンピューター音楽の流れと、この時代の技術の流用からなるプリミティブな音楽生成、そして後のチップチューンのような、限られた計算リソースの中で行われる表現が生み出す独特さへ連なる**趣味化**の流れへと二分化する歴史として描いた[@Tanaka2017,p19]。しかし、ベル研究所のVoderがなければ、音響遅延線の仕組みが計算機上を流れる信号を聴く行為をアフォードすることも、そこから生み出されるプリミティブで趣味的な音楽もなかったかもしれないことを思うとこの二分法も歴史的因果関係を捉え損ねやすくする恐れがある。

実際、ドーンブッシュは、パルス間隔制御による "既存の音楽をコンピューターで鳴らす"試みと "音楽をコンピューターを活用して作り出す"試みの間に、パルス制御を利用して独自の音楽を作り出す試みがあったことを指摘している[@Doornbusch2017,p303〜p304]。イギリスで、アラン・チューリングが国立物理研究所のために設計したPilot ACE（Automatic Computing Engine:ACEのためのプロトタイプ）は、BINACやUNIVAC、CSIRAC同様に音響遅延線メモリーを保持しており、他のマシン同様に診断目的のスピーカーが取り付けられていた。ACEを利用していたドナルド・デイヴィスは次のように振り返る。

> Pilot Aceとその後続機Ace Properはどちらも、独自の音楽を作曲することができて、コントロールデスクに取り付けられた小さなスピーカーから演奏することができた。私が〔コンピューターを主語にした〕作曲という言い方をしたのは、人が意図的に音程を選ぶような余地が一切なかったからだ。その音楽は無調ではあったが大変興味深く、上昇系のアルペジオに始まり、だんだんと、フーガが発展していくように複雑化し加速していった。そして複雑度が人間の認知を超え最終的に色のついた[^coloured]ノイズの中に溶けていった。[@Davis1994,p19 筆者訳]

[^coloured]: 周波数成分的に偏りのあるノイズを工学分野では一般的に可視光のスペクトラムの偏りが作る色に置き換えてピンク・ノイズ、レッド・ノイズ、ブラウニアン・ノイズなどと表現する。おそらく周波数成分が均一なホワイト・ノイズとは異なる音色だったことを指しているのだと推測されるが、適切な訳語が思い当たらなかったのでそのまま置き換えさせてもらった。

我々にとって興味深いのは、デイヴィスらが、診断機能の流用であり、かつ12音のピッチと発音という既存の音楽様式の再生産ではないにもかかわらず、明確に音楽を作っていることを自覚していたこと、しかもそれが一般的な音楽の作曲方法の発想と全く異なる故に、コンピューター**が**作曲を行っていたというレトリックを用いていることである。

さらに、デイヴィスらの試みは音響遅延線メモリーと宮崎のいうAlgorhythmicな聴取が明確にこの音楽の特異さに寄与していたことを示している。ACEが利用した音響遅延線メモリーはデータが常に音波として周回し続けているため、今日一般的に使われているメモリーや、チューリングがこの前に設計したManchester Mark Iが採用していた陰極線管メモリ（音響遅延線と対照的に、ブラウン管テレビの仕組みをメモリに転用したデバイス）と異なり、ランダムアクセス、つまり任意のタイミングで任意のデータの読み込みができない。それゆえ、プログラムをうまく最適化していくと、最終的に実行時間の中で支配的になるのは遅延線メモリーからのデータ転送の待ち時間になるという。デイヴィスらは、このデータ転送時間に関わるTranstimと呼ばれるトリガー部分の信号を観測することでプログラムの実行効率を診断できるのではないかと考えた。トリガー信号をそのまま計測メーターに入れると針が細かく振れすぎてしまうので高周波成分をフィルタリングするようにした。ここまで来れば、この平滑化された信号を直接スピーカーに送り聞いてみるのは自然なことだったという。この頃には液晶ディスプレイもなく、基本的にプログラムの結果はパンチカードに出力されるのでデバッグは難しい。その中でスピーカーは安価なわりに診断に大いに役に立つデバイスだったため、デイヴィスらも、取り付けるかどうかを誰かに相談するまでもなかったのだという。

しばらくしてそこに、もう1人のエンジニアであるデイヴィッド・クレイデンが新たなデバッグ目的の機能として、ACEの実行する命令を部分的に上書きできるスイッチを取り付けた。ACEの音楽はこの機能を活用（デイヴィスによれば、誤用）することで生まれたという。

> 詳細は覚えてないがこんな感じだったと思う。長い遅延線メモリにはプログラムである命令列が32個格納されていて、はじめは多分空に設定されている。プログラムはループに入り、それがスピーカーにある音程を鳴らさせる。命令を〔スイッチを用いて〕ディレイラインの中の適当なビットに加算する操作に固定することによって、このカウンターは最終的にタイミングを司るフィールドに桁あふれすることによって、定期的に別の種類のループに入る。

> 正しいスイッチの設定をすると、ループの大きさはしばらく一定で（1音）、その後突然〔ループの〕サイズが変化する。

> ループは常に32マイクロ秒の倍数になるので音程は常に31.25kHzを分周した周波数を持つことになる。その音楽は、平均律でもなければ和声的とも全く異なる、非常に奇妙な音階でできていたが非常に面白いものだった。[@Davis1994,p20 筆者訳]

デイヴィスらの生成した音楽は、当人たちに音楽であると認められており、かつその独特の音程変化というメディウム固有性は音響遅延線メモリーという装置の仕組みによってもたらされたものである。

ドーンブッシュは、これら1950年代初頭のコンピューター音楽が歴史的にはその後のMUSICシリーズのような、田中が言うところの "専門家された"コンピューター音楽の歴史に直接的に影響を与えなかったことから、その歴史の中から排除されアメリカ中心的な歴史になってしまっていることを批判した。確かに、ベル研究所で行われたMUSIC以降の取り組みには、明確にプロフェッショナルな音楽のバックグラウンドを持つ作曲家などの人間が研究に加わるようになり、かつ黎明期の取り組みは特に参照されていないという意味で断絶がある。しかし当然、そうした専門家の持っていた "音楽"のバックグラウンドは、言い換えればその時代に一般的に受容されていた音楽の共通認識、であり、コンピューターが音楽の定義を押し広げるために活用されてきたことを肯定するのであれば、BINACやUNIVAC I、CSIRAC、Ferranti Mark Iのような既存のメロディを奏でるための実践はともかくとして、ACEで行われていた取り組みはその歴史の中に数え入れられて然るべきということになろう。

<!-- [^williams]。

[^williams]:ただ、1951年には同時に、アラン・チューリングが設計したManchester Mark Iやそれを商用化したFerranti Mark Iでも同様にスピーカーにパルスを送ることで音楽を生成する試みが行われており、これらの計算機は音響遅延線メモリーではなく、陰極線管メモリーという、いわばテレビの技術を転用したような仕組みのメモリーを用いていた。音響遅延線がデータを逐次的にしか読み出せないのに対し、陰極線管メモリーは任意の箇所のデータを随時取り出せる、ランダムアクセスが可能という特徴を持っていた。Mark Iでスピーカーから音を出す試みは、Hootという特別な命令を実行することでスピーカーに明示的にパルスが送られ、その間隔をダミーの何もしない命令などを挟むことによって調整することで特定の音程を出せることがチューリングの書いた操作マニュアルに示唆されていた。音響遅延線メモリーを用いるCSIRACの音生成はこれらイギリスにおける取り組みであったHoot命令を用いることで行われていたが -->

# マックス・マシューズとMUSICの位置づけ（1950年代後半-ノンリアルタイム）

ここからはMUSICシリーズの方へと焦点を移し、 どのようにしてMUSICが "正当な"コンピューター音楽の歴史に位置づけられてきたのかを検討していく。MUSICシリーズの発展の歴史的変遷の詳細は[@Lazzarini2013]が詳しいので具体的事実関係に関してはそちらを参照されたい。

## パルス符合変調とMUSICシリーズの概要

MUSICがそれ以前のシステムと異なっていたのは、パルス符合変調（PCM）と呼ばれる、音声波形を一定時間に分割（標本化）、各時間の音圧を離散的な数値として表す（量子化）、今日のコンピューター上における音声表現の基礎的な方法に基づいた計算を行ったことだ。パルス符合変調の元となる標本化定理はナイキストによって1928年に示され[@Nyquist1928]、パルス符合変調はリーブスにより1938年に開発されている。

この考え方を簡単に表したのが、[@Hartley1928]より抜粋した[@fig:hartley_pcm]である。時間を横軸、音圧を縦軸にとった音声波形のグラフをグリッド状に区切り、連続した数値を離散化された数値のリストに変換する。区切るグリッドが少ないほど、実際の波形との誤差が量子化歪みとして現れる一方、グリッドを細かくするほどに必要なデータの量は増えていく。また、標本化定理より、例えば1000Hzの周波数成分までを持つ波形を標本化するとき、その2倍である2000Hz以上、つまり横軸のグリッドを2000分の1秒より細かく設定する必要があることが知られている。例えばサンプリング周波数が1800Hzだった場合、表現できるのは900Hzまでとなり、1000Hzの正弦波をこのサンプリング周波数で標本化すると$1800-1000=800Hz$の信号が折り返し歪み(エイリアス信号)として現れてしまう。

人間の知覚できる周波数の上限が20000Hz程度となっているので、その2倍である40000Hz以上の標本化周波数で、かつ量子化歪みが十分に少なくなるように量子化ビット数を決めておけば、人間が近くできうる範囲ではおおよそどのような波形でも数値として表現できるということになる[^compactdisk]。

実際には連続した波形を離散化するだけであればこのような考え方の考慮で十分なのだが、標本化/量子化した波形同士を演算する場合にはもう少し細かい事情を考慮する必要があるため問題点も指摘されてきており[@Puckette2015]、音楽プログラミング言語設計の根幹にも関わってくるのだが、それはのちに議論する。

[^compactdisk]: 例えばコンパクトディスクの規格などはこういった考慮から標本化周波数44000Hz、量子化ビット数16bitと定められている。なお、サンプリング周波数や量子化ビット数を意図的に荒くすることで得られる可聴範囲の歪みを利用した表現が今日ビット・クラッシャーと呼ばれるエフェクトである。

<!--図 清書したほうがいいかも -->

![ハートレーの論文[@Hartley1928]より、PCMの概念を表した図。](img/hartley_pcm.png){width=70% #fig:hartley_pcm}

MUSICはIからVまでの5バージョンが代表的なものとして知られている。マシューズが直接的に開発に関わったのは基本的にこれらI〜Vまでだが、1960年代におけるコンピューターアーキテクチャの多様化やプログラミング言語の発展に伴い、MUSIC IV以降には様々なMUSICの名を冠する派生システムが作られた。

例えば、MUSIC IVの実装言語は当時主要な言語であったFORTRANで実装されていたが、当時新しく作られたばかりのC言語に実装されたMUSIC 4C(M4C)やCmix、プリンストン大学で同等の機能を持つものとして実装されたMUSIC IVB、MUSIC IVBF、さらにIVBやIVBFをIBM360コンピューター上で動くように移植したMUSIC 360、360をもとに、さらにDEC社の発売したミニコンピューターであるPDP-11へ移植したMUSIC 11、MUSIC VをもとにC言語で実装されたCmusic、さらに現在まで開発が続くCSound、と言った様々な派生系がある。これらを総合してMUSIC Nシリーズと呼ぶこともあるのだが、マシューズが関わっていない派生系などをどこまで含めるのかには明確な合意がない。本稿ではI〜Vの総称として用い、派生プログラムを含める意味で用いるときは常にそう注釈することにする。

1957年に作られたMUSIC IはIBM 704というコンピューター上で動作する、対称系の三角波の波形に対してエンベロープを掛ける程度の波形の合成ができるシステムだった。MUSICはPCMに基づくことで、パルスのあり/無しで音程を作る音声合成より、理論的に遥かに多様な音色を作ることが可能なシステムではあったが、計算速度の問題上、デバッグ用スピーカーを用いるのとは異なり、リアルタイムでの実行は敵わなかった。（なお、IBM 704のメインメモリは磁気コアメモリであり、この時期以後、音響遅延線メモリーを採用するコンピューターはほぼ作られていない。）MUSIC NシリーズはVまで基本的にこのノンリアルタイムな方式を踏襲する。

MUSIC Iの時の実際の利用方法としては、まずMUSICのプログラムをコンピュータが利用できるIBM本社まで持ち込み、計算結果を磁気テープにバイナリデータとして書き出し、磁気テープをベル研究所に持ち帰り、そこにあった真空管製の12bitデジタル-アナログコンバーターに通して、アナログの電圧波形に変換し、それをスピーカーへつなぐことではじめて音が出せるようなシステムになっていた[@Roads1980;@Roads2001]。MUSIC Iの時点でのサンプリング周波数について記述されている文献はいないが、後年のIBM 7090上で動作していたMUSIC IV(1963年)では、同様に12bitでの量子化で、実際の計算速度が1秒間に5000サンプル程度が性能の限界だったのに対し、実際に計算結果を再生するときにはそれを1秒に30000サンプルに早回しすることができたと記述が残っている[@Mathews1963]。

## Unit Generatorの発明と、内部状態の保持方法

MUSICシリーズは1958年にIIが、1960年にIII、1963年にIVという発展を遂げ、「The Digital Computer as a Musical Instrument」[@Mathews1963]という論文でその内容がまとめられる。1969年までにはVが開発完了し、その利用方法や背景となる理論が書籍「The Technology of Computer Music」[@Mathews1969]にまとめられる。この項では、その実際の利用方法について詳細に検討することで、今日の言語に通ずる影響について考察する。

マシューズによればMUSIC Nシリーズにおいて重要な概念は以下の3つである。

- Stored Function
- Unit-Generator
- The Note Concept

Stored Function（保存された関数）はあまり耳馴染みのない単語だが、これは今日一般的にウェーブテーブルと呼ばれる、ある周期関数の1周期分の計算結果を、事前に512サンプルなどに分割して配列として保持しておき後から利用する方法である。

コンピューター音楽の歴史の中で重要な概念となった**Unit Generator**（以下UGen）はMUSIC IIIで初めて導入された。Unit Generatorは、サイン波などのオシレーター、加算器、乱数生成器など信号処理の基礎単位（Unit）を表す概念である。今日では、モジュラーシンセサイザーの個々のモジュールを想像するのが直感的に理解しやすいだろう。複数のUnit Generatorを接続することで、PCMベースの信号処理を比較的簡単に表現できるようになったのである。

ただし実際の時系列としては、UGenのコンセプトはいわゆるモジュラーシンセサイザーを作るよりも前に、あるいは同時多発的に発明されていた[@Nishino2016,p7〜p8]。例えばロバート・モーグはこうしたコンピューターを用いた音声合成にそこまで興味を持っていなかったものの、手動のつまみではなく電圧により音程を操作するVCO（Voltage-Controlled Oscillator）のアイデアに取り組んでいたとされる1964年[@Pinch2004,p12〜p31]に、マシューズと共に音声分析・合成の研究をしていた作曲家ジャン=クロード・リセとこうした話題について文通していたとされ、むしろモーグの方がMUSICの仕組みに影響を受けていた可能性もあるとされている[@Parks2009,p20]。モーグやブックラ以前のいわゆるアナログシンセサイザーとは、RCA Mark I（1955）やRCA Mark II（1957）のような、同時代のコンピューター同様、大量の真空管を用いた部屋の壁一面を覆い尽くすような大きさで、パンチカードから演奏データを読み込むことで演奏できるようになっていた。田中の表現を借りれば "電子音楽アナログコンピュータ"[@Tanaka2017,p20]に近いものだった。この事実関係は、例えばマッカートニーがSuperColliderを制作するにあたってモジュラーシンセサイザーをソフトウェア上で再現することがひとつのモチベーションであり[@McCartney2020]、かつUnit Generatorという単語を公式に用いていたりすることもあり、なお誤解を招きやすい。

このUnit Generatorとモジュラーシンセサイザーの時系列はコンピューター音楽の歴史研究ではすでに周知のものになりつつあるが、電子音楽の歴史という広い視点へ立ってみると、既存の言説に微妙な注釈を入れる必要が出てくる。例えばテベルジュによる電子音楽と産業、消費社会の関係性の考察[@Theberge1997]や、それを多く参照するディドゥックのMIDIの歴史研究[@Diduck2018]では、モジュラーシンセサイザーによる西欧の音楽様式に縛られない表現が、結果的に12音の鍵盤という既存の様式を導入することによってコンシューマープロダクトとして成功することになったという図式を作っている。しかし、MUSICの3つのコンセプトを考えると、そのシステムはUGenという音色の自由度を飛躍的に上げる仕組みを導入しつつも、同時にスコアとオーケストラという既存の音楽様式のアナロジーに立つことで、コンピューターという抽象的な計算装置を音楽家が利用するに当たってのハードルを下げているからだ。

その、The Note Conceptは文字通り、音価の配列を宣言的に記述できるようにした仕組みである。これが既存の仕組みとどう異なったのかというのは、例えば同時期のRCA Mark IIにおける音価情報の入力方法が、どちらかといえばオルゴールや、自動演奏機械の方法に則ったデータ入力方法だったことと対比して考えるとわかりやすい。

> 紙テープのロールは秒速10cmで動き、最高でBPM240を作り出す。長い音価は個々の穴の組み合わせで構成され、最後の穴が来るまで音価が持続する仕組みが用いられた。[Crab2013,筆者訳]

こうした、音の時間的長さとパンチホールの連続という空間的長さが結びついていた状況から、MUSICは音符に相当する記述を次のように宣言可能にした。MUSIC Vの最も簡単なコード例の中から抜粋する[@Mathews1969,p45]。

```
 NOT 1.00 1 .50 500 8.45 ;
```

`NOT`に続く5つの数値はここではそれぞれ、*開始時間（秒）*、*楽器のインデックス*、*音価の長さ（秒）*、*音量（0〜2047）*、*音程（周波数（Hz）*511/20000）*を意味する。音程はここでは331Hz（基準ピッチ442HzにおけるE3に相当）ということになる。音程がこのような遠回しな表現になっている理由を説明すると、511という数値は標準的なウェーブテーブルのサイズ、20000がサンプリング周波数に相当するものになっており、8.45という数値はここで、1サンプルあたりに読み進めるウェーブテーブルのインデックスの増分をあらわしている。つまり、0サンプル目の時は波形テーブル0の位置、1サンプル目の時は8.45（8番目と9番目のテーブルの値を補完する）、2サンプル目の時は16.9番目の……と読み進めていき、511を超えたらまた0番目に戻るようにするのである。

ただし、もちろん実際にはこのような指定は12音だけを扱いたい作曲家には煩雑になる。そこでMUSICでは`CONVT`と呼ばれる名前のサブルーチン（≒関数）を定義すると、処理の途中で`NOT`命令の任意のフィールドを任意に書き換えることができるようになっており、今日におけるMIDIノートのように、12音音階を直接整数に対応させた数値で記述することも可能になる。つまり、12音をベースにした作曲の思考にそれなりに簡単に対応できるようにしつつも、音価レベルであっても必ずしも12音に絶対的に縛られることのないバランスを取った仕組みになっている。

このように、五線譜における音符相当の記述は`NOT`命令を一行ずつ記述することによって実現される。このNOT命令列は最終的に、実行されるときには`P1`から`P30`という名前で利用できる値の配列を、論理時間に応じて書き換えることで信号処理と組み合わせられる。どのように利用されるかをUGenの宣言を見ることで説明する。Unit Generatorの接続構成も、1行ずつ宣言的に行われる。同じサンプルからUGen定義の一部を抜き出そう。

```
OSC P5 P6 B2 F2 P30
```

ここで、`OSC`はStored Function、つまり波形テーブルを周期的に読み出す発振器を配置することを宣言している。パラメーターは順番に、音量、周波数（実際には1サンプルあたりのテーブルインデックスの増分）、出力バス、テーブルを作る関数、一時変数（後述）である。音量と周波数がそれぞれ`P5`、`P6`という表記になっているのは、スコアにおける音価パラメーターの現在の値を取得するのを意味する。`P1`は`NOT`という文字列それ自体、`P2`は開始時間である`1.00`を指し、というように番号が`NOT`の宣言におけるそれぞれのフィールドに対応するので、`P5`と`P6`はそれぞれ先の`NOT`宣言における`500`と`8.45`に対応するわけである。

では最後の`P30`は何か。これは、現在時刻におけるウェーブテーブルの読み出し位置を一時的に保存するために、音価パラメーターの使われない余ったフィールドである、`P30`を使って保存することを意味している。`P2`から`P30`までできる`Pn`パラメーターは、実は決まった音程、音量といったパラメーターの記述の規則があるわけではなく、ある種時間に応じて値が変化する汎用のデータ配列として用いられているのだ。

実際、`NOT`の宣言においてパラメーターを`P6`以降も直接記述することで、例えば、ビブラートの強弱用のパラメーターのような、音符に対するメタデータを付随させるような使い方ができるようになっている。それを、信号処理における内部状態の保存にも流用しているのが`OSC`における`P30`の役割というわけだ。それゆえ、実際には他で使われてないパラメーターであれば`P30`以外のどの値を用いても問題ない。

一時変数としての`Pn`の利用方法は例えばバンドパスフィルタのUGenである`FLT`でも行われている。`FLT`UGenは、1、2サンプル前の出力を保持しフィードバックすることで実現される（2-poleフィルタと呼ばれる種類のものである）。例えば、時刻$i$におけるUGenの出力$O_i$は、入力サンプル$I_i$、フィードバックの強さを表す係数をそれぞれ$I2_i$、$I3_i$を用いて次のように表される。なお、このフィードバックの係数は適切なフィルタの係数を持つようにz変換などを用いて事前に計算されねばならない。

$$O_i = I_i + I2_i \cdot O_{i-1} - I3_i \cdot O_{i-2}$$

このフィルターを扱うMUSIC Vの命令は次のようになる。

```
FLT B4 I1 B5 I2 I3 P28 P29 ;
```
ここでは、`B4`が入力バス、`B5`が出力バスを表すと仮定する。ここで`P28`、`P29`は先ほどの`OSC`の例と同じく、余っているノートの配列を使って、1サンプル前、2サンプル前の出力を一時的に保存するための記憶領域として用いられている。

この余ったノート配列を一時変数の記録領域として利用していることは、実は今日までに続く音楽プログラミング言語の理論を考えるにあたって重要な役割を果たす事項である。重要なことは、この一時変数を用いることによって、一時刻前の計算結果を利用して、漸化式的に次のサンプルを決定することができ、それが何を意味するかというと、時刻の純粋な写像$y=f(t)$では表せないような信号の表現ができるということである。

入力波形を時間方向にスライドするディレイのような表現は信号処理の理論において（特に楽器の物理モデリングなどで）も、純粋に音響効果としても重要な役割を果たすのだが、これを現実的に計算機で実現するためには、一度メモリに音声データを保存し、あとで読み出すということになる（遅延によってデータを記憶していた音響遅延線メモリと対称の関係である）。Unit Generatorのような信号処理器同士の接続をより厳密に数学的に形式化していこうと思うと、このような、一時的に保存してあとで読み出す作業を形式化するのは難易度が高いのである。これは、今日PLfMを作るにあたって、汎用プログラミング言語上でライブラリとして言語を実装することが難しい理由の1つでもある。実際、MUSICでは内部状態の保持を必要とするUGen同士のフィードバック接続を基本的には想定していないようだが、それでも最低限のフィードバック表現を音価のための余ったパラメータ領域を流用することで実現しているのは、今日にしてみればあまり自然には出てこない発想である。

## 言語としてのMUSICの位置付け

さて、MUSIC Nは当時のシステムとしては音楽家がコンピューターを利用するハードルを下げたのは確かだが、今日の音楽プログラミング言語から比較してどの程度言語としての性質を持っていたのだろうか。

そもそもMUSIC Nというプログラム自体の実装は、IIIまで機械語、IVでマクロアセンブリ言語、VでFORTRAN IVを用いて実装されている。なお、FORTRANの最初のバージョンが開発されたのは1956年[@FORTRAN]という、MUSIC Iのわずか1年前であり、これだけでも今日のPLfMについて、一般的に想起してしまう汎用プログラミング言語やその理論を音楽へ応用したものというイメージは歴史的には実は大きく異なるということがわかる。

もっとも、この時代におけるFORTRANを含めたプログラミング言語とは、そもそも当然インタラクティブなGUIもテキストエディタもなかったため、利用する時は実際に一度タイプライターなどでテキストに起こしてから、それをキーパンチ機を用いて1行ごとに72文字までの命令を1枚のパンチカードに打ち起こして、大量のパンチカードをコンピューターに読み込ませることでプログラムを実行していた。そのためこの時期のプログラミング言語は基本的に1行1命令といった行ごとの記述形式になるのが自然だった。

基本的にはMUSICのプログラムを作る工程も、スコアとUGenの接続を実際に紙などに描き起こした後に、それをテキスト形式の命令列に置き換え、最終的にパンチカードに打ち込むというステップを踏んで実行されていたと考えるのが自然である。そのため、既存の12音の様式に則った音楽をコンピューターで鳴らすだけであれば、MUSIC以降は作曲家はコンピューターの知識を必要としなくなったというのは真と言えるだろう。ただしもちろんそれは、作家が書いた楽譜を命令列へと変換する作業を誰かに任せている、ということを仮定している。加えて、コンピューターならではの表現を追求しようと思うと、必然的に`CONVT`サブルーチンの定義をFORTRANで行う必要があったように、コンピューターやプログラミング言語に関する知識が必要になってくる。標準では存在しないUGenを独自に新しく定義しようと思った場合にも同様である。

これは現代においてもPLfMが、コンピューターに関する専門的な知識がなくとも音楽が作れると謳いつつも、結局はその言語に固有の表現を行おうと思うとそれなりの知識習得が必要になってくるという両面性を持つことに通じる点と言えよう。

<!-- その上で、この時代にはサブルーチンやループなど、コンピューターの基本的な制御構造自体を、ハードウェアに依存しない形で記述できることそれ自体がFORTRANのような（当時としては）高級言語の特徴だったため、そうした機能はFORTRANをそのまま利用していたと見なすのが自然だろう。
 -->


<!-- > Pass Iでは、作曲家がコンピュータに挿入した順番にカードが処理される。Pass IIでは、音符データが演奏される時間順序に並べ替えられる。したがって、Pass IIでは、例えばテンポを変えるなど、時間の関数である変換を適用するのが便利である。

> パスIIIは演奏段階であり、作曲家が直接指定した音符やコンピュータが生成した音符がオーケストラプログラムに提示される。その出力は直接的に音に変換されるようになっている。数値サンプルは磁気テープやディスクに保存され、その後音に変換される可能性が高い。また、この図では、デジタル磁気テープによる各パス間の通信が示されている。もし、ディスクがあれば、これらの仕事はもっとうまくいくだろう。
 -->


## マシューズが想定したコンピューターと音楽の関係

MUSICシリーズは世界で初めてPCMベースの音声信号処理を実現したシステムである。しかし、PCMが理論上、サンプリング周波数÷2の範囲内で、量子化ビット数に応じたシグナル-ノイズ比の範囲内でならあらゆる種類の音を表現できるからといって、必ずしもPCMをベースにしたMUSICがその範囲内で理論上どんな種類の音楽も実現できるシステムだと結論づけるには留保が必要である。

なぜなら既に述べたようにMUSIC Iというプログラム自体は三角波にエンベロープを掛ける程度の信号処理しか実現できなかったのであり、仮に今日十分な計算速度を持つコンピューターを用いて、無限にオシレーターが利用できたとしても、三角波の音量制御だけができるソフトウェアで、あらゆる種類の音楽を生成できると言い張るには無理があろう。仮にMUSIC Vに至っても、例えば`NOT`パラメーターとして利用できるパラメーターの配列の数は最大29まで、のような実質的な制限は存在している。"理論上無限に"を言い張ることにはあまり意味がない。これは今日あらゆる種類の音声合成、音楽生成プログラミング環境に対しても同等に言えることであり、MUSIC Iが理論上無限を謳うのならば、例えばチップチューンにおけるPCM再生のテクニックを使えば、それも理論上無限を言い張ることができる。同様に、Pilot ACEで作られていた奇妙な音楽は、PCMでも再現できる波形の範疇のはずだが、それをMUSICや今日のあらゆる種類のプログラミング環境にヒントなしで辿り着く可能性は直感的に皆無であろう。

それでも、PCMという符号化方式が持つ実質的にあらゆる種類の音を表すことができるというテーゼは、音楽のためのプログラミング環境が持つ（かりそめの）普遍性を知らしめるために重要な役割を果たしていたことは間違いない。もちろん、マシューズ自身も、Stored Function、UGen、Noteという抽象化の方法が必ずしも唯一の方法ではないことを示唆してはいるが、当時としてはこの抽象化方法が実行速度と、表現可能な範囲のバランスを考えると理に適っていたのだ[@Mathews1969,p39]。実際マシューズはMUSICを制作するにあたって、これまで既存の楽器では奏でられなかったような新しい種類の音が作れることに強く関心を持っていたことを語っている。

> 本質的に、サンプリング理論はサンプルから音を作るにあたって本当に限界が存在しないことを示している。人間が聴くことのできるあらゆる種類の音を、正しい数値、正確性、サンプルの組み合わせによって作ることが可能であるため、コンピューターは普遍的な楽器なのだ。他の楽器は、例えばバイオリンなどはとくに、美しく愛らしいものではあるとはいえ、常にその音はバイオリンのような音しか出せないし、最低でもバイオリンじゃないような音を作ることがとても難しい。[@Park2009,p11 筆者訳]

後半の発言はほとんどトートロジーのようにも聞こえるが、このようなコンピューターが理論的には無限の種類の音を出せるということはクセナキスをはじめとしたコンピューターを音楽に応用する可能性を肯定するものたちの典型的な謳い文句である。

またマシューズは同時に、自身がバイオリン演奏をしていたが決して演奏が巧くはなかったため、身体的卓越を必要としない仕組みを作りたかったこと、さらに作曲家が曲を作ってもオーケストラに演奏してもらえる機会がなければ発表できなかったり、何年も経ってからようやく初めて演奏されるという状況に対するひとつの解として、作曲したものをコンピューターに演奏させるという2つのモチベーションを挙げている。

> 私の興味は2つあった。
> ひとつは、私はバイオリン演奏をいつも好んできたが、それは人生のうちほとんどはアマチュアの弦楽四重奏の中でのことで、決して巧くはなかった。だからほとんどの楽器が要求してくる、そういった手先の器用さ〔manual dexterity〕をほとんど必要としないような、より良い音楽が作れるようになりたかった。
> またこれまでたくさんの作曲家がオーケストラのための曲を書いたが未だに一度も演奏されたことがなかったり、何年も経ってからやっと演奏されたという状況があるのを感じていた。だから（コンピューターが）作曲家が書いたものをほぼ即座に聴けるような手段を提供できるのではないかと思った。[@Park2009,p10 筆者訳]


これらを総合し簡単に言い換えると、マシューズが考えていたコンピューターを音楽に用いる理由とは次の3つのようになるだろう。

まず1つはこれまで存在しなかった新しい音楽の探求だ。これはサンプリング定理に基づくものでない、音価レベルでのアルゴリズミック・コンポジションにも共通して言えるモチベーションと言える。2つ目は身体拡張、あるいは自動演奏の追求である。これは今日のNIME研究のような、コンピューターを用いて音を生成するための身体との境界面：インターフェースを作ることによって音楽と身体の新しい関係性を発見するものだ。これは最終的に1つ目の新しい音楽表現につながることもあるし、逆に、表現自体はこれまでも存在していたが演奏に高度な技能が必要とされるものを誰でも演奏できるように、といった既存の音楽様式の再生産につながるモチベーションにも読み替えられる。

そして最後の作曲家のためのオーケストラに代わるものとしてのコンピューターとしての視点は、作曲におけるクイック・プロトタイピングのしやすさと表現するのがわかりやすいだろう。ただし、もう少し本論文の興味に引き付けて読んでみると、**コンピュータ以前はオーケストラという装置が（とくにクラシック音楽に由来する）作曲のインフラストラクチャとして機能していた**という視点を導入できる。つまり、有名な作曲家であればオーケストラに頻繁に演奏してもらえる機会に恵まれ、そうでなければ作曲家は楽譜上に音を配置し、ピアノなど少ない数の楽器である程度のシミュレーションをしながら作曲することしかできないという状態だったと言える。特に実験的な作品、すなわち既に共通して使われている楽譜上の記述では足りない表現を指示しようとしたり、大人数が演奏してみてはじめて曲の修正ができるような複雑な作品を作ろうとしたときにはオーケストラに実際に演奏してもらえるか否かは思い通りに作品が作れるかどうかに大きく関わっていたということができるだろう。

こうしたマシューズの発言を読むと、確かにMUSICの取り組みは初めて意図的に音楽を生成するためのシステムを構築する試みであったし、マシューズ自身もPCMをベースにすることによる音楽生成の可能性の広がりを強く意識してはいた。しかし一方で、例えばエジソンのフォノグラフがはじめから音楽の記録再生だけではなく口述筆記やアーカイブのような様々な用途を想定していたように[@Fukuda2015]、マシューズも必ずしもコンピューターを用いた新しい種類の音楽生成だけに着目していたわけではないことも強調しておく必要がある。マシューズは今日ではコンピューター音楽の父のように位置付けられているものの、当時にしてみれば、作曲家としては位置付けられておらず、当人もアマチュアとして音楽を嗜む科学者であったことを自認している。

これは、CSIRACでの音楽生成を行なったエンジニアのひとりであるジェフリー・ヒルが音楽経験を持ってはいた、という状況とそう大差ないと認識する方が自然だ。ドーンブッシュはこのようにマシューズとMUSICの功績を安易に "世界初のコンピューター音楽"という言葉に縮約しないように警鐘を鳴らしているのである[@Dornbusch2017,p305]。

マシューズの功績をなるべく正確に捉えるのであれば、それは初めてPCMをベースにしたシステムを作ったこと、Stored Function-UGen-Noteという最低限かつ表現に広がりのある抽象化方式を考案したこと、そして、**初めて、職業としての作曲家へ研究に積極的に関わってもらうよう働きかけたこと**である。

> 最初にこれらの音楽のためのプログラムを作ったとき、当初のユーザーは作曲家ではなく、心理学者のガットマンとジョン・ピアース、そしてわたしという、基本的に科学者だった。私たちは、音楽家がこの言語を習得し、これを使って自分自身を表現できるかどうかを知りたかった。そこで私たちは、この実験をやってみたいという冒険心あふれる音楽家や作曲家を探した。[@Roads1980,筆者訳]



# 1960~1970年代の現代音楽とアカデミア

60年代以後の現代音楽

- 難解かつサイエンティフィックな方向性、ミルトン・バビット、ブーレーズ（アカデミアに偏り、聴衆から離れていく、Who cares if you listen?）
- ポストモダン：ケージ、チューダーなどフルクサス側

ただし後者も、ケージとバックミンスターフラーが一緒に関わってたりとかで、いわゆるデザイン・サイエンス的運動にどちらも入れられるあたり、Scienticismは音楽の中ではさらに広がりがある

その中で言うとマシューズは微妙な立ち位置、ガッツリ研究所にいる正統派みたいに見えるけど、バビットとはある意味対照的

マシューズとケージはニューヨーク同士だし一応交流もあった[@Park2009]

マシューズはIRCAMに音楽ディレクターとして一度赴任し、今でいうNIMEの方向性に近い研究を提案したが、IRCAMの人たちは高速リアルタイム可変信号処理できるプラットフォーム（4x）の方に執心しておりうまくいかず、その後もコラボレーションは続いたもののディレクターは1年で降りる（以後はずっとフランス人ディレクター）[@Born1997]

# MUSIC 11、CSound（Vercoe）

MUSIC 11でsignal/controlの区分が導入される


# ボーンのIRCAM 4Xプログラミングの分析

リアルタイムで処理できない程度に重かったこと
楽器のモデルを計算できる理論が存在しなかったこと→Rissetに始まる物理モデリング合成研究（Analysis-Synthesisアプローチ）

80年代にしては計算のシンプルさに対して複雑な音色が出せるFM合成とかはあったが、コンピューター音楽言語は大学や研究所に限られていたし、リアルタイム性にもまだ欠けていた extreme mediation, both temporal and conceptual

モデルがしっかりしていないと音を改善することができないけど、トライ&エラーに時間がかかるのでモデルの妥当性を確かめるのにも時間がかかってしまうパラドックス


技術のトリクルダウン：研究所でパイオニア的テクノロジーが発達し、それがコマーシャルに低価格化していくという考え方をIRCAMは持っていた
一方で、マシューズのようなインターフェース系のを拒否し、かなり基礎的な部分に投資するという矛盾

その上で、当時出てきたばかりのMacintoshの登場や、コンピューターアーキテクチャの発展にかなり振り回されている（むしろ逆ではという状況）


ヤマハの人がCXのデモにIRCAMにきた話

4xは当時最強のスペックだった→これ何がそうさせたんだろう？オシレーターとかはソフトウェアで仮想化できたんだろうか？西野の文献読む必要あり→できた。Variable Digital Signal Processorの話

OSから作ってた　ハードは凄かったがソフトとペリフェラルが弱い

Chant 歌声合成、Formes PatchworkとOpenMusicの手前

Chant／Formesのグループからは、音楽概念の高度な発達というコンピューターのポテンシャルを無視していると思われてた

Chant/FormesはLISP製、VAX/UNIXシステムで動いていたノンリアルタイムシステム
    users could create their own "personalized environment"
    object oriented

The use and the development of software involve the writing of coded instructions within a software language or the creation of a completely new language, within the context of a hierarchy of such languages. At each of the hierarchy a traslation occurs between any two adjacent language or levels of code. Instructions from the language at a higher level mus be translated into aform whereby they can be "read" and executed by the lower-level code or language without any (or with minimal) loss of "meaning"

The hierarcy of codes that normally operates in computer software include, at the lowest level, machine code, the instructions that drive athe hardware, writtedn in binary form; at the next level up, assembler code, made of mnemonic abbreviations of machine code; above this, the general operating system that provides a basic framework and set of servicies; and above this, any of the major lkanguages such as FORTRAN, Pascal, C, or LISP.

〜〜

Computer music software such as that used and produced by IRCAM adds yet a further level of mediation, hierarchy, and translation, sincce the music languages are themselvels based upon, or written in, established general languages.

Thus, Music V is written in FORTRAN, Cmusic in C, IRCAM's Chant in FORTRAN, and Formes in LISP.


Chantを使うにはFORTRANの知識も必要だったし、Formesを使うにはLISPの知識も必要だったので、それを勉強するためにまずLISPについて勉強しなくてはならない →**つまりこの時点ではまだ、後にMcCartneyがSuperColliderの設計指針として挙げる、プログラミングというコンピューターハードウェアを使うための専門的知識が必要な事項をencapsulateし、音楽のNotationに集中させるという意味での音楽”言語”の概念は達成されていなかったということが言えるだろう。**



# 90年代


音楽プログラミング言語の歴史において90年代は、パーソナルコンピューターでも高性能化と低価格化を背景として、4Xのような研究所でしか使えない高級なハードウェアでなくとも、DSP専用の計算ハードウェアを用いないCPU処理でのリアルタイム音声合成が可能になった時代である。

ここでの高性能化と低価格化に関しては、具体的にどの製品の登場によってブレイクスルーが発生したという具体的な事例があるわけではないが、例えば久保田は「PowerPC G3やPentium IIといったCPUによって、CDクオリティのサウンド処理に必要な、ある種のスレッショルドを超えたのだろう」と説明している[Kubota2017,88p]。

SuperCollider

# 2000年代

## より詳細な時間制御（ChucK、LC、Gwion）

## 低レイヤの拡張（Faust、Extempore、Kronos、Vult、Soul）

## 高レイヤの拡張（TidalCycles、Sonic Pi、IXI、Gibber、Foxdot、Takt、Alda）



# 小括

音楽プログラミング言語は、元々は50~60年代のMUSIC Nシリーズのような音楽をコンピューターで使うためのシステムとして生まれたものであるが、それはリアルタイムで音を生成できるわけではなく、磁気テープに書き出した結果を改めて再生するようなシステムだった。そしてこれは必ずしも今日の音楽プログラミング言語に限らず、DAWのような音楽制作ソフトウェア全般の祖先となるようなものだったと言えるだろう。

やがて70~80年代には、音楽プログラミングの歴史は2つに分岐する。一つはの時期普及し始めたパーソナルコンピューターにおける、音声合成ICチップを用いたアマチュアを中心とするチップチューン、もう一つはIRCAMに代表される研究所レベルにおけるプロフェッショナルな現代音楽の文脈における研究、こチップチューンに使われた音声合成チップは非常に限られた数のオシレーターに対してCPUがその周波数や発音タイミングの命令を行う構成をとることで、計算コストのかかる音色の制御をハードウェアに任せて（既に成立している音楽体系である）五線譜的な表現に注力できるようにした。
一方でIRCAMでの取り組みでは、4A,4B,4Cと言ったハードウェアではチップチューン同様にハードウェア的にオシレーターの上限が決まっているような構成から、4Xでハードウェア的にはオシレーターを持たず抽象的な計算ユニットだけでリアルタイム音声合成ができるようになったという進展が見られた。
チップチューンとIRCAMでの取り組みを対比すると、この時代の取り組みはいずれもリアルタイム性を重視する代わりに、表現の自由度かハードウェア的コストのどちらかを犠牲にしていたと見ることができる。チップチューンではオシレーターの数を超える発音はできないし、4Xは自由度が高い代わりにOSから独自で構成された非常に複雑な構成をしていた。このようにどちらかを犠牲にしてでもリアルタイム性を重視したのは、音楽をコンピューターを用いて生成するにあたって、人間は何かしらの記号、シンボルを解すことでしかコンピューターに対して命令を与えることができないという理由があった。つまり、音楽を生成するモデルを人間が考えコンピューターに実行してもらうプロセスが必要になるが、このモデルが妥当かどうかを検証するにはまたコンピューターに実際に実行してもらうプロセスが必要となる。（特にIRCAMのような新しい表現を追求する場においては）例え金銭的にコストがかかったとしてもリアルタイムにトライアンドエラーができる場所があることが重要だったと言えるだろう。

そして、90年代には、パーソナルコンピューターでも高性能化と低価格化を背景として、DSP専用の計算ハードウェアを用いないCPU処理でのリアルタイム音声合成が可能になった。またこの時期にパーソナルコンピューターに用いられるオペレーティングシステムもWindows、Macintoshなどに落ち着き、一つのCPUの中で複数のアプリケーションを実行するマルチタスキングのシステムもインフラストラクチャとして安定してきて、ユーザーアプリケーションとして音声合成を扱う場合にはプリエンプティブスケジューリングの中で可能な範囲でのリアルタイム性の追求という形に落ち着いたと考えられる。
またこの時期にSuperColliderが汎用プログラミング言語において進んできた構造化プログラミングの手法を音楽プログラミングのために取り入れた。ここで初めて、音楽プログラミング言語はプログラミングという専門的なタスクから音楽、音声合成の抽象化という行為を引き剥がす機能を持つようになったと言えるだろう。


2000年代以後の音楽プログラミング言語は大きく分けてChucKやLCに代表されるプリエンプティブスケジューリング環境下での正確なイベント制御、Faustに代表されるUGenレベルの低次の自己拡張性の追求、SuperColliderクライアントに代表される高次の自己拡張性の追求おいう3つの方向性があったとまとめられる。