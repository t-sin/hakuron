# 音楽のためのプログラミング言語とはなんなのか？

本章では音楽のためのプログラミング言語（Programming Language for Music：PLfM）の存在論の通時的な整理を行う。

## PLfMの定義

ここで初めて、ここまでは字義通りの定義に留めて暗黙的に用いてきたPLfMという用語をより明確に定義することにする。音楽のためのプログラミング言語は既存の文献では、Computer Music Language[@McCartney2002;@Mcpherson2020]、Language for Computer Music[@Dannenberg2018]、Computer Music Programming Systems[@Lazzarini2013]などの呼ばれ方がされているが、それぞれの語の使用に明確なコンセンサスがあるわけではない。

とはいえ、なぜ既存の語を捨ててまでPLfMという新しい語を用いる必要があるのだろうか。これには少なくとも3つの理由がある。

1つ目は、Computer Musicという語を用いることによって、そのための道具が、コンピューターを用いることで新しい音楽表現を追求するというイデオロギーを持つ、特定の音楽様式と結びついてしまうことを避けるためだ。今日ではあらゆる音楽制作と再生のためにコンピューターが用いられている以上、あらゆる音楽が**弱い意味でのComputer Music**と呼ぶことができる。しかしそれらの多くはコンピューターでなければ不可能な、コンピューターというメディア固有の表現を行っているわけではない。同様に、たとえばFaust[@Orlarey2004]のような、信号処理のアルゴリズムを抽象化することに特化したプログラミング言語は新しい音楽表現を必ずしも目的としていないが、その技術的要素の多くはComputer Musicのための言語と共通するところがある。またそれらのコンピューター音楽の文脈とは、3章で解説した、Macintosh以後のユーザーの誕生以前から行われてきた取り組みであることを頭に入れる必要がある。つまり、プログラミングという作業がユーザーから隠蔽される以前の1950〜1970年代におけるコンピューターの利用することはほぼ必然的にプログラミングを行うことでもあった。それゆえ、1990年代以降に現れてきた、パーソナルコンピューティング環境で使うことを想定された言語とは、コンピューターを使うためのインターフェースの中でよりユーザーフレンドリーで直感的な選択肢があるにも関わらず、それでもなお人工言語という形を選んでいるという面が強調されて然るべきである。

2つ目はPLfMという枠組みを用いることで、これまでの文献では比較対象に入れられること自体が少なかった、MML:Music Macro Languageのような言語を、五線譜上の記法を直接的にテキストに置き換えたような、単にコンピューター上のテキストというフォーマットで音楽を表すことを目的とした言語たちも、チップチューンのような広い意味でのコンピューターを用いた音楽文化を作るための要素として議論の土台にあげることができる。

3つ目は、Computer Musicという語を避けたのと同じように、Programming EnvironmentやProgramming Systemといった語を避ける理由である。これは、音楽のためのプログラミング言語といった時に、たとえばMaxのような、ある特定のアプリケーションを想像するニュアンスを抑えるための選択だ。たとえば、汎用プログラミング言語の理論においては、プログラミング言語、と言った時にはその言語を実行するためのソフトウェアやプログラムのことを必ずしも指していない。たとえば同じC++という言語であったとしても、それを実行するソフトウェア（コンパイラあるいはインタプリタ）はGCC、Clang、Microsoft Visual C++といったように複数存在し得るからだ。これらのコンパイラは、どれもC++の厳格な言語仕様で定まっている通りの動作をするが、言語仕様で未定義とされてる動作はそれぞれ異なるし、コンパイラが出力する実行バイナリ（≒アプリケーション）の中身は同じソースコードだったとしても異なる。音楽プログラミング言語においては、基本的にある言語＝特定のアプリケーションであることがほとんどだが、根本的にはアプリケーションの設計実装という作業とプログラミング言語の設計実装という作業は異なり、本研究が対象にしたいのは言語（Language）の設計なのだ。こうしたニュアンスを込めて筆者はEnvironmentやSystemという語を用いないことにした。

極論を言えば、Faustのような厳密に意味論が定義されている言語においては、コンピューターを用いなくてもそのソースコードを手作業で解釈し実行することが可能だということを考えれば、プログラミング言語はコンピューターを使うための道具であることは間違いないにせよ、徹頭徹尾人間のための道具でしかない。だからComputer Music Languageとも、Computer Music Programming Environmentとも、呼ばずに、ただ音楽のためのプログラミング言語：Programming Language for Music、PLfMなのだ。

## PLfMに関連する既存のサーベイ

まずは、既存の音楽のためのプログラミング言語に関連したサーベイについてを列挙しておく。

既存の音楽のためのプログラミング言語や、コンピューター音楽のためのシステムのサーベイとしては[@Roads2001]や[@Nishino2016]、[@Dannenberg2018]が存在する[^authorsinfo]。ローズによるコンピューター音楽の手法を網羅的に記述した著書では、（原題：Computer Music Tutorial）初期のコンピューター音楽の試みが過去のインタビュー[@Roads1980]なども踏まえて記述されている。西野と中津のサーベイではこれよりも詳しく、1940年代から2010年代まで時系列順でコンピューターを用いた音、音楽生成の試みの歴史が網羅的に記述されている。ダネンバーグの文献は西野同様歴史的に代表的な言語の紹介と、より言語の特性や分類について構成的な整理が行われている（これは主に第5章で足掛かりとする部分である）。

[^authorsinfo]:なお、西野はLC[@Nishino2014]の、ダネンバーグはNyquist[@Dannenberg1997]他多数の言語の設計者でもある。このことからもやはり、音楽プログラミング言語の歴史を記述するには基本的にその設計や実装に関する知識や経験が必要になっていることが窺える。


またメディア考古学的検証を行う本研究において重要な1940〜1960年代までのコンピューター音楽黎明期歴史の整理が詳しくなされている文献として、オーストラリアにおける初期のコンピューターを用いた音楽生成の試みを研究したポール・ドーンブッシュの調査がある[@Doornbusch2005]。この調査を契機として2005年以降、イギリスの放送局BBCやアーティストのirrlicht projectにより最古のコンピューター音楽生成の検証が進んだ[@Fildes2008][@Irrlicht2015]。これらの調査はのちに日本において、田中がチップチューン（1980年代の初期パーソナルコンピューターやゲーム機において用いられていた音声生成用のICチップを利用した音楽ジャンル）の歴史的起源として位置付けることで、改めてまとめられている[@Tanaka2017][^chiptune]。またドーンブッシュ自身もこれらの検証を踏まえ改めて黎明期の歴史を整理し、アメリカ中心的コンピューター音楽の歴史観を批判的検証を行った[@Doornbusch2017]。

[^chiptune]: 本研究内であまり触れられなかったが、チップチューン自体は日本での独自の発達を遂げたこともあり、田中も記述するように、いわゆるコンピューター音楽の歴史としては傍流の歴史として扱われていもるが、その文化の中から生まれたMML（Music Macro Language）と呼ばれる、テキストで音楽の内容を記述するための言語の系列は、そのハードウェアやプラットフォームごとに仕様が異なる派生の複雑さや、ゲーム音楽研究との接点、その後のTakt[@Nishimura2014]のような、より汎用プログラミング言語の理論へ近づいた言語など各方面に影響を与えていることを考慮すれば、音楽のためのプログラミング言語の歴史研究や、より今日正統な発展を遂げたと思われている音楽文化との関連性の批判的検証が必要だと考えられる。チップチューンをメディア論の視点から検証している近年の研究には[@Hidaka2021]がある。

本研究ではこれらの研究を参照しつつも、3章で見てきたメディア装置としてのコンピューターと、パーソナルコンピューティングの歴史との対応関係から、時代ごとの取り組みの焦点の変化をより明確に描き出すことを試みる。

## PLfM史概略

そのために本章では、PLfMの歴史的推移の大枠を捉えるために3つの視点での区分を導入する（1.2.は西野らのサーベイで使われていた観点である）。

1. **リアルタイム性**：一度結果を書き込んでから読み出すのではなく、実時間で音声信号の処理ができるかどうか。（そもそも音声信号処理をしない、というのも含め)
2. **可変DSP**：音声合成のためのモジュールが有限か、（理論上）ソフトウェアで無制限に利用できるか。
3. **Lab or PC**：研究所における取り組みかパーソナルコンピューティングか

これらの取り組みが歴史的にそれぞれ異なる方向性の試みが重なるように行われてきた。2、3章同様に、歴史の見取り図を[@fig:plfmhistory]に示す。

![PLfMの歴史を、リアルタイム性、可変DSP、Lab or PCの視点で分類した概略。](./img/plfmhistory.pdf){#fig:plfmhistory width=100%}

1950〜1970年代の計算機自体の黎明期は全ての信号処理をCPU上で行うと、例えば10秒の音声信号の計算結果を得るのに10秒以上かかってしまう状況だった。そのため、研究内容は例え非リアルタイムだとしてもソフトウェア上で音声を計算して合成する可変DSPを重視した試みと、それよりも、たとえ限られた範囲での合成しかできなくともリアルタイムインタラクションを重視する試みという、トレードオフの選択があった。この両立は研究所レベルでは1984年のIRCAM 4Xというワークステーションによって成されたというのが一般的な認識だ。

そして、行われていた場所が大学などのラボラトリーか個人かという違いも重要な点である。すでに述べたようにMacintosh以後のパーソナルコンピューティングこそがプログラミングという作業をコンピューターの利用から分離したからである。1980年代以降のパーソナルコンピューター黎明期は、個人向けの低価格コンピューターにおける計算機リソースという制限から、1970年代までのラボラトリーにおける状況が繰り返されていた。つまり、音声合成ICによる限られた範囲での音楽制作の試みである、チップチューンがこの部分に当てはまる。

1990年代に入って以降は、パーソナルコンピューターでも専用の音声合成ICなしにリアルタイムの信号処理が可能な程度に性能向上と低価格化が進み、SuperColliderや（OpCode社によって商用化された）Max、Puredataなどよりハードウェアに依存しない形式のPLfMが登場した。

2000年代以降はさらに3つの流れが発生する。1つはより**正確なイベントスケジューリング**である。これは、専用のICを用いず音声合成をすることによって、処理タイミングがOSのスケジューラーに依存するようになり複雑さと不確定性が増したことと、この頃登場したmicrosoundのような、音声データを細かく切り刻み再配置するような手法に正確なタイミング制御が求められたことという、技術的背景と表現からの要求の両側面がある。またこの時期、汎用プログラミング言語のためのツールや理論が発展することによって、音楽のための言語にもその技術が流入してくることになった。その結果、Faustに代表される、これまでブラックボックスとして隠蔽されていた信号処理のプリミティブな部分をハードウェアに依存せず記述可能な、**低レイヤー方向の拡張**を目的とした言語の登場につながった。また同時に、この時期登場してきたRubyやHaskell、Pythonなど様々な汎用性の高い言語をフロントエンドとして用い、SuperColliderを音声合成エンジンとしてだけ利用することで、言語の意味論をこれまでのPLfMと大きく変える、**オルタナティブな抽象化**を試みる言語も多く現れることになる。

本章において注意して読んで欲しいのは、時代ごとの変遷の見通しをよくするために3つの視点の区分を導入してはいるものの、それらはむしろ、キッパリと区分できない境界線のにじみを強調するために用いているということだ。例えば、計算リソースが限られた時代におけるリアルタイム性と可変DSPのトレードオフは、そのどちらに重きを置いていたかという、各システムのスタンスを理解しやすくするが、現実には微妙な例がいくつもある。これには例えばチップチューンで用いられるテクニックのことを考えると理解しやすくなる。

初期のチップチューンはPSG（Programmable Sound Generator）という、CPUからの命令で、3~4種類の音程や発音タイミング、エンベロープ（音量変化）の形を制御できるシンセサイザーを用いることで作られる音楽だった。しかし、エンベロープによる音量調節をCPU側から、つまりソフトウェアの命令で非常に細かく制御してしまうことで、例えば矩形波しか出せないようなチップでも疑似的に三角波やノコギリ波に近い波形を生み出せる[@Tanaka2017,p73]。この疑似三角波は純粋な三角波とも異なる音色を持つため今やチップチューンらしさを象徴する音色でもある[^magical8bit]。この方法論はさらに突き詰めると、音量制御を細かく制御することで任意の音声波形を再生するというテクニックにもつながる[@Holst1998]。このように、リアルタイム性と可変DSPのトレードオフは、コンピューターが理論上はどんな処理も可能であることを謳う以上、同じシステム同じハードウェアだとしてもその境界線が揺らぐことは防ぎようがない。

[^magical8bit]: 例えばチップチューンとジャズ、ポップスを掛け合わせた音楽を作っている8bitミュージック・ユニット、YMCKのメンバーであるYokemuraが開発するMagical 8bit Plugというソフトウェアシンセサイザーは疑似三角波が作れることを売りにしている。 "三角波を8bit機の処理能力の限界内で擬似的に再現した波形です。ベースなどに適しています。波形のギザギザによるビービーというノイズが特徴で、8bitっぽさの演出には欠かせない音色です。"[@Yokemura2020]

同様に、研究所における取り組みと個人の取り組みという区分も、直感的に理解しやすい分け方として用いたものではあるが注意して読む必要がある。そのためには、[@fig:plfmhistory]における1990年代以降の取り組みをやや個人の側に寄りながらも、研究所にもまたがる形で配置している意図を説明しなくてはならない。

現在活発に使われているPLfMは、コミュニティベースで開発されていることはあっても、その開発は当初の設計者が継続して中心的役割を果たしていることがほとんどで、かつその多くは（筆者も含めて）大学などの研究機関、あるいは企業に所属している人間である。言い換えると、組織の力を借りつつ個人のスキルを活かすことでどうにか成り立っているという状況である。（唯一の希有な例外としてはSuperColliderがあり、これについては本章内で改めて解説するのだが、）こうした視点に立つと、黎明期の研究所におけるコンピューター音楽の研究の中にも、非常に個人的な興味の中から生まれてきた言語やソフトウェアもあるのではないかと考えられるだろう。しかしこの視点を用いて筆者は、伝統的なコンピューター音楽の権威主義的性質を否定しようとしているわけではない。第2章でロスナーの個人主義批判を見てきたように、研究所における音楽のためのシステム開発の中にも、論文には名前が残らない形での様々な貢献があったはずだということには常に注意を払い続ける必要がある。さらに、研究所中心なコンピュータ音楽史に対するオルタナティブの提示という構造は、PC以後のコンピューターを用いる音楽実践を、安易に権威化されたコンピューター音楽からの個人による民主的取り組みの萌芽と捉えることで、逆にチップやコンピューターを規格化/製造する企業などによるインフラストラクチャが生み出す、研究所とは異なる形の権力構造から目を逸らすことにもなりかねない。

集団の中の個人性、これが本章を考える上で持つべき視点である。音楽を作り出す個人は常にある集団の中の個人でしかありえないし、同様にある社会集団が生み出すテクノロジーもまた、特異な個人の寄せ集めである。

# 最古の "コンピューター音楽"の解体

本研究が焦点を当てるのは音楽のためのプログラミング言語であるとはいえ、直接的に言語という手段を用いない時代まで遡って、コンピューターと音楽の関わりを見つめることは背理法的に、どこからが音楽のためのプログラミングをよりそれらしくしたのかを明らかにしてくれるだろう。

同時に、コンピューターを用いた音楽の生成の試みを掘り下げるということは、ここから先は "音楽的"な試みであるとされる一般的理解の境界線を明らかにすることである。これは音楽土木工学における問いの1つ、音楽に関わる技術を作ったりメンテナンスする人は一体どこまでが "音楽的"な試みをしているのだろうか、という問題につながる。音楽学者のクリストファー・スモールは音楽を作られた作品ではなくて行為として捉えるためにミュージッキングという概念を導入した[@Small2010]。スモールの考えでは音楽にまつわる様々な行動＝ミュージッキングには例えばコンサートの中でチケットもぎりを担当する人も等しく音楽実践の一部に位置づけられる。スターンはこの考え方を音楽に関わるテクノロジーにも同様のことが言えると指摘する[@Sterne2014]。例えば楽器やソフトウェアを作ったりする人は音楽産業の中にはカウントされないがどうしてなのか、そもそも本当に "音楽産業"など存在しているのか、と。ではこの問いは突き詰めれば当然このようになるはずである。コンピューターそのものを開発する行為も音楽的行為の中に含まれる可能性はないのだろうか、と。すなわち音楽的行為の外側からの解体である。

この試みのために、まずは歴史を電子計算機の登場した時代である1950年付近まで遡ることにしよう。本稿で取り扱う "コンピューター"とは、ひとまず、リレーのような機械的スイッチではなく真空管やトランジスタのような電子的スイッチを用いて計算を行うもの以降という区分を取り扱うことにする。具体的な例としては、アイオワ州立大学で1941年に開発されたABCとペンシルベニア大学（通称ムーア・スクール）で1946年に開発されたENIACなどが挙げられる。ENIACが当初弾道計算という軍事技術のために作られたのはよく知られている事実であるが、同様に黎明期のコンピューターの用途は弾道計算を含めた数理モデルのシミュレーションや暗号解析であった。それにも関わらず、コンピューターで音楽を鳴らす事例はENIACのわずか3年後の1949年にすでに行われていることが知られている。それが、BINACである。

BINACはENIACを開発したジョン・プレスパー・エッカートとジョン・モークリーが立ち上げたエッカート・モークリー・コンピューター・コーポレーション（EMCC）によって作られた電子計算機である。BINAC開発に携わったエンジニアのルイス・ウィルソンは、1990年にチャールズ・バベッジ・インスティテュートで行われた通称UNIVACカンファレンスと呼ばれる会議でBINACの制作段階を振り返り[^univac]、近くに置いていたAMラジオが、真空管をスイッチングする際に発生する微弱な電磁波を拾うことで規則的な音を出すことに気がついた。これを逆手にとって、ウィルソンはコンピューターの出力にスピーカーとパワーアンプを繋ぎ検証のために利用し始めたのだ。黎明期のコンピューターにはこうして、デバッグ用途などにスピーカーが取り付けられることが珍しくなかった。さらにこのスピーカーの機能を用いて音楽を鳴らしたのがフランシス・エリザベス・スナイダー・ホルバートン（通称、ベティ・スナイダーもしくはベティ・ホルバートン）であるとされている[^betty]。ホルバートンはENIACの実際の利用におけるプログラミング作業を行なった6人の女性の1人として知られる人物で、エッカートやモークリーとともにBINACの開発作業にも関わっていた。彼女がアメリカで「Happy Birthday to You」の次に定番の歌である「彼はいい奴だ（For He's A Jolly Good Fellow）」を演奏したのは公的なパフォーマンスというよりも、仲間内のエンジニアを驚かせるためにしたこととされている[@UNIVACConf1990,p72][^binac]。

[^univac]: UNIVACはEMCCが作った世界初の商業コンピューターで、当初先に作られていたUNIVACの制作段階で資金が枯渇し、UNIVACの発注元とは別のノースロップ社へ納品するために先にBINACが制作された。このためBINACの方を世界初のコンピューターと見なす場合もあるが、BINACは納品後は動作不良などが多く実用的にあまり使われなかったことからそこには数えいれられない場合が多い。

[^binac]: [@Irrlicht2015]。田中はIrrlicht Projectがはじめてホルバートンが世界初のコンピューター音楽の実践を行なったことを指摘している[@Tanaka2017,p13]が、後述の2012年の宮崎による文献で既に、（ホルバートンが音楽を鳴らしたことには触れていないものの、）ウィルソンのラジオから聞こえる動作音の部分を引用していることは付しておく。なお、田中の文献の該当部分には引用者名の誤植があることにも注意されたい。

[^betty]: ただし、ドーンブッシュはこのカンファレンス内での発言を除いて音楽を演奏したとされる記述が見当たらず、計算機が音楽を演奏するという、当時にしてはわかりやすい目を引く活用例がどこにも記述されていないことから、ホルバートンの演奏については、行われた可能性があった、と言うに留めるべきだと注意を促している[@Doornbusch2017,p299]。もっとも、BINACを最古に位置付けることに消極的な立ち位置に立つことで、ドーンブッシュの研究対象であるCSIRAC（と同年のFerranti Mark I）が最古のコンピューターによる音楽生成であるという主張につなげている側面も否めない。

宮崎はこの、アルゴリズムが作り出すビットパターンが生み出す音を聴く行為がプログラミングやその中のデバッグ作業に組み込まれている状況を "Algo**rhythmic** Listening"と形容し、ややもすると視覚表象文化の視点からの分析に偏りがちなメディアとしてのコンピューターを扱う実践[^chun]の中に、聴覚的な技法が取り入れられてきたことを分析している[@Miyazaki2012]。この視点は本研究のような、音楽の実践者が技術の極めて基礎的な部分を扱う学問を考える時に興味深い視点である。極めて誇張した言い方をすれば、**コンピューターを音楽制作に応用するまでもなく、コンピューターを扱うことはそもそも音楽的な実践だったのではないか**、という問いを立てる必要があるのだ。

[^chun]: 例えば、メディア研究者ウェンディ・フイ・キョン・チュンによる、ソフトウェアの持つイデオロギー性を視覚文化との兼ね合いから検証した "ソフトウェアについて、もしくは視覚的知識への固執"という論考を見よ[@Chun2005]。

実際、UNIVACカンファレンスの中には、UNIVAC開発に関わったランス・アームストロングによるこんな発言がある。初期のコンピューターのプログラミングの熟練に関して、統計数学を扱う研究者と、計算手（つまり電子計算機以前の人力による計算作業を行う人たち）のプロジェクトマネージャーを比べると、はじめは統計数学の研究者の方が覚えるのが早いのだが、4、6ヶ月も経つとどちらも似たようなものになるのだという。ところが、それとは関係なく、誰が気づいたかもわからないし、なぜかもわからないのだが、音楽の（おそらくは演奏の）技能が高い人はプログラミングの上達も速いという相関があるのだという。発言はさらにアームストロング同様UNIVAC開発に関わったフローレンス・クーンズとモデレーターでフンボルト州立大学のコンピューター史研究者ヘンリー・トロップを交えて、以下のように続く。

> アームストロング：まあそれはそれで、プログラマーたちをテストするために "弓とフィドルを持ってくるのを忘れずに。"なんてのはひどい話になってしまうわけだけど。（会場笑い声）

> トロップ：それは、後々には確かにそうで、トランジスタが世界を一変させてしまった後、誰もそんなことになるなんて考えもしなかったほどにプログラマーが必要になってしまって、彼らは本当に音楽専攻の学生たちを大量に採用したんだよ。

> クーンズ：確かに、音楽と数学には大きな相関がある。

> トロップ：でもそれは1960年代になるまでは起きなかったのだけど。[@UNIVACConf1990,p60 筆者訳]

トロップの指す、1960年代の音楽専攻の学生の大量雇用という事実の詳細に関しては不明であるし、音楽の技能とプログラミングの技能の相関性もひどく主観的なものではある。しかしそれでも筆者は、初期のコンピュータープログラミングの作業にAlgorhythmic Listening的な実践が入ってくることは必然だったと主張したい。着目すべきは、BINACと同時期のコンピューター、例えばUNIVAC I（1951）やオーストラリアのCSIRAC（当初の名前はCSIR Mark 1）（1950）、イギリスのACE(1952〜1953)などに使われていた記憶装置、音響遅延線メモリーである。

音響遅延線メモリーとは、何か物理的な状態を変えることでデータを保存するのではなく、水銀などで満たされたタンクの端に取り付けたスピーカーから、バイナリデータを超音波のパルスの有り/無しに対応させ発音し、その音を反対側の端に取り付けたマイクロフォンで拾い、またデータをスピーカーへフィードバックすることで、スピーカーからマイクロフォンへ届くまでの音波の遅延時間分のパルスを保存できるという、いわば延々と通信し続けることによってデータを保存するメモリーである。

筆者はかつてこの音響遅延線メモリーという、今や使われる理由のない技術を作り直す作品《Acoustic Delay (⇔) Memory》（2015、[@fig:adm]）と《送れ｜遅れ/post|past》（2016、[@fig:adm2]）を制作した[@Matsuura2016][^admother]。筆者はこの作品の制作過程で行なったサーベイで、ミシガン大学のコンピューター史研究者、ピーター・エクスタインが描いた、ENIACやBINACの設計者であり、音響遅延線メモリーの仕組みの考案者、ジョン・プレスパー・エッカートの人生史[@Eckstein1996]に着目した。そこで描かれていたのは、エッカートが音に関するエンジニアリングを続ける中でコンピューター制作にたどり着いたという軌跡である。宮崎が着目していなかった点として、ラジオに流れる電磁波を拾うまでもなく、音響遅延線メモリーという装置はそもそも音楽文化とテクノロジーの交差点上に生まれた装置であることが彼の生い立ちを知ることでわかる。

<!-- ![《Acoustic Delay (⇔) Memory》(2015)。ディスクリートICで作られた音響遅延線メモリー回路によって、展示空間に8bitのバイナリデータを保持し、Webサイト上からそのデータを読み書きできる。大きな音を出したりマイクを遮ったりすることで物理的にデータに干渉することもできる。](img/adm.JPG){#fig:adm width=80%}

![《送れ｜遅れ/post|past》(2016)。物理的には独立した2台の通信機能だけを持つ送受信デバイスが、空間に配置されることによってはじめて音響遅延線メモリーとして記憶装置の機能を果たす。](img/postpast.JPG){#fig:postpast width=80%} -->

\begin{figure}[htbp]
  \begin{minipage}{0.5\hsize}
    \begin{center}
        \includegraphics[width=1\hsize,keepaspectratio]{img/adm.JPG}
    \end{center}
    \label{fig:adm}
    \caption{《Acoustic Delay (⇔) Memory》(2015)}
  \end{minipage}
  \begin{minipage}{0.5\hsize}
    \begin{center}
        \includegraphics[width=1\hsize,keepaspectratio]{img/postpast.JPG}
    \end{center}
        \label{fig:adm2}
    \caption{《送れ｜遅れ/post｜past》(2016)}
  \end{minipage}

\caption*{{《Acoustic Delay (⇔) Memory》では、ディスクリートICで作られた音響遅延線メモリー回路によって、展示空間に8bitのバイナリデータを保持し、Webサイト上からそのデータを読み書きできる。大きな音を出したりマイクを遮ったりすることで物理的にデータに干渉することもできる。《送れ｜遅れ/post｜past》は物理的には独立した2台の通信機能だけを持つ送受信デバイスが、空間に配置されることによってはじめて音響遅延線メモリーとして記憶装置の機能を果たす。}}
\end{figure}


[^admother]: 筆者に先行して、アーティストのキム・ユンチュルが2005年に同様に音響遅延線メモリーを再構築する作品 "Hello World!"を作成している[@Yunchul2005]。他、本論文の執筆中に開催された、人工生命研究者の団体ALTERNATIVE MACHINEが行なっている展示 "ALTERNATIVE MACHINE"にて、音響遅延線メモリーを構築しその中にブロックチェーン上に記録されるNFT（Non-Fungible-Token）データを保存する作品「遅延記憶装置」を作成、展示しており、筆者は自身の制作の経験から技術アドバイザーとして関わっている。 https://7768697465686f757365.com/portfolio/wh015-alternativemachine/

エッカートは幼少期から音に関わるテクノロジーに強く興味を持っており、5歳の時に親からラジオを買ってもらい、7歳の頃には塹壕ラジオ（コイルとクリスタルなどで作れる簡単な仕組みのラジオ）を作り、13歳の頃にはレコード用のアンプリファイアーを制作していた。高校生や、ムーア・スクールに入学した頃にはすでに礼拝堂に鐘の音をスピーカーから流すためのサウンドシステムを構築したりして小金稼ぎをしていたともされている。しかも、特に着目すべきことは、彼が単に問題解決や功利主義のためにだけでなく、ある種のエンタテインメントを目的としたものづくりを積極的に行っていた、今日でいうところのメイカー的側面があったことである（この中でも、極め付けは、嘘発見器の応用の要領で作られた、カップルにそれぞれハンドルを握らせ、その状態でキスをするとその情熱の度合いを電球や大音量の発振音で賑やかす "osculometer(キス測定装置)"なるものである）。

エッカート自身は直接的に音楽や音声合成のようなテクノロジーに取り組むことはなかったものの、彼がムーア・スクールに勤める中での仕事の一つとして、1940年のニューヨーク万博にベル研究所が出展した、人工発話装置Voder[^voder]のような、計算機以前の電気的音声合成の技術には強く影響を受けたことが語られている。また、高校生の時にパイプオルガンに触れたこと、そしてのちにハモンド社が開発した100本以上の真空管が用いた電子オルガンを知ったことは、後の膨大な数の真空管で構成される電子計算機の存在に現実味を持たせるのにも一役買っていた。真珠湾攻撃の年である1941年、ムーアスクールが海軍やマサチューセッツ工科大学（MIT）のレーダー研究を行う研究所と協力を始めるようになり、エッカートは音声や通信に関わる技術のバックグラウンドを買われて仕事を行うようになる。そこで担当したのが、のちにトランジスタを開発する1人であるウィリアム・ショックレーが先行して研究していた、レーダー信号からノイズを除去するためのパルス遅延装置だった。ショックレーは超音波パルスの媒体に不凍液であるエチレングリコールと水の混合液を用いており、しかも片方の端からパルスを出し、反対の端で一度反射させてから同じ側でもう1度受信する仕組みを採用しており、結果として帰ってきたパルスの波形がどうしても歪んでしまう欠点があった。エッカートはここに、以前映画用フィルムの音声トラック書き込み装置を高精度化させる試みの中で培った、超音波の媒体としての水銀の有用性という知識を持ち込んだのである。さらに、片方で反射するのではなく、反対側で一度マイクロフォンでパルスを受信してフィードバックさせればより長い遅延時間を作り出すこともできる。

[^voder]: エクスタインの文献ではVocoderとなっているが、エッカートが体験したとされるキーボードで発音できるデバイスはVoderである。Vocoderは今日一般的に人の声を模す音声合成を指して使う語だが、ベル研究所のVocoderとはここではVoderという合成機能と対にになっていた分析を行う機械の名前である。 https://120years.net/wordpress/the-voder-vocoderhomer-dudleyusa1940/

こうした流れで音響遅延線メモリーの仕組みは形作られた。エッカートはこの時すでにヴァネバー・ブッシュの機械式計算機である微分解析器（Differential Analyzer）を見て、真空管によるスイッチングを大量に用いれば同様の機能を、機械では不可能な圧倒的速度で実現できるのではないかという構想をあたためていた。しかし、そこで問題になっていたのは記憶装置をどうするかということである。電子計算機の速度を生かすのであれば、メモリーも真空管を組み合わせたフリップフロップ回路[^flipflop]で実現すればいいが、そこまですると必要な真空管の数が一気に膨大になり、真空管が熱で劣化することを考えるとメンテナンスの問題も出てくる。一方でメモリだけを信頼性の高い機械式のスイッチなどで代用してもせっかくの実行速度のボトルネックとなってしまう。そこにはまった最後のピースこそが、音波のパルス列を用いることで、適度に高速で、かつ適度に大容量な音響遅延線メモリーという仕組みだったのである。

[^flipflop]: スイッチング回路の出力をフィードバックすることで状態を保持することができる回路。1ビットのデータ保存に複数個の真空管が必要になる。

つまり、音響遅延線メモリーという、その一見してどういった経緯で思いつくだろうかのかという奇妙な仕組みの考案の背景にはそれなりの必然性があったのである。エッカートが音にまつわる技術を再編成することで初めて電子計算機の誕生にたどり着けたのだとすれば、その仕組みを用いたコンピューターのデバッグの中に聴覚の技法が現れるのは不思議なことではない。改めて言い切ってしまおう。コンピューターを作ることそれ自体が音楽に関わる行為のひとつだった。

このことから考えられるべき示唆はいくつもあるだろう。まず、今日使われているコンピューター技術は、クロード・シャノンの定理が音声の符号化に貢献したことに限らず、音や通信にまつわるテクノロジーの上に成り立っていることはより強調されて然るべきである。加えて、音響遅延線メモリーの生い立ちは、計算機技術の歴史的位置付けをCivil Engineering、つまり非軍事技術観点からさらに見直す材料にもなる。電子計算機が歴史的に軍事技術として現れてきたことはもはや否定のしようもないし、テクノロジーが常に非中立的にならざるを得ないことを工学者は頭に入れ続けなければならない。しかし同時に、計算機を戦争だけが生み出した怪物のように捉えてしまうことは、テクノロジーを責任を持って扱う今日のCivil Engieeringのあり方に何ら希望を与えてくれないという困難さも併せ持つ。ならば立てるべき問いとはこうなる。WW2無くして現代の電子計算機は生まれなかったのかもしれないが、同時にエッカートの、自らの楽しみや、身の回りの人の楽しみのために行ってきたティンカリング無くしても現代の電子計算機の姿はなかったのではないか？

そして本章の問題意識に戻れば、エッカートの作った音響遅延線メモリーはラボラトリーのように制度化された場所における個人のDIY的試みが与えた影響は、この時期のいわゆる "コンピューターを使った音楽生成"から、いわゆる "コンピュータ音楽"における最も古いシステムである、ベル研究所で1957年に作られたMUSICの誕生についての橋渡しについて考える材料にもなる。

1950年前後のコンピューターから鳴らされる音楽は、基本的に既存の音楽のメロディの制御だけ、つまり周波数と発音する/しないという2種類のパラメータの制御に過ぎなかった。パルスを出すか出さないかの制御しかできない故に、その音色は常にいわゆるビープ音のような音にしかならない。加えて、これらの実践はあくまでデバッグ目的のスピーカーや、偶然の発見からの、いわば音楽のためには作られていない技術の流用である。

田中はこの時代以降のコンピューターを用いた音楽文化が、この後開発されるMUSICシリーズのような**専門化**された正当なコンピューター音楽の流れと、この時代の技術の流用からなるプリミティブな音楽生成を、後のチップチューンのような、限られた計算リソースの中で行われる表現が生み出す独特さへと連なる**趣味化**の流れに二分化する歴史として描いた[@Tanaka2017,p19]。しかし、ベル研究所のVoderがなければ、音響遅延線の仕組みが計算機上を流れる信号を聴く行為をアフォードすることも、そこから生み出されるプリミティブで趣味的な音楽もなかったかもしれないことを思うとこの二分法も歴史的因果関係を捉え損ねやすくする恐れがある。

実際、ドーンブッシュは、パルス間隔制御による "既存の音楽をコンピューターで鳴らす"試みと "音楽をコンピューターを活用して作り出す"試みの間に、パルス制御を利用して独自の音楽を作り出す試みがあったことを指摘している[@Doornbusch2017,p303〜p304]。イギリスで、アラン・チューリングが国立物理研究所のために設計したPilot ACE（Automatic Computing Engine:ACEのためのプロトタイプ）は、BINACやUNIVAC、CSIRAC同様に音響遅延線メモリーを保持しており、他のマシン同様に診断目的のスピーカーが取り付けられていた。ACEを利用していたドナルド・デイヴィスは次のように振り返る。

> Pilot Aceとその後続機Ace Properはどちらも、独自の音楽を作曲することができて、コントロールデスクに取り付けられた小さなスピーカーから演奏することができた。私が〔コンピューターを主語にした〕作曲という言い方をしたのは、人が意図的に音程を選ぶような余地が一切なかったからだ。その音楽は無調ではあったが大変興味深く、上昇系のアルペジオに始まり、だんだんと、フーガが発展していくように複雑化し加速していった。そして複雑度が人間の認知を超え最終的に色のついた[^coloured]ノイズの中に溶けていった。[@Davis1994,p19 筆者訳]

[^coloured]: 周波数成分的に偏りのあるノイズを工学分野では一般的に可視光のスペクトラムの偏りが作る色に置き換えてピンク・ノイズ、レッド・ノイズ、ブラウニアン・ノイズなどと表現する。おそらく周波数成分が均一なホワイト・ノイズとは異なる音色だったことを指しているのだと推測されるが、適切な訳語が思い当たらなかったのでそのまま置き換えさせてもらった。

我々にとって興味深いのは、デイヴィスらが、診断機能の流用であり、かつ12音のピッチと発音という既存の音楽様式の再生産ではないにもかかわらず、明確に音楽を作っていることを自覚していたこと、しかもそれが一般的な音楽の作曲方法の発想と全く異なる故に、コンピューター**が**作曲を行っていたというレトリックを用いていることである。

さらに、デイヴィスらの試みは音響遅延線メモリーと宮崎のいうAlgorhythmicな聴取が明確にこの音楽の特異さに寄与していたことを示している。ACEが利用した音響遅延線メモリーはデータが常に音波として周回し続けているため、今日一般的に使われているメモリーや、チューリングがこの前に設計したManchester Mark Iが採用していた陰極線管メモリ（音響遅延線と対照的に、ブラウン管テレビの仕組みをメモリに転用したデバイス）と異なり、ランダムアクセス、つまり任意のタイミングで任意のデータの読み込みができない。それゆえ、プログラムをうまく最適化していくと、最終的に実行時間の中で支配的になるのは遅延線メモリーからのデータ転送の待ち時間になるという。デイヴィスらは、このデータ転送時間に関わるTranstimと呼ばれるトリガー部分の信号を観測することでプログラムの実行効率を診断できるのではないかと考えた。トリガー信号をそのまま計測メーターに入れると針が細かく振れすぎてしまうので高周波成分をフィルタリングするようにした。ここまで来れば、この平滑化された信号を直接スピーカーに送り聞いてみるのは自然なことだったという。繰り返すがこの頃には液晶ディスプレイもなく、基本的にプログラムの結果はパンチカードに出力されるのでデバッグは難しい。その中でスピーカーは安価でありながら診断に大いに役に立つデバイスだったため、デイヴィスらも、取り付けるかどうかを誰かに相談するまでもなかったのだという。

しばらくしてそこに、もう1人のエンジニアであるデイヴィッド・クレイデンが新たなデバッグ目的の機能として、ACEの実行する命令を部分的に上書きできるスイッチを取り付けた。ACEの音楽はこの機能を活用（デイヴィスによれば、誤用）することで生まれたという。

> 詳細は覚えてないがこんな感じだったと思う。長い遅延線メモリにはプログラムである命令列が32個格納されていて、はじめは多分空に設定されている。プログラムはループに入り、それがスピーカーにある音程を鳴らさせる。命令を〔スイッチを用いて〕ディレイラインの中の適当なビットに加算する操作に固定することによって、このカウンターは最終的にタイミングを司るフィールドに桁あふれすることによって、定期的に別の種類のループに入る。

> 正しいスイッチの設定をすると、ループの大きさはしばらく一定で（1音）、その後突然〔ループの〕サイズが変化する。

> ループは常に32マイクロ秒の倍数になるので音程は常に31.25kHzを分周した周波数を持つことになる。その音楽は、平均律でもなければ和声的とも全く異なる、非常に奇妙な音階でできていたが非常に面白いものだった。[@Davis1994,p20 筆者訳]

デイヴィスらの生成した音楽は、当人たちに音楽であると認められており、かつその独特の音程変化というメディウム固有性は音響遅延線メモリーという装置の仕組みによってもたらされたものである。

ドーンブッシュは、これら1950年代初頭のコンピューター音楽が歴史的にはその後のMUSICシリーズのような、いわゆるコンピューター音楽の歴史に直接的に影響を与えなかったことから、その歴史の中から排除されアメリカ中心的な歴史になってしまっていることを批判した。確かに、MUSIC以降の取り組みには、歴史上初めてマックス・マシューズという、明確に音楽のバックグラウンドを持つ人間がその研究に加わるようになったという意味で黎明期の取り組みからは断絶がある。しかし当然、マシューズの持っていた "音楽"のバックグラウンドは、言い換えればその時代に一般的に受容されていた音楽の共通認識、であり、コンピューターが音楽の定義を押し広げるために活用されてきたことを肯定するのであれば、BINACやUNIVAC I、CSIRAC、Ferranti Mark Iのような既存のメロディを奏でるための実践はともかくとして、ACEで行われていた取り組みはその歴史の中に数え入れられて然るべきということになろう。


<!-- [^williams]。

[^williams]:ただ、1951年には同時に、アラン・チューリングが設計したManchester Mark Iやそれを商用化したFerranti Mark Iでも同様にスピーカーにパルスを送ることで音楽を生成する試みが行われており、これらの計算機は音響遅延線メモリーではなく、陰極線管メモリーという、いわばテレビの技術を転用したような仕組みのメモリーを用いていた。音響遅延線がデータを逐次的にしか読み出せないのに対し、陰極線管メモリーは任意の箇所のデータを随時取り出せる、ランダムアクセスが可能という特徴を持っていた。Mark Iでスピーカーから音を出す試みは、Hootという特別な命令を実行することでスピーカーに明示的にパルスが送られ、その間隔をダミーの何もしない命令などを挟むことによって調整することで特定の音程を出せることがチューリングの書いた操作マニュアルに示唆されていた。音響遅延線メモリーを用いるCSIRACの音生成はこれらイギリスにおける取り組みであったHoot命令を用いることで行われていたが -->

# マックス・マシューズとMUSICの位置づけ

ここからは、実際のところマシューズらによるMUSICシリーズはどのようなシステムだったのかについてより詳細に検討する。MUSICシリーズの発展の歴史的変遷の詳細は[@Lazzarini2013]が詳しいので具体的事実関係に関してはそちらも参照してほしい。

MUSICがそれ以前のシステムと異なっていたのは、パルス符合変調（PCM）と呼ばれる、音声波形を一定時間に分割（標本化）、各時間の音圧を離散的な数値として表す（量子化）、今日のコンピューター上における音声表現の基礎的な方法に基づいた計算を行ったことだ。パルス符合変調の元となる標本化定理はナイキストによって1928年に示され[@Nyquist1928]、パルス符合変調はリーブスにより1938年に開発されている。

この考え方を簡単に表したのが、[@Hartley1928]より抜粋した[@fig:hartley_pcm]である。時間を横軸、音圧を縦軸にとった音声波形のグラフをグリッド状に区切り、連続した数値を離散化された数値のリストに変換する。区切るグリッドが少ないほど、実際の波形との誤差が量子化歪みとして現れる一方、グリッドを細かくするほどに必要なデータの量は増えていく。また、標本化定理より、例えば1000Hzの周波数成分までを持つ波形を標本化するとき、その2倍である2000Hz以上、つまり横軸のグリッドを2000分の1秒より細かく設定する必要があることが知られている。例えばサンプリング周波数が1800Hzだった場合、表現できるのは900Hzまでとなり、1000Hzの正弦波をこのサンプリング周波数で標本化すると$1800-1000=800Hz$の信号が折り返し歪み(エイリアス信号)として現れてしまう。

人間の知覚できる周波数の上限が20000Hz程度となっているので、その2倍である40000Hz以上の標本化周波数で、かつ量子化歪みが十分に少なくなるように量子化ビット数を決めておけば、人間が近くできうる範囲ではおおよそどのような波形でも数値として表現できるということになる[^compactdisk]。

実際には連続した波形を離散化するだけであればこのような考え方の考慮で十分なのだが、標本化/量子化した波形同士を演算する場合にはもう少し細かい事情を考慮する必要があるため問題点も指摘されてきており[@Puckette2015]、音楽プログラミング言語設計の根幹にも関わってくるのだが、それはのちに議論する。

[^compactdisk]: 例えばコンパクトディスクの規格などはこういった考慮から標本化周波数44000Hz、量子化ビット数16bitと定められている。なお、サンプリング周波数や量子化ビット数を意図的に荒くすることで得られる可聴範囲の歪みを利用した表現が今日ビット・クラッシャーと呼ばれるエフェクトである。

<!--図 清書したほうがいいかも -->

![ハートレーの論文[@Hartley1928]より、PCMの概念を表した図。](img/hartley_pcm.png){width=70% #fig:hartley_pcm}

MUSICはIからVまでの5バージョンが代表的なものとして知られている。マシューズが直接的に開発に関わったのは基本的にこれらI〜Vまでだが、1960年代におけるコンピューターアーキテクチャの多様化やプログラミング言語の発展に伴い、MUSIC IV以降には様々なMUSICの名を冠する派生システムが作られた。

例えば、MUSIC IVの実装言語は当時主要な言語であったFORTRANで実装されていたが、当時新しく作られたばかりのC言語に実装されたMUSIC 4C(M4C)やCmix、プリンストン大学で同等の機能を持つものとして実装されたMUSIC IVB、MUSIC IVBF、さらにIVBやIVBFをIBM360コンピューター上で動くように移植したMUSIC 360、360をもとにさらにPDP-11へ移植したMUSIC 11、MUSIC VをもとにC言語で実装されたCmusic、さらに現在まで開発が続くCSound、と言った派生系がある。これらを総合してMUSIC Nシリーズと呼ぶこともあるのだが、それにマシューズが関わっていない派生系も含めるか否かには明確な合意がない。本稿ではI〜Vの総称として用い、派生プログラムを含める意味で用いるときは常にそう注釈することにする。

1957年に作られたMUSIC IはIBM 704というコンピューター上で動作する、対称系の三角波の波形に対してエンベロープを掛ける程度の波形の合成ができるシステムだった。この頃のシステムはリアルタイムで波形を電気信号として生成しスピーカーを鳴らせるようなものではない。まずMUSICのプログラムをコンピュータが利用できるIBM本社まで持ち込み、計算結果を磁気テープにバイナリデータとして書き出し、磁気テープをベル研究所に持ち帰り、そこにあった真空管製の12bitデジタル-アナログコンバーターに通してはじめて音が出せるようなシステムになっていた[@Roads1980;@Roads2001]。なお、IBM 704のメインメモリは磁気コアメモリであり、この時期以後、音響遅延線メモリーを採用するコンピューターはほぼ作られていない。

MUSIC Iの時点でのサンプリング周波数について記述されている文献はいないが、後年のIBM 7090上で動作していたMUSIC IV(1963年)では、同様に12bitでの量子化で、実際の計算速度が1秒間に5000サンプル程度が性能の限界だったのに対し、実際に計算結果を再生するときにはそれを1秒に30000サンプルに早回しすることができたと記述が残っている[@Mathews1963]。

マシューズはMUSICを制作するにあたって、シャノンらが示した標本化と量子化によって、考えうるあらゆる種類の音が計算によって生み出せる、それもこれまで既存の楽器では奏でられなかったような新しい種類の音が作れることに強く関心を持っていたことを語っている。

> 本質的に、サンプリング理論はサンプルから音を作るにあたって本当に限界が存在しないことを示している。人間が聴くことのできるあらゆる種類の音を、正しい数値、正確性、サンプルの組み合わせによって作ることが可能であるため、コンピューターは普遍的な楽器なのだ。他の楽器は、例えばバイオリンなどはとくに、美しく愛らしいものではあるとはいえ、常にその音はバイオリンのような音しか出せないし、最低でもバイオリンじゃないような音を作ることがとても難しい。[@Park2009,p11 筆者訳]

後半の発言はほとんどトートロジーのようにも聞こえるが、実際コンピューターが理論的には無限の種類の音を出せるということはクセナキスをはじめとしたコンピューターを音楽に応用する可能性を肯定するものたちの典型的な謳い文句ではある。

また彼は同時に、自身がバイオリン演奏をしていたが決して演奏が巧くはなかったため、身体的卓越を必要としない仕組みを作りたかったこと、さらに作曲家が曲を作ってもオーケストラに演奏してもらえる機会がなければ発表できなかったり、何年も経ってからようやく初めて演奏されるという状況に対するひとつの解として、作曲したものをコンピューターに演奏させるという2つのモチベーションを挙げている。

> 私の興味は2つあった。
> ひとつは、私はバイオリン演奏をいつも好んできたが、それは人生のうちほとんどはアマチュアの弦楽四重奏の中でのことで、決して巧くはなかった。だからほとんどの楽器が要求してくる、そういった手先の器用さ〔manual dexterity〕をほとんど必要としないような、より良い音楽が作れるようになりたかった。
> またこれまでたくさんの作曲家がオーケストラのための曲を書いたが未だに一度も演奏されたことがなかったり、何年も経ってからやっと演奏されたという状況があるのを感じていた。だから（コンピューターが）作曲家が書いたものをほぼ即座に聴けるような手段を提供できるのではないかと思った。[@Park2009,p10 筆者訳]


これらを総合し簡単に言い換えると、マシューズが考えていたコンピューターを音楽に用いる理由とは次の3つのようになるだろう。

まず1つはこれまで存在しなかった新しい音楽の探求だ。これはサンプリング定理に基づくものでない、音価レベルでのアルゴリズミック・コンポジションにも共通して言えるモチベーションと言える。2つ目は身体拡張、あるいは自動演奏の追求である。これは今日のNIME研究のような、コンピューターを用いて音を生成するための身体との境界面：インターフェースを作ることによって音楽と身体の新しい関係性を発見するものだ。これは最終的に1つ目の新しい音楽表現につながることもあるし、逆に、表現自体はこれまでも存在していたが演奏に高度な技能が必要とされるものを誰でも演奏できるように、といった既存の音楽様式の再生産につながるモチベーションにも読み替えられる。

そして最後の作曲家のためのオーケストラに代わるものとしてのコンピューターとしての視点は、作曲におけるクイック・プロトタイピングのしやすさと表現するのがわかりやすいだろう。ただし、もう少し本論文の興味に引き付けて読んでみると、**コンピュータ以前はオーケストラという装置が（とくにクラシック音楽に由来する）作曲のインフラストラクチャとして機能していた**という視点を導入できる。つまり、有名な作曲家であればオーケストラに頻繁に演奏してもらえる機会に恵まれ、そうでなければ作曲家は楽譜上に音を配置し、ピアノなど少ない数の楽器である程度のシミュレーションをしながら作曲することしかできないという状態だったと言える。特に実験的な作品、すなわち既に共通して使われている楽譜上の記述では足りない表現を指示しようとしたり、大人数が演奏してみてはじめて曲の修正ができるような複雑な作品を作ろうとしたときにはオーケストラに実際に演奏してもらえるか否かは思い通りに作品が作れるかどうかに大きく関わっていたということができるだろう。

こうしたマシューズの発言を読むと、確かにMUSICの取り組みは初めて意図的に音楽を生成するためのシステムを構築する試みではあったものの、例えばエジソンのフォノグラフがはじめから音楽の記録再生だけではなく口述筆記やアーカイブのような様々な用途を想定していたように[@Fukuda2015]、必ずしもコンピューターを用いた新しい種類の音楽生成だけに着目していたわけではないことは強調しておく必要がある。

マシューズは今日ではコンピューター音楽の父と位置付けられているものの、当時にしてみれば、CSIRACでの音楽生成を行なったひとりのジェフリー・ヒルと似たように、アマチュアとして音楽を嗜む科学者であったことを自認している。ドーンブッシュも強調しているように、マシューズの功績を正確に捉えるのであれば、それは**初めて、職業としての作曲家へ研究に積極的に関わってもらうよう働きかけたこと**である。

> 最初にこれらの音楽のためのプログラムを作ったとき、当初のユーザーは作曲家ではなく、心理学者のガットマンとジョン・ピアース、そしてわたしという、基本的に科学者だった。私たちは、音楽家がこの言語を習得し、これを使って自分自身を表現できるかどうかを知りたかった。そこで私たちは、この実験をやってみたいという冒険心あふれる音楽家や作曲家を探した。[@Roads1980,筆者訳]





# ボーンのIRCAM 4Xプログラミングの分析

リアルタイムで処理できない程度に重かったこと
楽器のモデルを計算できる理論が存在しなかったこと→Rissetに始まる物理モデリング合成研究（Analysis-Synthesisアプローチ）

80年代にしては計算のシンプルさに対して複雑な音色が出せるFM合成とかはあったが、コンピューター音楽言語は大学や研究所に限られていたし、リアルタイム性にもまだ欠けていた extreme mediation, both temporal and conceptual

モデルがしっかりしていないと音を改善することができないけど、トライ&エラーに時間がかかるのでモデルの妥当性を確かめるのにも時間がかかってしまうパラドックス

学生がCmusicで適当に音量をデカくしたせいでfoldover歪みが発生していたが、それが案外良かった→しかしシステム側で発生した歪みだったので再現できない 本当はなんでもできるはずのコンピューターが何故！

技術のトリクルダウン　研究所でパイオニア的テクノロジーが発達し、それがコマーシャルに低価格化していくという考え方をIRCAMは持っていた

ヤマハの人がCXのデモにIRCAMにきた話

4xは当時最強のスペックだった→これ何がそうさせたんだろう？オシレーターとかはソフトウェアで仮想化できたんだろうか？西野の文献読む必要あり→できた。Variable Digital Signal Processorの話

OSから作ってた　ハードは凄かったがソフトとペリフェラルが弱い

Chant 歌声合成、Formes PatchworkとOpenMusicの手前

Chant／Formesのグループからは、音楽概念の高度な発達というコンピューターのポテンシャルを無視していると思われてた

Chant/FormesはLISP製、VAX/UNIXシステムで動いていたノンリアルタイムシステム
    users could create their own "personalized environment"
    object oriented

The use and the development of software involve the writing of coded instructions within a software language or the creation of a completely new language, within the context of a hierarchy of such languages. At each of the hierarchy a traslation occurs between any two adjacent language or levels of code. Instructions from the language at a higher level mus be translated into aform whereby they can be "read" and executed by the lower-level code or language without any (or with minimal) loss of "meaning"

The hierarcy of codes that normally operates in computer software include, at the lowest level, machine code, the instructions that drive athe hardware, writtedn in binary form; at the next level up, assembler code, made of mnemonic abbreviations of machine code; above this, the general operating system that provides a basic framework and set of servicies; and above this, any of the major lkanguages such as FORTRAN, Pascal, C, or LISP.

〜〜

Computer music software such as that used and produced by IRCAM adds yet a further level of mediation, hierarchy, and translation, sincce the music languages are themselvels based upon, or written in, established general languages.

Thus, Music V is written in FORTRAN, Cmusic in C, IRCAM's Chant in FORTRAN, and Formes in LISP.


Chantを使うにはFORTRANの知識も必要だったし、Formesを使うにはLISPの知識も必要だったので、それを勉強するためにまずLISPについて勉強しなくてはならない →**つまりこの時点ではまだ、後にMcCartneyがSuperColliderの設計指針として挙げる、プログラミングというコンピューターハードウェアを使うための専門的知識が必要な事項をencapsulateし、音楽のNotationに集中させるという意味での音楽”言語”の概念は達成されていなかったということが言えるだろう。**



# 90年代


音楽プログラミング言語の歴史において90年代は、パーソナルコンピューターでも高性能化と低価格化を背景として、4Xのような研究所でしか使えない高級なハードウェアでなくとも、DSP専用の計算ハードウェアを用いないCPU処理でのリアルタイム音声合成が可能になった時代である。

ここでの高性能化と低価格化に関しては、具体的にどの製品の登場によってブレイクスルーが発生したという具体的な事例があるわけではないが、例えば久保田は「PowerPC G3やPentium IIといったCPUによって、CDクオリティのサウンド処理に必要な、ある種のスレッショルドを超えたのだろう」と説明している[Kubota2017,88p]。


# 2000年代

## より詳細な時間制御（ChucK、LC、Gwion）

## 低レイヤの拡張（Faust、Extempore、Kronos、Vult、Soul）

## 高レイヤの拡張（TidalCycles、Sonic Pi、IXI、Gibber、Foxdot、Takt、Alda）



# 小括

音楽プログラミング言語は、元々は50~60年代のMUSIC Nシリーズのような音楽をコンピューターで使うためのシステムとして生まれたものであるが、それはリアルタイムで音を生成できるわけではなく、磁気テープに書き出した結果を改めて再生するようなシステムだった。そしてこれは必ずしも今日の音楽プログラミング言語に限らず、DAWのような音楽制作ソフトウェア全般の祖先となるようなものだったと言えるだろう。

やがて70~80年代には、音楽プログラミングの歴史は2つに分岐する。一つはの時期普及し始めたパーソナルコンピューターにおける、音声合成ICチップを用いたアマチュアを中心とするチップチューン、もう一つはIRCAMに代表される研究所レベルにおけるプロフェッショナルな現代音楽の文脈における研究、こチップチューンに使われた音声合成チップは非常に限られた数のオシレーターに対してCPUがその周波数や発音タイミングの命令を行う構成をとることで、計算コストのかかる音色の制御をハードウェアに任せて（既に成立している音楽体系である）五線譜的な表現に注力できるようにした。
一方でIRCAMでの取り組みでは、4A,4B,4Cと言ったハードウェアではチップチューン同様にハードウェア的にオシレーターの上限が決まっているような構成から、4Xでハードウェア的にはオシレーターを持たず抽象的な計算ユニットだけでリアルタイム音声合成ができるようになったという進展が見られた。
チップチューンとIRCAMでの取り組みを対比すると、この時代の取り組みはいずれもリアルタイム性を重視する代わりに、表現の自由度かハードウェア的コストのどちらかを犠牲にしていたと見ることができる。チップチューンではオシレーターの数を超える発音はできないし、4Xは自由度が高い代わりにOSから独自で構成された非常に複雑な構成をしていた。このようにどちらかを犠牲にしてでもリアルタイム性を重視したのは、音楽をコンピューターを用いて生成するにあたって、人間は何かしらの記号、シンボルを解すことでしかコンピューターに対して命令を与えることができないという理由があった。つまり、音楽を生成するモデルを人間が考えコンピューターに実行してもらうプロセスが必要になるが、このモデルが妥当かどうかを検証するにはまたコンピューターに実際に実行してもらうプロセスが必要となる。（特にIRCAMのような新しい表現を追求する場においては）例え金銭的にコストがかかったとしてもリアルタイムにトライアンドエラーができる場所があることが重要だったと言えるだろう。

そして、90年代には、パーソナルコンピューターでも高性能化と低価格化を背景として、DSP専用の計算ハードウェアを用いないCPU処理でのリアルタイム音声合成が可能になった。またこの時期にパーソナルコンピューターに用いられるオペレーティングシステムもWindows、Macintoshなどに落ち着き、一つのCPUの中で複数のアプリケーションを実行するマルチタスキングのシステムもインフラストラクチャとして安定してきて、ユーザーアプリケーションとして音声合成を扱う場合にはプリエンプティブスケジューリングの中で可能な範囲でのリアルタイム性の追求という形に落ち着いたと考えられる。
またこの時期にSuperColliderが汎用プログラミング言語において進んできた構造化プログラミングの手法を音楽プログラミングのために取り入れた。ここで初めて、音楽プログラミング言語はプログラミングという専門的なタスクから音楽、音声合成の抽象化という行為を引き剥がす機能を持つようになったと言えるだろう。


2000年代以後の音楽プログラミング言語は大きく分けてChucKやLCに代表されるプリエンプティブスケジューリング環境下での正確なイベント制御、Faustに代表されるUGenレベルの低次の自己拡張性の追求、SuperColliderクライアントに代表される高次の自己拡張性の追求おいう3つの方向性があったとまとめられる。