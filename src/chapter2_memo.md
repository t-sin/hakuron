日常的行為の中に存在する、体系化されることのない様々な技法（method）に着目するエスノメソドロジーの分野の代表的研究者であるエリック・リビングストンは数学の証明プロセスや科学実験を自ら行うことによってその過程に存在する暗黙的なテクニックの存在を明らかにする研究を行っている[@Livingston1986]。エスノメソドロジーは通常その分析方法が会話分析という手法と結び付けられているが、西阪はリビングストンのような自ら実践することで明らかにするという手法も存在することを強調している[@UCILAB2020]。

---

ここでフレイリングの提言の以下の部分を改めて読み直すと、彼がリサーチ行為の枠組みをinto、through、forという3つのリサーチとして示そうとしていた理由は実のところ、研究プログラムのメタ研究といったモチベーションよりも、研究の形骸化や自己目的化への危惧であるように読める。

> 確かに、研究〔ここではResearchの訳〕は学術的な問題であると同時に、同じくらい政治的、もしくは財政的な問題になってしまった。そして、少し脱線するが、アカデミアで生計を立てている人たちが自分で「アカデミック」という用語を軽蔑の意味で使っているのを見ると可笑しくなってしまう。研究は概念的や実践的なものまでもと同様に肩書きの問題になってしまった。そして正直なところ私はこれを心配している。こうした感情的伝統の中で研究を行える場所も残るかもしれないが、肩書きや階級、逆スノッブについて白熱した議論を交わすのではなく冷静な研究を行う必要がある。[@Frayling1993,筆者訳]

フレイリングはRが大文字のResearchという単語における辞書的な定義がイノベーションのための基礎研究という意味が小文字のrのresearchよりも強調されるようになったという変化にまず着目していた[^rosenberg]。学術研究を行う機関である大学でさえも、その研究を行うために必要な資金がある特定の「期待」された領域に向けて投入される様な状況がある限り、リッテルがデザインサイエンスにおける意地悪な問題で提起したように、またレンテやブラウンが期待の社会学で議論したように、研究開発が合理性という皮に隠れた無自覚の自己反映的なループに巻き込まれざるを得ない。フレイリングの議論の焦点もまた、3つの区分けそのものよりも、研究という行為への自己反省の足らなさの方だったのだ。

[^rosenberg]:Researchという語がResearch & Development（R&D）という用語として、この大文字の意味合いを含み始めた原点には、何らかの知の発見（≒効率化）によって物の生産コストが低下するという、経済学の中にテクノロジー発展が重要な要素として考慮されるようになった歴史がある。ローゼンバーグはその原点に、今日のコンピューターにおける、データとプログラムを等価に扱う考え方を先取りしていた、機械式計算機である階差機関/解析機関を開発したチャールズ・バベッジの『機械類と製造業の経済について』という論考にあると評している。バベッジのこの論考はカール・マルクスの資本論における労働と生産の関係性に影響を大きく与えたことが言及されている[@Rosenberg2001]。[@Arato1996,p133]も参照。

こうした意図から離れてリサーチの3つの分類だけが広まってしまい、RtDをひとつのメソドロジー程度の認識で用いるという、結果的に皮肉にもフレイリングが危惧した研究プログラム自体の形骸化とも言える状況が発生している。とはいえもちろん、ツィマーマンをはじめとして研究としてのデザインについて考察し続けている者からは、フレイリングの3分類が曖昧であり、デザイン研究者が取るべき実践のガイドラインとしての理論たりえないという理由をもとに批判的な形で継承がなされてきてもいる[@Jonas2004][^fraylingcrit]。これはSSK自体が行き過ぎた相対主義として後に批判を喰らったことの相似形とも言えよう。そうした批判的視点を持ちつつも、形骸化する事なく実践としての知の創出をする学問としてのデザインのフレームワーク作りは継続して試みられている。代表的なものがコスキネンやツィマーマンらによる『構成的デザインリサーチ（Constructive Design Research）』である。

[^fraylingcrit]: 筆者としては、そもそもフレイリングは3つの分類の用語を提示することそのものは硬化した研究プログラムの区分けを崩す例示のひとつとしてあげたに過ぎないと見る方が妥当で、フレイリングを責める事はお門違いのようにも感じる。とはいえ例えばフレイリングの『新しいバウハウス：21世紀の芸術教育』[@Frayling2007]という論文を始めとして、彼の議論には自らの手でものを製作したり、制作を通じて教える事に重きが置かれ、言葉を用いたコミュニケーションや理論作りの重要性が相対的に軽んじられ過ぎているように読めるのも確かではある。

[^ofandinto]: フレイリングの論文ではintoだったが、ofやaboutといった語が用いられることもある。

コスキネンらは、広がりつつあるデザインの対象とその手法を活動の場という視点でLab、Field、Showroomという3つに分類した[@Koskinen2015]。LabにおけるRtDは本論文でも中心的に議論の対象となっている、HCI研究、生態学、認知心理学などに由来し、プロトタイピングと評価実験を繰り返しその価値を提示する。FieldでのRtDは文化人類学などに由来する、ユーザーがいる環境を社会科学的な質的な調査法を中心に理解することを目指した研究である。そしてShowroomでのRtDはいわゆるクリティカル・デザイン[@Marpus2019]やスペキュラティブ・デザイン[@Dan2013]に代表される、ありえるかもしれない並行世界に存在しうるような装置や、時にユーモアを交えた不合理さを持つ、必ずしも現代において役立つものではないプロダクトのような人工物を、物語、展示や映像といった表象を組み合わせることで、社会構造に対する批評的役割を担ったり、議論を促すことを目的としたアプローチだ。

---

# NIMEの研究法に関する研究

ダールは音楽のための新しいインターフェース：NIME研究は学術研究としてどのように位置づけられるかを、特にデザイン学を中心に検討した論考の中で、いくつかの指摘と問題提起を行う[@Dahl2015]。まず、新しい楽器を作ることは意地悪な問題の一類型であるということだ。もちろん、楽器の研究という領域が、例えば環境問題のように複数の関係者が相反した利害関係にあるような大規模な問題と全く同じような類の意地悪さではないのは確かだが、最低でも明確な議論のガイドラインが存在しないということや、一般化できる事例が少な過ぎて理論化しづらいという意味では、明確に科学的事実を明らかにする研究パラダイムとは異なるということは直感的に受け入れられる。

加えて、ダールはストークスの研究という行為を基礎的理解(≒知の創出)のための研究であるか？という軸、実用されることを考慮されているか？という軸の2つで分割した「パスツールの象限」[@Stokes1997]を参照しNIME研究の位置付けを試みる。[@fig:stoke]において、知の創出かつ実用を目的としない左上の領域は例えば量子力学におけるボーアのような基礎研究の領域であり、一方実用されることだけを目的とし知の創出そのものは目的としない右下のエジソンの発明群、そして、右上に実用的な理由に牽引されつつも新しい知の基礎となったパスツールの乳酸発酵の研究などがある。

![ストークスによる研究の象限。[@Dahl2015]より。](img/stokequad.png){#fig:stoke width=50%}

この象限にNIME研究を位置付けるとすればどこになるだろうか？右下の実用だけを意識したものだとすれば、それは企業が生産する楽器群とどのように区別ができるのだろうかという問題がある。実際にダールは、フォールマンによるデザインリサーチの3つ組という分類[@Fallman2003]を参照し、パスツールの象限における右下の領域と対応する、「産業とのインターフェース」であるDesign ProcessというカテゴリはNIME研究において企業が一般的に楽器を作るプロセスと対応するという。

一方右上の実用を通じて知の創出を得る研究なのだとすれば、1.その知とは一体なんなのか、2.その知をいかに証明できるか、3.その知は一体いつ発生するのかといった問題群が浮かび上がってくる。仮にNIMEで創出できる知もゲイバーがいうようなAnnotated Portfolioのように、理論として一般化することができず、個々の差異との比較によって議論されるものだとしたらいったん1.(What)と2.(How)は横に置いておいてもよいだろう。しかし3.(When)は筆者のようなソフトウェアを研究するものにとっては考えるべき重要な問題になる。つまり、NIME研究をするという時に、楽器を作る過程で得られる知見と、その楽器を演奏するとき、あるいは評価実験などでユーザーに使ってもらうことで得られる知見とが混在していることをダールは指摘しているのだ。


# プログラミング言語における量と質の相互作用

こうしたデザインの個別性に伴う理論の一般化や評価の困難さは本論文で検討するもう1つの領域、プログラミング言語一般に関する評価について検討することでもう少し明確な議論をすることができる。

音楽目的に限らない、システムレベルのソフトウェア開発などに使われるプログラミング言語の設計を考えれば、その言語によって作られるアプリケーションの実行速度は明らかに重要な評価指標であり、また（使われるマシンの種類などの環境を揃えれば）定量的に計測することが可能である。では、あるプログラミング言語のコンパイラの機能Xの実装を工夫することにより、実行速度を20%ほど改善できたとしたら、それはなんらかの事実を解明したことになるのだろうか、それともある手法やソフトウェアといった人工物を生成したことになるのだろうか？といったように、プログラミング言語という分野自体、1960年頃から工学的なアプローチで長く研究がされている中、学術的研究としてどう正当化するか、特にどのように評価すればよいかという話題は2009年のACM SIGPLAN（Special Interest Group of Programming Language:プログラミング言語の国際会議を主催する団体で最も大きなもの）主催の*PLATEAU: Evaluation and Usability of Programming Languages and Tools*が開催されるまであまり触れられてこなかった。

2010年のPLATEAUでマークストラムは新しい言語の実装や設計などのアイデアを論文として提示する際の正当化の方法として、**主張と根拠の整合性(Claim-Evidence-Correspondense)**という見方を提示する[@Markstrum2010]。マークストラムによれば、プログラミング言語の論文で提示される主張は大きく分けて3種類あるという。1つ目はこれまで存在しなかった新機能を作ったというもの、2つ目はすでにある既存の機能の効率性を上げるような内容、3つ目は望ましい言語の特徴(property)、つまりこの言語は直感的である、読みやすい、効率的であるといったような内容だ。しかしこれまでのプログラミング言語の提案の論文には、1と2、つまり新機能と機能の増強に関しては論文が査読されたものであるならば十分認められるものであるが、望ましい言語の特徴に関してはその主張と、それを支える根拠は両方とも提示されているが論理的な結びつきは不十分なものが多数あるとしたのだ。

こうした研究はのちにより近年のコブレンツらによるプログラミング言語における評価語彙の整理（第5章で詳しく紹介する）などにつながる。主張と根拠の整合性を証明する方法論は統計的な評価実験のような実証主義的方法だけでは不十分とマークストラム自身も警鐘しており、例えば近年のミュラーとリングラーの修辞的フレームワークのように、論文中で主張された表現の変遷などを人文学的手法で辿ることで明らかにするような研究が進んでいる[@Muller2020]。

つまりプログラミング言語という領域における知の創出の体系は、新しい文法の提示のような生成的な人工物、ベンチマークテストのような量的に計測、比較ができる結果、ユーザーインタビュー調査のような質的調査の結果、主観評価実験のような統計的手法による量的なユーザーフィードバックといった様々な種類の論拠を組み合わせて、効率が良い、読みやすい、表現力が高いと言った主張をなんらかの修辞的な方法によって結びつけることによって行われる。

このようにプログラミング言語という領域が、論拠に関してはある程度反証可能性が存在する、しかし最終的に作られた言語そのものの定量、定質的評価が難しいという形而上学とも形而下学(≒科学)とも言える独特の体系を持つ理由としては、量的な特徴の変化がその言語の質的な変化に大きく影響を与えることが時にあるという事情も挙げられる。

例えば音楽プログラミング言語に関していえば、アルゴリズムを用いた作曲を同じ手法でも、人間が手動では行えないような計算速度を借りることによって異なる種類の音楽にすることができるし、第4章で見るように、音楽プログラミング初期の一度信号処理の結果をテープに書き込んでから再生する方式と、80年代以降のリアルタイムに信号処理ができる環境とでは作られる音楽の質も大きく異なる。一見単純な実行速度という量的な指標はある段階から質的な問題へと転化するのだ。加えて、より事情を複雑にする問題としては、例えばライブコーディングのような、プログラムの動的変更のしやすさという質的特徴を強化した言語を作ろうと思うと、プログラムの実行性能は静的な言語と比べると低くならざるを得ないという、いわば**量的特徴と質的特徴間のトレードオフ**が発生するという問題がある。

こうした事情を鑑みれば、ベンチマークや主観評価実験、インタビュー調査といったあらゆる手法による証拠そのものにはそれが存在するだけである程度一次資料として有用性もあるし、ただ実行速度が上がるような改善をもたらすだけでもその言語の存在意義に関わる変化となる可能性はある。そこから先の、その言語を使うことによって何が起きるか、何が可能になるかといった主張の正当化の手段として、ゲイバーの言うような人工物のAnnotationを補強するものとしてこれらの論拠を使い、個別の言語の具体性をより高め、理論化は個々の差異を見ることによって可能になる反証できない生成的学問として取り扱う。このような実証的研究法と形而上学的研究法の棲み分けをすることでRtDとしてのプログラミング言語研究という領域を定義することができよう。音楽プログラミング言語は、汎用プログラミング言語よりも量的に測れない指標が必然的に多くなってくる研究対象であるため、半ば惰性的に続けられている工学的アプローチ以外の研究方法を定義しておくことは有益なはずだ。



# 音楽プログラミング言語のデザインという研究領域

![ゲイバー、マークストラム、ミュラーらの議論を統合した、RtDとしてのプログラミング言語の研究の概念を表した図。](img/researchprogram.png){#fig:researchprogram width=80%}

さて、ここまでの議論を[@fig:researchprogram]にまとめた。プログラミング言語のための新しい文法やアルゴリズムを提案するという作業は時に既存の研究内容から飛躍することもある生成的な作業である。そしてそれによって成される主張は時に主観的な用語を用いる不明瞭なものでもある。そうした主張を支えるものとして、ある程度反証可能な方法で論拠を提示する。これには主にベンチマークや主観評価実験に基づく定量的データによる論拠、文献調査、インタビューとその分析、エスノグラフィと言った様々な手法によって得られる質的なデータによる論拠、時にその両方が存在することになる。その論拠と主張は適切な修辞によって結び付けられ、そのclaim-evidenceの結び目を持って生成した言語やアルゴリズムという人工物に注釈：Annotationを付ける。これらの要素はある程度独立性があるため、例えば論拠となっているベンチマークの測定に間違いがあったり、質的データ分析の方法論が適切でなかったとしてそれらのデータを取り下げたとしても、作られた音楽プログラミング言語という人工物そのものを丸ごと棄却しなければならないわけではない。新たにデータを分析し直して適切に主張と結びつけ直すことができればその言語の学術的価値を改めて提示することができるだろう。

注意しなくてはならないのは、論拠はあくまでも主張と結びつくことによってはじめて意味があるものとなるので、単に言語Aにおけるベンチマークと言語Bのベンチマークを比較してAのほうが性能が高かったからといって、言語Bは淘汰されるべきということにはならない。これは量的特徴と質的特徴のトレードオフの問題を考えれば当然とも言えることだろう。加えて、論拠には様々な研究パラダイムが混在しており、質的研究の中でも実証主義的—解釈主義的なスペクトラムが存在しているため、自分がどの立ち位置で論拠を見出そうしているのかを依然自覚する必要がある[@Otani2019,p30~p32]。それゆえ、例えば実証主義的な方法に依った論拠として、ベンチマークを取るのであればその手法や環境は再現可能性が担保されているべきだし、評価実験を行なったのであれば、その実験自体も追試が可能なものであり、実験によって得られたデータの分析方法も検証可能なものでなければならない。音楽プログラミング言語という人工物を提示することによる主張それ自体が反証不可能だからといって、そのための注釈もすべて反証不可能であり再現性が担保される必要もないということには当然ならない。

加えて、音楽プログラミング言語という領域で考えれば、ダールのNIME研究におけるWhenの問題を考えれば、作られた音楽プログラミング言語を用いて作った音楽、プログラム、パフォーマンスといった二次人工物とでも言うべきものが、それ自体デザインの成果としての比較対象になる独立性を持ちながらも、その経験を通したインタビューやエスノグラフィといった、新たに一次人工物に注釈を付けるための材料にもなっているという、再帰的な構造を持っていることがわかる。


---

メディア考古学は注目する対象が多岐にわたるためその全容を一望することは難しいが、本研究はメディア考古学の中でも音、コンピューター、インフラストラクチャという3つの要素が関わる研究であるため、ここでそれぞれの領域において行われているメディア考古学的実践について触れておく。

まず音に関してだが、メディア考古学というアプローチをこの語を用いて整理する研究は映像のような視覚メディアが中心的ではあるものの、音に関わるメディア文化史に関しても、ジョナサン・スターン「聞こえくる過去」[@Sterne2016]に代表される、サウンド・スタディーズと呼ばれる研究領域はメディア考古学と近しい性質を持っている。サウンド・スタディーズ（音研究）は現在では視覚優位のメディア文化論に対するアンチテーゼ[^visualmedia]や、音楽学に収まりきらない、音が関わる文化全般に対するカルチュラル・スタディーズや文化人類学的アプローチによる研究といった側面を持つものの、スターンが前掲書で行った音響再生産文化（Sound Reproduction Technology）としてのフォノグラフやレコードに関わる文化史の再考を「わざと思索的に語った歴史」「歴史をある種の実験室として用いている。それは、音、技術、文化に関して新しい問いを提示することを学ぶため」(ibid,p44)と表現していることが大久保の「移行期にあるメディア理論を再構築するための『実験室』『驚異の部屋』としてメディア考古学的視点が機能しているのだ」[@Okubo2021,p295]という表現と重なることからわかるように、明らかなメディア考古学的アプローチと言える。

[^visualmedia]: もっとも、音文化の研究において視覚優位なメディア文化観に対する聴覚の重要性だけを強調することは、結果的に不要な二項対立や聴覚の神秘化を引き起こしかねないものとして「視聴覚連\UTF{79B1}」とスターンが批判しているもの[@Sterne2016,p27〜34]である。

そして2つ目、コンピューターに関しては、コンピューター自体が「ニューメディア」と表現されることがあるために考古学というアプローチと一見遠い分野に見えそうな一方、計算機に関わる技術自体が驚異的な速度で、様々な隣接領域を巻き込みながら成長している分野である以上、その過去を忘却する速度や歴史的記述の多面性という意味ではコンピューター史以上にメディア考古学的実践が似合う領域もない。それゆえ、この分野では歴史的記述の長さそのものは短くとも、積み上げられてしまった技術要素を研究者自ら解体することによって、透明な背景となっている歴史や文化観を析出させるような性質が強い。古いものではフリードリヒ・キットラーのインテルCPUに搭載されたプロテクトモード批判[@Kittler1998]のような論考が挙げられる。またこうした研究はソフトウェア研究（Software Studies）と呼ばれる分野においてウエンディ・フイ・キョン・チュンが1950年代の最初機コンピューターにおけるプログラミングからソフトウェアに内在するイデオロギーについて検討したように[@Chun2001]、またアレクサンダー・ギャロウェイがTCP/IPのような基礎的プロトコルの成立過程などから新しい形の管理/制御（Control）の権力構造を議論したように[@Galloway2017]、さらに、ソフトウェアやプログラムに完結しきらない、ハードウェアとの連関を考慮するプラットフォーム研究(Platform Studies)におけるイアン・ボゴストとニック・モントフォートのアタリ製ゲーム機が生み出す映像の研究[@Bogost2009]のように、コンピューターというメディア装置の研究においてメディア考古学的省察は至る所で行われている。またこうしたソフトウェア・スタディーズやプラットフォーム・スタディーズに共通する態度としては、研究者自らがプログラミングを行うことで、高度に専門化されているコンピューティング技術やそれに関わる産業構造の批評を表層的な知識でなく実践的に行う、ブラックボックスを開く態度という特徴がある。このことはソースコードそのものを批評的に読み解くクリティカル・コード・スタディーズという研究を行うマーク・C・マリノによるキットラーがソフトウェア研究の過程で残したソースコードの検討[@Marino2001]や、キットラーの研究を行う梅田拓也による、キットラーのアナログシンセサイザー制作経験を彼のプロテクト・モード論考の背景として検討したもの[@Umeda2021]などに見ることができる。

本研究で特に着目するのは（第3章で詳しく検討するが）アラン・ケイとアデル・ゴールドバーグが提示したメタメディアとしてのコンピューターに対する歴史的位置づけと評価である。レフ・マノヴィッチは「ニューメディアの言語」[@Manovich2001]において映画との比較という、これもある意味での考古学的検証においてコンピューターのメディア固有性について検討したが、続く「Software Takes Command」ではコンピューターが新しいメディウム自体を自分自身で生み出せるメタメディアであることにその特徴をより強調し、その原点としてケイとゴールドバーグのDynabookという装置をおいた[@Manovich2013]。一方で、コロラド大学ボルダー校においてMedia Archaeology Labを主宰するロリ・エマーソンは具体詩（コンクリート・ポエトリー）とコンピューターを用いた視覚表現を行うアーティストの歴史的接点として、ケイとゴールドバーグのメタメディアとしてのコンピューターの思想を歴史的原点に位置付けるが、それはマノヴィッチと対照的に、現代のパーソナルコンピューティング環境がメタメディアの不完全な形で社会実装であり、アクセス不可能な装置となってしまった失敗の歴史として位置付けている[@Emerson2014]。本研究はエマーソンの立場に近い態度で、音楽ソフトウェアという異なる領域からやはりDynabookのようなメタメディアとしてのコンピューターの思想を検討することになるが、この立場におけるメディア考古学としてのコンピューター史は、映像や音響メディア史において歴史に記述されることの少なかったメディア装置を掘り返すこととは異なり、ケイとゴールドバーグのようにある種の正当な歴史においても原点として位置付けられている活動を、むしろ失敗した試みとして書き換える試みである。これは「歴史-理論」の軸でいえば既存の歴史に置かれている要素をあえて読み替える「理論」側に寄った位置づけであり、より厄介で注意を要するものだが、本研究における音楽プログラミング言語の制作の一体どこがメディア考古学的なのかという質問への回答は、エマーソンと同じく**歴史を辿ることで現代のコンピューター環境を、万能メディア装置のなり損ないと位置付ける**ことにあると言えるだろう。

3つ目にインフラストラクチャという視点におけるメディア考古学的アプローチという立場だ。ここで筆者が言うインフラストラクチャとは主にISOやANSI、JISで定義されるような標準規格（Standard）やフォーマットのことを意味しており、補集合的な補足をすれば、装置でもなく、アルゴリズムでもなく、コンピュータープログラムでもないが社会や文化に大きな影響を与えているもの、という表現ができるだろう。このインフラストラクチャへの注目の大きな背景としては、科学技術社会論における研究対象が、社会に大きくインパクトを与えた革新的研究（イノベーション）ではなく、すでに社会に浸透してしまったテクノロジーに注目するという反動を起こしたことが挙げられる。ふだん不可視のインフラストラクチャに着目することで知の伝播や政治的権力関係のかたちを明らかにするというアプローチを、ボウカーは「インフラ的転倒(Infrastructural Inversion)」[^inversion][@Star2001,p34]と呼んだが、このインフラの分析においてその成立過程の記述のような通時的分析が多く含まれればそれはメディア考古学的アプローチと多く共通するところがある。これは既に挙げたギャロウェイのプロトコル概念の分析も含まれるし、スターンが「聞こえくる過去」の後に「MP3」で提示したフォーマット理論[@Sterne2012]の概念と大きく共通する。またボウカーはインフラ的転倒の代表例としてベッカーの「アート・ワールド」[@Becker2019]のような、芸術を、それを支える美術館や経済システム、特定の集団の方に着目することで、「アート作品を生み出す社会分業こそが、それに固有の作品を生産する」(ibid、p423訳者解説)という視点の転換を例に挙げているが、これは音楽という領域で検討すれば、スモールの「ミュージッキング」[@Small2009]のように、ある文化を構成する社会的集団を、たとえば音楽であれば作曲家や演奏家、聴衆だけではなくコンサートのスタッフや楽器製作者も含めることでよりその権力関係や文化のあるべき姿の議論の焦点を合わせやすくするアプローチとも捉えられる。実際スターンはスモールの議論を延長し、「音楽産業など存在しない」というエッセイで、音楽ソフトウェアの開発者が音楽文化を構成する一員として捉えられることが少ないことの不自然さを指摘している[@Sterne2017]。本研究においても、Computer Music Languageという、ある種独立して捉えられてきたソフトウェア群の歴史を、コンピューターを用いている音楽の中におけるソフトウェアというより広い枠組みのなかで編み直すことを目標としており、かつそうした音楽ソフトウェアやプログラミング言語は、それらを用いて作られた音楽だけを事後的に分析する上では不可視なインフラストラクチャと言える点で、これらの研究と問題意識を共有している。

[^inversion]:「インフラ的転倒」という訳は[@Fukushima2017]に倣っている。

---

# ブラックボックス

ブラックボックスとはそもそもサイバネティクスの中でAshbyが挙げたとされる概念である。

## ブラックボックスの4分類

<!-- 1. ????：起きている現象が・・・・何だ？ -->
1. 知識的：技術に対する知識の欠如
1. 身体機能的：起きている現象が人間の生理的/認知的限界を超えている
1. 言語的：何が起きているかはわかっているが、その現象を言語化できない
1. 社会的：契約や法律によって技術の内部を覗き見ることが不可能である

### 理解不可能性：知識的ブラックボックス

例えばAppleの最新のMacbookProに入っているCPUのことを考える。我々は普段CPUがどう動いているかについてわからないままにCPUを使うことができてしまっている。
わからない最もシンプルな理由は、CPUとは何かについて勉強してないので、それがどういうものかわからない、ということである。このカテゴリで重要なのは”学習すれば解決できる”という話である。
この例えを使っている例としてはPrikkaとHerzのサーキットベンディングをメディア考古学的実践として捉える論考がある。彼らは次のような図としてアマチュアが中身の動作原理を理解しないまま、回路をつなぎ変えることで別の目的に（例えば、幼児向けおもちゃをノイズ楽器へと）再利用するモデルを示している。彼らがNon-expertという用語を使っていることからも、ここでの意図は改造者に電子回路の詳しい知識がないことに重きが置かれていることがわかる。


### 知覚不可能性：身体機能的ブラックボックス

しかし、私はCPUの基本的な仕組みや構成方法を知っているが、CPUが実際に動いているところを眺めたところで、いまCPUのこの機能が使われているのだということを知覚することは不可能だ。仮に自分でトランジスタを一つ一つはんだ付けしていってCPUを自作したとしても、それが正しく動いている”らしい”と判断できるのは、結局のところ入力に対して期待した出力が出てくることで確かめている＝ブラックボックスとしての利用であることに変わりはない。
もっと単純な例で、１つのトランジスタが動く場合を考える。トランジスタがゲート-エミッタ間を流れる電流に伴ってコレクタ-エミッタ間を流れる電流を増幅させる作用があることを私たちは勉強で知っているし、不純物の混入で余った/足りなくなった電子が電圧によって移動して～というミクロスケールでの動作の概念についても理解している。しかし私たちは実際に電子が移動している様をこの目で確かめたわけではない。結局これも、入力に伴って出力が変化した、という入出力の対応をもって確かめているに過ぎない。

これは突き詰めれば人間の身体機能が限界にある、といって良いだろう。我々は電子が移動する様を直に見ることはできないし、3GHzで変化するHIGH/LOWの電圧変化をリアルタイムのスピードで知覚することはひとまずできない。

ここで、1.の学習も人間の身体機能の一部なのでまとめても良いのではないかという考えもあるだろう。しかし、電子が移動する様を眺めるには何か（例えばオーグメンテッドヒューマン的に視力を改造するSF的処置が未来に可能になったとして）、外在的な要因で身体機能を強化することでなら可能になるだろうが、人一人が自らの力だけでそこまでの身体的変化を起こすことはまず不可能だろう。まあ、学習のための本とかだって立派なテクノロジーであって外部的要因でしょ、ということもできるのだが、ひとまずCPUの例では直感的に別種の要因であると言えそうなのでこのままにしておこう。

### 翻訳不可能性：言語的ブラックボックス

これは少しCPUの例では説明の仕方を変えよう。言語化不可能なことによるブラックボックス性といった時に筆者が想定しているのは、主に機械学習のモデルが下す判断の根拠を説明するのが不可能である、といった問題である。
与えられた画像がイヌかそうでないかを判断するプログラムがあったとしよう。人間が画像認識のアルゴリズムを組み合わせてモデルを構築した場合、プログラムのソースコードにおける関数や変数の名前は人間が解釈可能なものとして残っている。そのため、何かイヌじゃない画像を入れたのに結果がイヌと判断されてこれはおかしい、となった時に、少なくともどの時点での計算での結果がおかしいかがわかり、構築したモデルが正確でない要因をある程度判断することができる。
これが、学習ベースの推論の場合は、モデル自体は大量の並列化された非線形関数とそれ同士の入出力重みづけが並んでいるだけで、学習に伴ってその重みづけのパラメーターが少しづつ変化していくだけだ。イヌかどうかを99%近くの確率で判断できたとしても、中身を開けたとしても”なぜ”イヌと判断できるのかの理由はわからない。まぁ実際にはCNNのような二次元の重み付けをつけるようなモデルでは、最終的に重みづけの分布も二次元になるのでなんとなく、この層ではこういう特徴を抽出しているっぽい、とかいう解釈らしいこともできるし、機械学習の判断として実用的に（例えば社会的信用スコアをAIで出す時に根拠が出せないのは結構困るので）モデルの説明をするような研究も進んでいる。 https://tjo.hatenablog.com/entry/2019/12/19/190000
もちろん、別に機械学習モデルでなくても、例えば我々の目や脳が何かを見た時にあれをイヌだとかイヌじゃないとか判断していたとして、脳の電気信号をいくら計測したところで”なぜ”イヌかがわかるわけじゃないので、機械学習の方が劣っているとかそういう話をしたいのではここではない。そもそも前掲の記事でも紹介されているように説明する必要性がないという議論もあるhttps://arxiv.org/abs/1811.10154　。
重要なのは1とも2とも違って、人間が頑張って学習すれば剥がれるブラックボックスでもないし、人間の身体機能をいかに強化したところで解決する問題でもないという意味で種類が違う、ということである。


### アクセス不可能性：社会的ブラックボックス

さて、1,2のCPUの例えに戻る。私はCPUの仕組みを勉強していて動作原理をある程度理解しているが、最新のMacbookProに搭載されているCPUの構造を知ることはできない。なぜなら最新のMBPに搭載されているM1チップというAppleが独自に設計したチップの仕様はオフィシャルには公開されていないので知ることができない、ということである。
こうしたことは大概のプロプライエタリなソフトウェアでも同じことがいえ、ブラックボックスの入出力の対応を確かめる＝リバースエンジニアリングによってある程度内部の仕組みを知ることができる可能性はある。しかし、場合によってはエンドユーザー利用規約でリバースエンジニアリングが禁止されており、中身を知ろうとしたことがバレれば訴えられる……こともあるかもしれない。
1~3のブラックボックスと明らかに異なるのは、誰かが意図的にこのブラックボックスを作っていることだ。それは経済的、政治的な都合などによって生み出され、社会的に存在する権力構造を利用して存在し続けている。
なので、コンピューターを自由な装置として利用するべきであるという方向性での、メガプラットフォーム批判に使われる材料になる。iPhoneという小さなコンピューターを、OSレベルでカスタマイズすることはできず、Appleの公証を受けたアプリケーションしかインストールすることができない、とかに対して。

## ブラックボックスの混用

さて、例えばこの4種類のブラックボックスが混ざって使われている例を少し見てみる。
例えば緒方の「コンヴィヴィアル・テクノロジー」（2021）の序章には以下のような文が並ぶ。


> ～わたしたちはもはや、自分一人でスマホを作ることはできないし、SNSのタイムラインがどのようなアルゴリズムで表示されているのかを理解することもできない。あるテクノロジーのメカニズムやアルゴリズムがブラックボックス化することは、人々をブラックボックスを作る側と使う側、管理する側とされる側に知らず知らずのうちに分断してしまう。ブラックボックス化された道具がますます便利になり、不自由を感じなくなればなるほど、ここをもっとこうしたいとか、もっと別のこんなものがあったらいいのにといった創造的な発想が生まれなくなってしまい、道具を自分なりに改良したり、新しい道具を自らつくりだそうという主体性がだんだんと奪われていくのである。(34p)

> ～行き過ぎた専門化は化学をブラックボックス化し、知識への盲信（または不信）を招く、そうしてわたしたちは、自ら世界と関わり、考え、判断し、意思決定する能力を徐々に失っていくのである。(55p)

ひとつ目の文章は1:学習と4:社会的の2つの要素で構成されているように思えるが、ふたつ目の文章は1に重きが置かれているように思える。自分一人でスマホを作ることができないのは、やろうと思えばできる事だが自分一人でやろうとすると、膨大な量の勉強が必要なことに加えて、作るのに必要な道具や設備の必要性など、コストパフォーマンス的な面からも現実的でなくなるからだ。だが、実際には作れなくはない。実際、肥大化し過ぎた科学の知識体系を自分の手で可能な限りゼロから作る事で明らかにする試みとしてはトマス・トウェイツの「トースター・プロジェクト」のような例がある。
だが一方で、SNSのタイムラインが表示されるアルゴリズムを我々が知ることができないのはその技術を理解するのに必要な知識量だけの問題ではなかろう。めちゃくちゃプログラミングをやっているFacebook社のエンジニアもTwitterのタイムラインが表示されるまでの仕組みを完全に知ることはできない。




インビジブルコンピューターとは、認知言語学てきな思想に基づくインターフェースを改善するための思想だったと言える。

そこで重視されてきた（と思われてきた）のは身体感覚と言語機能の接続であり、適切な概念モデルを元にシグニファイア（シニフィアン）を作ることが良いインターフェースデザインの方法だったと言える。

しかし一方でこれらの思想の中で軽視されてきたものとして、ノーマンはインフラストラクチャの重要性を指摘していたことを思い出すべきだ。

すなわちある集団に対してのインターフェースデザインをするためには、その集団で共有されている概念モデルを（例えばエスノグラフィなどによって）同定し、

