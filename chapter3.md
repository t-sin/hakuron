# 音楽プログラミング言語の歴史

ここまで、第2章では、コンピューターがメディア装置として扱われるようになるまでの歴史と、その中で培われてきた、コンピューターは不可視になってゆくべきだという思想についてを紹介した。そして第3章では音楽の流通インフラストラクチャの変遷の歴史を辿ってきたのだった。

本章では、この2つの歴史を踏まえた上で音楽プログラミング言語とはどのような特性（≒ナラデハ特徴(by松永)）を持っているのかについてを再整理する。

まずは、既存の音楽プログラミング言語に関するサーベイの内容について軽く触れておく。音楽プログラミング言語に関する文献は基本的に、（本論文もそうだが）個別のプログラミング言語実装についてが主内容の論文の前段として書かれているものがほとんどだが、サーベイに特化した文献としては[@Nishino2016]と[@Dannenberg2018]が存在する[^authorsinfo]。

[^authorsinfo]:なお、NishinoはLC[@Nishino2014]の、DannenbergはNyquist[@Dannenberg1997]他多数の言語の設計者でもある。このことからもやはり、音楽プログラミング言語の歴史を記述するには基本的にその設計や実装に関する知識や経験が必要になっていることが窺える。


NishinoとNakatsuのサーベイでは1940年代の電子計算機誕生直後から試みられてきた、コンピューターを音楽に用いるための歴史を時系列に追いかけている。Dannenbergのサーベイでは、同様に歴史的変遷を辿った上で、各言語を特徴づける要素をSyntax、Semantics、Library、Development Environment、Community &Resourcesという6つの要素として提示し、また、汎用プログラミングでは考慮されることの少ない音楽特有の課題を列挙し、代表的な言語間での記述の違いを分析している。

また[@Tanaka2017]は70〜80年代におけるチップチューンと呼ばれる、初期のパーソナルコンピューターやゲーム機においてとられた、音声生成用のICチップを用いた音楽についての歴史を主に解説しているが、40〜60年代までのコンピューター音楽黎明期についての記述も厚い。

本章でも先行文献と同様に歴史的に代表的な言語を時系列に紹介するが、既出の文献では言及されていなかった点として、以下の2つの項目に着目し、大きく時代を1970年、1990年ごろを大きな区切りとし、3つに区分けして整理する。

1970年代の区切りは2章で見てきたコンピューターをメタメディア装置として扱う思想の始まりと、パーソナルコンピューターの登場という2種類の出来事である。

そもそも音楽プログラミング言語の祖先となるソフトウェアMUSICが開発されたのは正解で最初の(汎用)プログラミング言語FORTRANが作られた翌年であり、当然1章で見たアラン・ケイらによる対話的プログラミング環境や豊富な入出力インターフェースを備えるよりもずっと前のことである。
なので、必然的に音楽ソフトウェアのプログラミング自体も機械語を直接入力するかアセンブリ言語(機械語の命令をテキストと1対1対応させたプリミティブなプログラミング言語のようなもの)しかなかったし、そのソフトウェアに対する入力データ（≒楽譜）も同様の形式を取らざるを得ないものだった。

つまり、1950〜1970年代の音楽プログラミング環境は大まかにいってコンピューターで音楽を作るためのソフトウェア全般の祖先にあたるものであって、必ずしもプログラミングという行為やテキスト入力という形式の固有性を積極的に取り入れたものではない、ということだ。逆に、70年代以降の音楽プログラミング言語/環境はマウスや(文字入力や、ピアノ鍵盤どちらにせよ)キーボード入力といった直感的なインターフェースが選択肢として存在する中で敢えてプログラミングという手段を使うものとして設計されてきた、という違いがあると言えるだろう。

また、1990年代の区切りは、パーソナルコンピューターが専用のサウンドチップなしに、CPUだけで音声信号処理をリアルタイムで行えるようになったこと、そして汎用プログラミング言語の理論が音楽向けの言語にも流入し始めたことの2種類である。



チップチューンを歴史に入れること

図を入れる

なお、本稿では既存のサーベイでは特に70年代以前の研究所レベルのコンピューターを用いた取り組みに関してはNishinoらのサーベイに十分詳しい記述がなされているので、本稿に大きく関係しないと事例については省くことにし、より記述の少ない2000年、2010年代に作られた言語について積極的に取り上げる。

# 研究所レベルでの取り組み

世界で初めて音楽にコンピューターを利用した例、というのを考えるのは、世界初のコンピューターは何かという論争や、世界で最初のアルゴリズミック・コンポジションとは何かといった命題につながってしまいキリがなくなってしまうので、ひとまず本稿で扱うのはENIAC以降の電子計算機、つまり真空管を用いてHIGH/LOWの2値をスイッチングすることで任意の計算を行える機械が誕生して以降の計算機群についての事例に限定することにする。

はじめてコンピューターを用いて音楽を鳴らした最初期の例としては、イギリスのコンピューターBINAC、アメリカのUNIVAC I、オーストラリアのCSIRACなどが挙げられる。これらはもっぱらデバッグ目的で取り付けられていたスピーカーに2値の信号をマスタークロックの周波数から逆算して規則的な周期で送ってあげれば、任意の音程の信号が出せるだろうという考えで作られたものだ。

そのため、任意の音程やリズムを奏でることはできるが、レコードのように任意の音圧波形を想いのままに作れるというわけではなかった。

コンピューターを用いて任意の波形を生成するという課題に最初に真正面から取り組んだのがBell研究所のMathewsらによるMUSICシリーズだった。

MUSICがそれ以前のシステムと異なっていたのは、パルス符合変調（PCM）と呼ばれる、音声波形を一定時間に分割（標本化）、各時間の音圧を離散的な数値として表す（量子化）、今日のコンピューター上における音声表現の基礎的な方法に基づいた計算を行ったことだ。パルス符合変調の元となる標本化定理はナイキストによって1928年に示され[@Nyquist1928]、パルス符合変調はReevesにより1938年に開発されている。

この考え方を簡単に表したのがHartley1928における図nである。時間を横軸、音圧を縦軸にとった音声波形のグラフをグリッド状に区切り、連続した数値を離散化された数値のリストに変換する。区切るグリッドが少ないほど、実際の波形との誤差が量子化歪みとして現れる一方、グリッドを細かくするほどに必要なデータの量は増えていく。また、標本化定理より、例えば1000Hzの周波数成分までを持つ波形を標本化するとき、その2倍である2000Hz以上、つまり横軸のグリッドを2000分の1秒より細かく設定する必要があることが知られている。例えばサンプリング周波数が1800Hzだった場合、表現できるのは900Hzまでとなり、1000Hzの正弦波をこのサンプリング周波数で標本化すると$1800-1000=800Hz$の信号が折り返し歪み(エイリアス信号)として現れてしまう。

人間の知覚できる周波数の上限が20000Hz程度となっているので、その2倍である40000Hz以上の標本化周波数で、かつ量子化歪みが十分に少なくなるように量子化ビット数を決めておけば、人間が近くできうる範囲ではおおよそどのような波形でも数値として表現できるということになる[^compactdisk]。

実際には連続した波形を離散化するだけであればこのような考え方の考慮で十分なのだが、標本化/量子化した波形同士を演算する場合にはもう少し細かい事情を考慮する必要があるため問題点も指摘されてきており[@Puckette2015]、音楽プログラミング言語設計の根幹にも関わってくるのだが、それはのちに議論する。

[^compactdisk]: 例えばコンパクトディスクの規格などはこういった考慮から標本化周波数44000Hz、量子化ビット数16bitと定められている。

<!--図 清書したほうがいいかも -->

![Hartleyの論文におけるPCMの概念を表した図。](img/hartley_pcm.png){width=70% #fig:hartley_pcm}


MUSICはIからVまでの5バージョンが存在している。

1957年に作られたMUSIC IはIBM 704というコンピューター上で動作する、対称系の三角波の波形に対してエンベロープを掛ける程度の波形の合成ができるシステムだった。この頃のシステムはリアルタイムで波形を電気信号として生成しスピーカーを鳴らせるようなものではない。まずMUSICのプログラムをコンピュータが利用できるIBM本社まで持ち込み、計算結果を磁気テープにバイナリデータとして書き出し、磁気テープをベル研究所に持ち帰り、そこにあった真空管製の12bitデジタル-アナログコンバーターに通してはじめて音が出せるようなシステムになっていた[@Roads1980;@Roads2001]。

MUSIC Iの時点でのサンプリング周波数について記述されている文献はいないが、後年のIBM 7090上で動作していたMUSIC IV(1963年)では、同様に12bitでの量子化で、実際の計算速度が1秒間に5000サンプル程度が性能の限界だったのに対し、実際に計算結果を再生するときにはそれを1秒に30000サンプルに早回しすることができたと記述が残っている[@Mathews1963]。

MathewsはMUSICを制作するにあたって、シャノンらが示した標本化と量子化によって、考えうるあらゆる種類の音が計算によって生み出せる、それもこれまで既存の楽器では奏でられなかったような新しい種類の音が作れることに強く関心を持っていたことを語っている。

> Essentially the sampling theorem shows that there are really no limits to the sounds you can make from samples. Any sound the human can hear, you can make with the right number, accuracy, and combination of samples, so the computer is a universal instrument. Other instruments, the violin in particular, are beautiful, lovable, but they always sound like a violin—or at least it’s very difficult to make them sound not like a violin.[@Park2009]

また彼は同時に、自身がバイオリン演奏をしていたが決して演奏が巧くはなかったため、身体的卓越を必要としない仕組みを作りたかったこと、さらに作曲家が曲を作ってもオーケストラに演奏してもらえる機会がなければ発表できないことに対するひとつの解として、作曲したものをコンピューターに演奏させるという2つのモチベーションも挙げている。

> My interests came from two things. 
> One was that, although I’ve always loved to play the violin and I’ve had an amateur string quartet going most of my life, I was never very good at it, and so I wanted to be able to make better music that didn’t require such manual dexterity as almost all [musical] instruments require. 
> I also felt that there were many composers who would compose a piece for an orchestra and who would never hear the piece, or its performance would be delayed for years, and so [the computer] would provide a way for composers to write something and hear it almost immediately.

これらコンピューターを音楽に用いる3つの理由を簡単に言い換えると次のようになるだろう。

まず1つはこれまで存在しなかった新しい音楽の探求だ。これはサンプリング定理に基づくものでない、音価レベルでのアルゴリズミック・コンポジションにも共通して言えるモチベーションと言える。2つ目は身体拡張、あるいは自動演奏の追求である。これは今日のNew Interfaces for Musical Expressionにおける研究のような、コンピューターを用いて音を生成するための身体との境界面：インターフェースを作ることによって音楽と身体の新しい関係性を発見するものだ。これは最終的に1つ目の新しい音楽表現につながることもあるが、逆に、表現自体はこれまでも存在していたが演奏に高度な技能が必要とされるものを誰でも演奏できるように…といったモチベーションにも読み替えられる。そして最後の作曲家のためのオーケストラに代わるものとしてのコンピューターとしての視点は、クイック・プロトタイピングのしやすさと表現するのがわかりやすいだろう。

ただし、もう少し本論文の興味に引き付けて読んでみると、**コンピュータ以前はオーケストラという装置が作曲のインフラストラクチャとして機能していた**という視点を導入することもできるだろう。つまり、有名な作曲家であればオーケストラに頻繁に演奏してもらえる機会に恵まれ、そうでなければ作曲家は楽譜上に音を配置し、ピアノなど少ない数の楽器である程度のシミュレーションをしながら作曲することしかできないという状態だったと言える。特に実験的な作品、すなわち既に共通して使われている楽譜上の記述では足りない表現を指示しようとしたり、大人数が演奏してみてはじめて曲の修正ができるような複雑な作品を作ろうとしたときにはオーケストラに実際に演奏してもらえるか否かは思い通りに作品が作れるかどうかに大きく関わっていたということができるだろう。



MUSICシリーズにおいて触れておくべきことは、MUSIC IIIにおいてはじめて**Unit Generator**と呼ばれる、今日まで用いられる概念が登場したことと、MUSIC IVにおいてはじめてそれ自体の実装が汎用プログラミング言語(FORTRAN)で実装されたことである。

まず、Unit Generatorとは〜

